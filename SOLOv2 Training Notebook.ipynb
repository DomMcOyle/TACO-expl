{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvkVAr9ZHli4"
      },
      "source": [
        "# ML4CV project work\n",
        "\n",
        "Summary: Improving and explaining instance segmentation on a litter detection dataset\n",
        "\n",
        "Members:\n",
        "- Dell'Olio Domenico\n",
        "- Delvecchio Giovanni Pio\n",
        "- Disabato Raffaele\n",
        "\n",
        "The project was developed in order to improve instance segmentation results on the [TACO Dataset](http://tacodataset.org/).\n",
        "\n",
        "We decided to implement and test various architectures, among the highest scoring on COCO instance segmentation datasets, in order to compare their performances.\n",
        "We also tested some explainability methods on these models to try and explain model predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3HmmoiOHnYb"
      },
      "source": [
        "## This notebook contains:\n",
        "- SOLOv2 model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGL4szW1o_UM"
      },
      "outputs": [],
      "source": [
        "# Installing Detectron2 and required libraries\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git@5aeb252b194b93dc2879b4ac34bc51a31b5aee13'\n",
        "!pip install rapidfuzz==2.15.1\n",
        "!python -m pip install numpy==1.23.1\n",
        "import numpy as np\n",
        "np.bool = np.bool_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9P0HDTGnRDI"
      },
      "outputs": [],
      "source": [
        "# Repository cloning and requirements installation\n",
        "%cd /content/\n",
        "!git clone https://github.com/DomMcOyle/TACO-expl.git\n",
        "%cd /content/TACO-expl\n",
        "!git checkout solov2\n",
        "!git pull origin solov2\n",
        "%cd /content/TACO-expl/AdelaiDet/\n",
        "!python setup.py build develop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKG8tycQnkyb",
        "outputId": "1d71bbb3-692c-459c-d8c1-37b691781a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/MyDrive/\n"
          ]
        }
      ],
      "source": [
        "# loading drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/MyDrive/\", force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ444LmfnVe7",
        "outputId": "8be842fe-0115-49f9-9330-214b2f8d8771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "torch:  2.2 ; cuda:  cu121\n",
            "detectron2: 0.6\n"
          ]
        }
      ],
      "source": [
        "# checking detectron2 installation and nvcc version\n",
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9AS4fHgIhd_"
      },
      "source": [
        "In this Notebook we explore the capabilites of the SOLOv2 architecture for instance segmentation presented by Wang et Al. in this [paper](https://arxiv.org/pdf/2003.10152).\n",
        "\n",
        "![](./res/solov2.png)\n",
        "\n",
        "The architecture is an improved version of [SOLO](https://arxiv.org/pdf/1912.04488) (Segmenting Objects by LOcations) by the same authors. The original work proposes a one-shot instance segmentation architecture based on the idea of sub-dividing the input in a SxS grid and for each patch the network produces in parallel a classification distribution (for the object in the cell) and a segmentation mask (class-agnostic). Both prediction are conditioned to the position of the object (normalized pixel coordinates are concatenated to the input) and are produced at different scales. In fact, the network uses a backbone network and a FPN to extract features at different scales. Instances are finally selected with NMS.\n",
        "\n",
        "The second version of this architecture improves the efficiency of the NMS techinique and the time and space efficiency of the network itself. In fact, considering that the object are sparsely distributed on the image, its highly inefficient to produce a mask for all the locations. For this reason, locations are pre-filtered based on their features and then their maps are obtained by convoluting a feature map that condenses information from all the FPN scales and a learned kernel (also conditioned on the location) that avoids differentiating the last convolution layer for each scale.\n",
        "\n",
        "The framework is pre-implemented in [Detectron 2](https://github.com/facebookresearch/detectron2) and we exploit this library to perform our experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfdhgozHnZpe",
        "outputId": "8d302372-2ddf-491b-e24a-38e6df4ae779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/TACO-expl/AdelaiDet/adet/modeling/solov2\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "from detectron2.modeling import build_model\n",
        "%cd /content/TACO-expl/AdelaiDet/adet/modeling/solov2/\n",
        "from solov2 import SOLOv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07S-PvH0WNdf"
      },
      "source": [
        "We didn't train the model from scratch, because we opted for a fine-tuning of the weights already provided within the repository.\n",
        "\n",
        "The chosen backbone for the model was ResNet50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAagzHqSncrP"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "cfg_solo_base_path = '/content/TACO-expl/AdelaiDet/configs/SOLOv2/Base-SOLOv2.yaml'\n",
        "cfg_solo_r50_path = '/content/TACO-expl/AdelaiDet/configs/SOLOv2/R50_3x.yaml'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjgwtT7tWSZA"
      },
      "source": [
        "The training is handled by a config file in yaml format which has to be modified and loaded beforehand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFqN2ogZngcc",
        "outputId": "7ad63011-f7f5-4402-d1ed-fed30380036c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    CROP_INSTANCE: True\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  HFLIP_TRAIN: True\n",
            "  IS_ROTATE: False\n",
            "  MASK_FORMAT: bitmask\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: DefaultAnchorGenerator\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[32, 64, 128, 256, 512]]\n",
            "  BACKBONE:\n",
            "    ANTI_ALIAS: False\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_resnet_fpn_backbone\n",
            "  BASIS_MODULE:\n",
            "    ANN_SET: coco\n",
            "    COMMON_STRIDE: 8\n",
            "    CONVS_DIM: 128\n",
            "    IN_FEATURES: ['p3', 'p4', 'p5']\n",
            "    LOSS_ON: False\n",
            "    LOSS_WEIGHT: 0.3\n",
            "    NAME: ProtoNet\n",
            "    NORM: SyncBN\n",
            "    NUM_BASES: 4\n",
            "    NUM_CLASSES: 80\n",
            "    NUM_CONVS: 3\n",
            "  BATEXT:\n",
            "    CANONICAL_SIZE: 96\n",
            "    CONV_DIM: 256\n",
            "    CUSTOM_DICT: \n",
            "    EVAL_TYPE: 3\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4']\n",
            "    NUM_CHARS: 25\n",
            "    NUM_CONV: 2\n",
            "    POOLER_RESOLUTION: (8, 32)\n",
            "    POOLER_SCALES: (0.25, 0.125, 0.0625)\n",
            "    RECOGNITION_LOSS: ctc\n",
            "    RECOGNIZER: attn\n",
            "    SAMPLING_RATIO: 1\n",
            "    USE_AET: False\n",
            "    USE_COORDCONV: False\n",
            "    VOC_SIZE: 96\n",
            "  BLENDMASK:\n",
            "    ATTN_SIZE: 14\n",
            "    BOTTOM_RESOLUTION: 56\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "    POOLER_SAMPLING_RATIO: 1\n",
            "    POOLER_SCALES: (0.25,)\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    TOP_INTERP: bilinear\n",
            "    VISUALIZE: False\n",
            "  BOXINST:\n",
            "    BOTTOM_PIXELS_REMOVED: 10\n",
            "    ENABLED: False\n",
            "    PAIRWISE:\n",
            "      COLOR_THRESH: 0.3\n",
            "      DILATION: 2\n",
            "      SIZE: 3\n",
            "      WARMUP_ITERS: 10000\n",
            "  BiFPN:\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    NUM_REPEATS: 6\n",
            "    OUT_CHANNELS: 160\n",
            "  CONDINST:\n",
            "    BOTTOM_PIXELS_REMOVED: -1\n",
            "    MASK_BRANCH:\n",
            "      CHANNELS: 128\n",
            "      IN_FEATURES: ['p3', 'p4', 'p5']\n",
            "      NORM: BN\n",
            "      NUM_CONVS: 4\n",
            "      OUT_CHANNELS: 8\n",
            "      SEMANTIC_LOSS_ON: False\n",
            "    MASK_HEAD:\n",
            "      CHANNELS: 8\n",
            "      DISABLE_REL_COORDS: False\n",
            "      NUM_LAYERS: 3\n",
            "      USE_FP16: False\n",
            "    MASK_OUT_STRIDE: 4\n",
            "    MAX_PROPOSALS: -1\n",
            "    TOPK_PROPOSALS_PER_IM: -1\n",
            "  DEVICE: cuda\n",
            "  DLA:\n",
            "    CONV_BODY: DLA34\n",
            "    NORM: FrozenBN\n",
            "    OUT_FEATURES: ['stage2', 'stage3', 'stage4', 'stage5']\n",
            "  FCOS:\n",
            "    BOX_QUALITY: ctrness\n",
            "    CENTER_SAMPLE: True\n",
            "    FPN_STRIDES: [8, 16, 32, 64, 128]\n",
            "    INFERENCE_TH_TEST: 0.05\n",
            "    INFERENCE_TH_TRAIN: 0.05\n",
            "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    LOC_LOSS_TYPE: giou\n",
            "    LOSS_ALPHA: 0.25\n",
            "    LOSS_GAMMA: 2.0\n",
            "    LOSS_NORMALIZER_CLS: fg\n",
            "    LOSS_WEIGHT_CLS: 1.0\n",
            "    NMS_TH: 0.6\n",
            "    NORM: GN\n",
            "    NUM_BOX_CONVS: 4\n",
            "    NUM_CLASSES: 80\n",
            "    NUM_CLS_CONVS: 4\n",
            "    NUM_SHARE_CONVS: 0\n",
            "    POST_NMS_TOPK_TEST: 100\n",
            "    POST_NMS_TOPK_TRAIN: 100\n",
            "    POS_RADIUS: 1.5\n",
            "    PRE_NMS_TOPK_TEST: 1000\n",
            "    PRE_NMS_TOPK_TRAIN: 1000\n",
            "    PRIOR_PROB: 0.01\n",
            "    SIZES_OF_INTEREST: [64, 128, 256, 512]\n",
            "    THRESH_WITH_CTR: False\n",
            "    TOP_LEVELS: 2\n",
            "    USE_DEFORMABLE: False\n",
            "    USE_RELU: True\n",
            "    USE_SCALE: True\n",
            "    YIELD_BOX_FEATURES: False\n",
            "    YIELD_PROPOSAL: False\n",
            "  FCPOSE:\n",
            "    ATTN_LEN: 2737\n",
            "    BASIS_MODULE:\n",
            "      BN_TYPE: SyncBN\n",
            "      COMMON_STRIDE: 8\n",
            "      CONVS_DIM: 128\n",
            "      LOSS_WEIGHT: 0.2\n",
            "      NUM_BASES: 32\n",
            "      NUM_CLASSES: 17\n",
            "    DISTANCE_NORM: 12.0\n",
            "    DYNAMIC_CHANNELS: 32\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    GT_HEATMAP_STRIDE: 2\n",
            "    HEAD_HEATMAP_SIGMA: 0.01\n",
            "    HEATMAP_SIGMA: 1.8\n",
            "    LOSS_WEIGHT_DIRECTION: 9.0\n",
            "    LOSS_WEIGHT_KEYPOINT: 2.5\n",
            "    MAX_PROPOSALS: 70\n",
            "    PROPOSALS_PER_INST: 70\n",
            "    SIGMA: 1\n",
            "  FCPOSE_ON: False\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: True\n",
            "  MEInst:\n",
            "    AGNOSTIC: True\n",
            "    CENTER_SAMPLE: True\n",
            "    DIM_MASK: 60\n",
            "    FLAG_PARAMETERS: False\n",
            "    FPN_STRIDES: [8, 16, 32, 64, 128]\n",
            "    GCN_KERNEL_SIZE: 9\n",
            "    INFERENCE_TH_TEST: 0.05\n",
            "    INFERENCE_TH_TRAIN: 0.05\n",
            "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    LAST_DEFORMABLE: False\n",
            "    LOC_LOSS_TYPE: giou\n",
            "    LOSS_ALPHA: 0.25\n",
            "    LOSS_GAMMA: 2.0\n",
            "    LOSS_ON_MASK: False\n",
            "    MASK_LOSS_TYPE: mse\n",
            "    MASK_ON: True\n",
            "    MASK_SIZE: 28\n",
            "    NMS_TH: 0.6\n",
            "    NORM: GN\n",
            "    NUM_BOX_CONVS: 4\n",
            "    NUM_CLASSES: 80\n",
            "    NUM_CLS_CONVS: 4\n",
            "    NUM_MASK_CONVS: 4\n",
            "    NUM_SHARE_CONVS: 0\n",
            "    PATH_COMPONENTS: datasets/coco/components/coco_2017_train_class_agnosticTrue_whitenTrue_sigmoidTrue_60.npz\n",
            "    POST_NMS_TOPK_TEST: 100\n",
            "    POST_NMS_TOPK_TRAIN: 100\n",
            "    POS_RADIUS: 1.5\n",
            "    PRE_NMS_TOPK_TEST: 1000\n",
            "    PRE_NMS_TOPK_TRAIN: 1000\n",
            "    PRIOR_PROB: 0.01\n",
            "    SIGMOID: True\n",
            "    SIZES_OF_INTEREST: [64, 128, 256, 512]\n",
            "    THRESH_WITH_CTR: False\n",
            "    TOP_LEVELS: 2\n",
            "    TYPE_DEFORMABLE: DCNv1\n",
            "    USE_DEFORMABLE: False\n",
            "    USE_GCN_IN_MASK: False\n",
            "    USE_RELU: True\n",
            "    USE_SCALE: True\n",
            "    WHITEN: True\n",
            "  META_ARCHITECTURE: SOLOv2\n",
            "  MOBILENET: False\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  RESNETS:\n",
            "    DEFORM_INTERVAL: 1\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 80\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    FED_LOSS_FREQ_WEIGHT_POWER: 0.5\n",
            "    FED_LOSS_NUM_CLASSES: 50\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "    USE_FED_LOSS: False\n",
            "    USE_SIGMOID_CE: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    CONV_DIMS: [-1]\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  SOLOV2:\n",
            "    FPN_INSTANCE_STRIDES: [8, 8, 16, 32, 32]\n",
            "    FPN_SCALE_RANGES: ((1, 96), (48, 192), (96, 384), (192, 768), (384, 2048))\n",
            "    INSTANCE_CHANNELS: 512\n",
            "    INSTANCE_IN_CHANNELS: 256\n",
            "    INSTANCE_IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']\n",
            "    LOSS:\n",
            "      DICE_WEIGHT: 3.0\n",
            "      FOCAL_ALPHA: 0.25\n",
            "      FOCAL_GAMMA: 2.0\n",
            "      FOCAL_USE_SIGMOID: True\n",
            "      FOCAL_WEIGHT: 1.0\n",
            "    MASK_CHANNELS: 128\n",
            "    MASK_IN_CHANNELS: 256\n",
            "    MASK_IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    MASK_THR: 0.5\n",
            "    MAX_PER_IMG: 100\n",
            "    NMS_KERNEL: gaussian\n",
            "    NMS_PRE: 500\n",
            "    NMS_SIGMA: 2\n",
            "    NMS_TYPE: matrix\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 80\n",
            "    NUM_GRIDS: [40, 36, 24, 16, 12]\n",
            "    NUM_INSTANCE_CONVS: 4\n",
            "    NUM_KERNELS: 256\n",
            "    NUM_MASKS: 256\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THR: 0.1\n",
            "    SIGMA: 0.2\n",
            "    TYPE_DCN: DCN\n",
            "    UPDATE_THR: 0.05\n",
            "    USE_COORD_CONV: True\n",
            "    USE_DCN_IN_INSTANCE: False\n",
            "  TOP_MODULE:\n",
            "    DIM: 16\n",
            "    NAME: conv\n",
            "  VOVNET:\n",
            "    BACKBONE_OUT_CHANNELS: 256\n",
            "    CONV_BODY: V-39-eSE\n",
            "    NORM: FrozenBN\n",
            "    OUT_CHANNELS: 256\n",
            "    OUT_FEATURES: ['stage2', 'stage3', 'stage4', 'stage5']\n",
            "  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl\n",
            "OUTPUT_DIR: ./output\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: False\n",
            "  BASE_LR: 0.01\n",
            "  BASE_LR_END: 0.0\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 1.0\n",
            "    ENABLED: False\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 16\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 270000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (210000, 250000)\n",
            "  WARMUP_FACTOR: 0.01\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: None\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 100\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VIS_PERIOD: 0\n"
          ]
        }
      ],
      "source": [
        "sys.path.append('/content/TACO-expl/AdelaiDet/adet/config')\n",
        "from defaults import _C\n",
        "\n",
        "def setup_cfg(cfg_base, cfg_backbone):\n",
        "  \"\"\"\n",
        "  loads config from file and command-line arguments\n",
        "  :param cfg_base: config file path\n",
        "  :param cfg_backbone: uconfig file path of the backbone\n",
        "  \"\"\"\n",
        "  cfg = _C\n",
        "  cfg.merge_from_file(cfg_base)\n",
        "  cfg.merge_from_file(cfg_backbone)\n",
        "  return cfg\n",
        "\n",
        "cfg_solov2 = setup_cfg(cfg_solo_base_path, cfg_solo_r50_path)\n",
        "print(cfg_solov2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXBFStbMWxZ8"
      },
      "source": [
        "We leave the following cell reporting the number of trained parameters for the SOLOv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU8b_0sjT80T",
        "outputId": "27b507d1-806b-457a-e5d7-6dd3f1ef6bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46317392\n"
          ]
        }
      ],
      "source": [
        "model = build_model(cfg_solov2)\n",
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(num_trainable_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHfxVrm3nrzg"
      },
      "outputs": [],
      "source": [
        "# sets the radnom seed\n",
        "DEFAULT_RANDOM_SEED = 42\n",
        "# basic random seed\n",
        "def seedBasic(seed=DEFAULT_RANDOM_SEED):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "# torch random seed\n",
        "def seedTorch(seed=DEFAULT_RANDOM_SEED):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "# combine\n",
        "def seedEverything(seed=DEFAULT_RANDOM_SEED):\n",
        "    seedBasic(seed)\n",
        "    seedTorch(seed)\n",
        "\n",
        "seedEverything()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abjM4Yf0jP7s"
      },
      "outputs": [],
      "source": [
        "#!mkdir /content/official/\n",
        "!cp -r /content/MyDrive/MyDrive/official/ /content/\n",
        "!mkdir /content/output/\n",
        "!cp -r /content/MyDrive/MyDrive/solo_models/chkpt/ /content/output/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkkRWttdXOJI"
      },
      "source": [
        "We also load and register the dataset, generated in the \"MFDETR Training Notebook.ipynb\" and modified in \"validation resizing.ipynb\" as a coco dataset instance.\n",
        "\n",
        "Differently from the one employed in the Mask-Frozen DETR Training, the images in the validation set and test set are already rotated and resized to a 800x1333 maximum size due to resources requirements. Also the images in the training set are already rotated. This was done to simplify the Detectron2 training pipeline, which is already quite convoluted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmR4YkANn3eG",
        "outputId": "448cefe6-eb7c-41e0-8f33-b6dd162d960f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 06:07:46 d2.data.datasets.coco]: Loaded 1200 images in COCO format from /content/TACO-expl/data/annotations_off_0_train.json\n",
            "[05/10 06:07:46 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/TACO-expl/data/annotations_off_0_val.json\n"
          ]
        }
      ],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "with open(\"/content/TACO-expl/data/annotations_off_0_train.json\", \"r\") as f:\n",
        "    dataset = json.loads(f.read())\n",
        "classes = [elem[\"name\"] for elem in dataset[\"categories\"]]\n",
        "\n",
        "train_annotation_file = '/content/TACO-expl/data/annotations_off_0_train.json'\n",
        "val_annotation_file = '/content/TACO-expl/data/annotations_off_0_val.json'\n",
        "\n",
        "img_dir = '/content/official/'\n",
        "\n",
        "\n",
        "register_coco_instances(\"TACO_train\", {}, train_annotation_file, img_dir)\n",
        "MetadataCatalog.get(\"TACO_train\").set(thing_classes = classes)\n",
        "dataset_dicts_train = DatasetCatalog.get(\"TACO_train\")\n",
        "\n",
        "register_coco_instances(\"TACO_val\", {}, val_annotation_file, img_dir)\n",
        "MetadataCatalog.get(\"TACO_val\").set(thing_classes = classes)\n",
        "dataset_dicts_val = DatasetCatalog.get(\"TACO_val\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J9yZWEoXWPy"
      },
      "source": [
        "\n",
        "\n",
        "Here we check some of the configuration values employed for training. The training recipe provided in the repository was used as starting point for the one we actually employed in the end, which consists of:\n",
        "\n",
        "- Image normalization.\n",
        "- Data augmentation consisting of Random Horizondtal flip, random resizing with maximum size 1333x800, Random crop.\n",
        "- Optimizer is AdamW with 1e-4 weight decay and gradient clipping to 1\n",
        "- Learning rate is set to 1e-3\n",
        "and scheduled with a Warm-up cosine scheduler. The minimum learning rate was 1e-4.\n",
        "- Batch size set to 8.\n",
        "- Maximum Number of epochs set to 12."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhuvntPZn9Ar"
      },
      "outputs": [],
      "source": [
        "train_cfg = cfg_solov2.clone()\n",
        "with open(\"/content/TACO-expl/taco_train_solov2.yaml\", \"w\") as f:\n",
        "  f.write(train_cfg.dump())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDjfTaa9n9W8",
        "outputId": "5b9791bc-565e-4cbe-e233-c020d56cbbe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "4\n",
            "/content/output/chkpt/\n",
            "/content/output/chkpt/model_0001949.pth\n"
          ]
        }
      ],
      "source": [
        "train_cfg_loaded = get_cfg()\n",
        "train_cfg_loaded.set_new_allowed(True)\n",
        "train_cfg_loaded.merge_from_file(\"/content/TACO-expl/solov2_config/taco_train_solov2.yaml\")\n",
        "print(train_cfg_loaded.SOLVER.IMS_PER_BATCH)\n",
        "print(train_cfg_loaded.DATALOADER.NUM_WORKERS)\n",
        "print(train_cfg_loaded.OUTPUT_DIR)\n",
        "print(train_cfg_loaded.MODEL.WEIGHTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRqzlfJAGd06"
      },
      "source": [
        "By running the following cells, the model can be fine-tuned. Our experiments on this model were actually pretty scarce as the initial results were quite poor even when compared with those in the TACO paper.\n",
        "\n",
        "In fact, we tried to change the recipe by modifying learning rate and the number of stages of the backbone to be frozen, but the results were always around 12-13% MAP, so we abandoned the idea of adopting this model, because even if in is indesputably faster than MaskDINO and Mask-Frozen DETR, the tradeoff in performances is too high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MsfHBcWn_O1"
      },
      "outputs": [],
      "source": [
        "%cd /content/TACO-expl/AdelaiDet/tools/\n",
        "from train_net import Trainer\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N9mz87nF0TI",
        "outputId": "06b0c2c2-eca7-45cc-af60-a4a96a23c155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 06:29:47 d2.engine.defaults]: Model:\n",
            "SOLOv2(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ins_head): SOLOv2InsHead(\n",
            "    (cate_tower): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (7): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      (8): ReLU(inplace=True)\n",
            "      (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (10): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      (11): ReLU(inplace=True)\n",
            "    )\n",
            "    (kernel_tower): Sequential(\n",
            "      (0): Conv2d(258, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (7): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      (8): ReLU(inplace=True)\n",
            "      (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (10): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      (11): ReLU(inplace=True)\n",
            "    )\n",
            "    (cate_pred): Conv2d(512, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (kernel_pred): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (mask_head): SOLOv2MaskHead(\n",
            "    (convs_all_levels): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (conv0): Sequential(\n",
            "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "          (2): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (conv0): Sequential(\n",
            "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "          (2): ReLU()\n",
            "        )\n",
            "        (upsample0): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (conv0): Sequential(\n",
            "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "          (2): ReLU()\n",
            "        )\n",
            "        (upsample0): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "          (2): ReLU()\n",
            "        )\n",
            "        (upsample1): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (conv0): Sequential(\n",
            "          (0): Conv2d(258, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "          (2): ReLU()\n",
            "        )\n",
            "        (upsample0): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "          (2): ReLU()\n",
            "        )\n",
            "        (upsample1): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "          (2): ReLU()\n",
            "        )\n",
            "        (upsample2): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      )\n",
            "    )\n",
            "    (conv_pred): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[05/10 06:29:48 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[05/10 06:29:48 d2.data.datasets.coco]: Loaded 1200 images in COCO format from /content/TACO-expl/data/annotations_off_0_train.json\n",
            "[05/10 06:29:48 d2.data.build]: Removed 0 images with no usable annotations. 1200 images left.\n",
            "[05/10 06:29:48 d2.data.build]: Using training sampler TrainingSampler\n",
            "[05/10 06:29:48 d2.data.common]: Serializing 1200 elements to byte tensors and concatenating them all ...\n",
            "[05/10 06:29:48 d2.data.common]: Serialized dataset takes 1.79 MiB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<detectron2.engine.hooks.IterationTimer at 0x7b40e0ab1d80>,\n",
              " <detectron2.engine.hooks.LRScheduler at 0x7b40e0a8a050>,\n",
              " None,\n",
              " <detectron2.engine.hooks.PeriodicCheckpointer at 0x7b40e0a88cd0>,\n",
              " <detectron2.engine.hooks.EvalHook at 0x7b40e0a89ea0>,\n",
              " <detectron2.engine.hooks.PeriodicWriter at 0x7b40e0a88e20>]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(train_cfg_loaded)\n",
        "trainer.build_hooks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvnHug9_oDE_",
        "outputId": "7f145af0-f464-43da-d7f5-730caf63559e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 07:19:26 d2.checkpoint.c2_model_loading]: Following weights matched with model:\n",
            "| Names in Model                              | Names in Checkpoint                                                                                  | Shapes                                          |\n",
            "|:--------------------------------------------|:-----------------------------------------------------------------------------------------------------|:------------------------------------------------|\n",
            "| backbone.bottom_up.res2.0.conv1.*           | backbone.bottom_up.res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,1,1)             |\n",
            "| backbone.bottom_up.res2.0.conv2.*           | backbone.bottom_up.res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n",
            "| backbone.bottom_up.res2.0.conv3.*           | backbone.bottom_up.res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
            "| backbone.bottom_up.res2.0.shortcut.*        | backbone.bottom_up.res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
            "| backbone.bottom_up.res2.1.conv1.*           | backbone.bottom_up.res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |\n",
            "| backbone.bottom_up.res2.1.conv2.*           | backbone.bottom_up.res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n",
            "| backbone.bottom_up.res2.1.conv3.*           | backbone.bottom_up.res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
            "| backbone.bottom_up.res2.2.conv1.*           | backbone.bottom_up.res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |\n",
            "| backbone.bottom_up.res2.2.conv2.*           | backbone.bottom_up.res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n",
            "| backbone.bottom_up.res2.2.conv3.*           | backbone.bottom_up.res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
            "| backbone.bottom_up.res3.0.conv1.*           | backbone.bottom_up.res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,256,1,1)       |\n",
            "| backbone.bottom_up.res3.0.conv2.*           | backbone.bottom_up.res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
            "| backbone.bottom_up.res3.0.conv3.*           | backbone.bottom_up.res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
            "| backbone.bottom_up.res3.0.shortcut.*        | backbone.bottom_up.res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (512,) (512,) (512,) (512,) (512,256,1,1)       |\n",
            "| backbone.bottom_up.res3.1.conv1.*           | backbone.bottom_up.res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n",
            "| backbone.bottom_up.res3.1.conv2.*           | backbone.bottom_up.res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
            "| backbone.bottom_up.res3.1.conv3.*           | backbone.bottom_up.res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
            "| backbone.bottom_up.res3.2.conv1.*           | backbone.bottom_up.res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n",
            "| backbone.bottom_up.res3.2.conv2.*           | backbone.bottom_up.res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
            "| backbone.bottom_up.res3.2.conv3.*           | backbone.bottom_up.res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
            "| backbone.bottom_up.res3.3.conv1.*           | backbone.bottom_up.res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n",
            "| backbone.bottom_up.res3.3.conv2.*           | backbone.bottom_up.res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
            "| backbone.bottom_up.res3.3.conv3.*           | backbone.bottom_up.res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
            "| backbone.bottom_up.res4.0.conv1.*           | backbone.bottom_up.res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,512,1,1)       |\n",
            "| backbone.bottom_up.res4.0.conv2.*           | backbone.bottom_up.res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
            "| backbone.bottom_up.res4.0.conv3.*           | backbone.bottom_up.res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
            "| backbone.bottom_up.res4.0.shortcut.*        | backbone.bottom_up.res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |\n",
            "| backbone.bottom_up.res4.1.conv1.*           | backbone.bottom_up.res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
            "| backbone.bottom_up.res4.1.conv2.*           | backbone.bottom_up.res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
            "| backbone.bottom_up.res4.1.conv3.*           | backbone.bottom_up.res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
            "| backbone.bottom_up.res4.2.conv1.*           | backbone.bottom_up.res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
            "| backbone.bottom_up.res4.2.conv2.*           | backbone.bottom_up.res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
            "| backbone.bottom_up.res4.2.conv3.*           | backbone.bottom_up.res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
            "| backbone.bottom_up.res4.3.conv1.*           | backbone.bottom_up.res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
            "| backbone.bottom_up.res4.3.conv2.*           | backbone.bottom_up.res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
            "| backbone.bottom_up.res4.3.conv3.*           | backbone.bottom_up.res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
            "| backbone.bottom_up.res4.4.conv1.*           | backbone.bottom_up.res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
            "| backbone.bottom_up.res4.4.conv2.*           | backbone.bottom_up.res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
            "| backbone.bottom_up.res4.4.conv3.*           | backbone.bottom_up.res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
            "| backbone.bottom_up.res4.5.conv1.*           | backbone.bottom_up.res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
            "| backbone.bottom_up.res4.5.conv2.*           | backbone.bottom_up.res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
            "| backbone.bottom_up.res4.5.conv3.*           | backbone.bottom_up.res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
            "| backbone.bottom_up.res5.0.conv1.*           | backbone.bottom_up.res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,1024,1,1)      |\n",
            "| backbone.bottom_up.res5.0.conv2.*           | backbone.bottom_up.res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n",
            "| backbone.bottom_up.res5.0.conv3.*           | backbone.bottom_up.res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n",
            "| backbone.bottom_up.res5.0.shortcut.*        | backbone.bottom_up.res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |\n",
            "| backbone.bottom_up.res5.1.conv1.*           | backbone.bottom_up.res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |\n",
            "| backbone.bottom_up.res5.1.conv2.*           | backbone.bottom_up.res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n",
            "| backbone.bottom_up.res5.1.conv3.*           | backbone.bottom_up.res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n",
            "| backbone.bottom_up.res5.2.conv1.*           | backbone.bottom_up.res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |\n",
            "| backbone.bottom_up.res5.2.conv2.*           | backbone.bottom_up.res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n",
            "| backbone.bottom_up.res5.2.conv3.*           | backbone.bottom_up.res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n",
            "| backbone.bottom_up.stem.conv1.*             | backbone.bottom_up.stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (64,) (64,) (64,) (64,) (64,3,7,7)              |\n",
            "| backbone.fpn_lateral2.*                     | backbone.fpn_lateral2.{bias,weight}                                                                  | (256,) (256,256,1,1)                            |\n",
            "| backbone.fpn_lateral3.*                     | backbone.fpn_lateral3.{bias,weight}                                                                  | (256,) (256,512,1,1)                            |\n",
            "| backbone.fpn_lateral4.*                     | backbone.fpn_lateral4.{bias,weight}                                                                  | (256,) (256,1024,1,1)                           |\n",
            "| backbone.fpn_lateral5.*                     | backbone.fpn_lateral5.{bias,weight}                                                                  | (256,) (256,2048,1,1)                           |\n",
            "| backbone.fpn_output2.*                      | backbone.fpn_output2.{bias,weight}                                                                   | (256,) (256,256,3,3)                            |\n",
            "| backbone.fpn_output3.*                      | backbone.fpn_output3.{bias,weight}                                                                   | (256,) (256,256,3,3)                            |\n",
            "| backbone.fpn_output4.*                      | backbone.fpn_output4.{bias,weight}                                                                   | (256,) (256,256,3,3)                            |\n",
            "| backbone.fpn_output5.*                      | backbone.fpn_output5.{bias,weight}                                                                   | (256,) (256,256,3,3)                            |\n",
            "| ins_head.cate_pred.*                        | ins_head.cate_pred.{bias,weight}                                                                     | (10,) (10,512,3,3)                              |\n",
            "| ins_head.cate_tower.0.weight                | ins_head.cate_tower.0.weight                                                                         | (512, 256, 3, 3)                                |\n",
            "| ins_head.cate_tower.1.*                     | ins_head.cate_tower.1.{bias,weight}                                                                  | (512,) (512,)                                   |\n",
            "| ins_head.cate_tower.10.*                    | ins_head.cate_tower.10.{bias,weight}                                                                 | (512,) (512,)                                   |\n",
            "| ins_head.cate_tower.3.weight                | ins_head.cate_tower.3.weight                                                                         | (512, 512, 3, 3)                                |\n",
            "| ins_head.cate_tower.4.*                     | ins_head.cate_tower.4.{bias,weight}                                                                  | (512,) (512,)                                   |\n",
            "| ins_head.cate_tower.6.weight                | ins_head.cate_tower.6.weight                                                                         | (512, 512, 3, 3)                                |\n",
            "| ins_head.cate_tower.7.*                     | ins_head.cate_tower.7.{bias,weight}                                                                  | (512,) (512,)                                   |\n",
            "| ins_head.cate_tower.9.weight                | ins_head.cate_tower.9.weight                                                                         | (512, 512, 3, 3)                                |\n",
            "| ins_head.kernel_pred.*                      | ins_head.kernel_pred.{bias,weight}                                                                   | (256,) (256,512,3,3)                            |\n",
            "| ins_head.kernel_tower.0.weight              | ins_head.kernel_tower.0.weight                                                                       | (512, 258, 3, 3)                                |\n",
            "| ins_head.kernel_tower.1.*                   | ins_head.kernel_tower.1.{bias,weight}                                                                | (512,) (512,)                                   |\n",
            "| ins_head.kernel_tower.10.*                  | ins_head.kernel_tower.10.{bias,weight}                                                               | (512,) (512,)                                   |\n",
            "| ins_head.kernel_tower.3.weight              | ins_head.kernel_tower.3.weight                                                                       | (512, 512, 3, 3)                                |\n",
            "| ins_head.kernel_tower.4.*                   | ins_head.kernel_tower.4.{bias,weight}                                                                | (512,) (512,)                                   |\n",
            "| ins_head.kernel_tower.6.weight              | ins_head.kernel_tower.6.weight                                                                       | (512, 512, 3, 3)                                |\n",
            "| ins_head.kernel_tower.7.*                   | ins_head.kernel_tower.7.{bias,weight}                                                                | (512,) (512,)                                   |\n",
            "| ins_head.kernel_tower.9.weight              | ins_head.kernel_tower.9.weight                                                                       | (512, 512, 3, 3)                                |\n",
            "| mask_head.conv_pred.0.weight                | mask_head.conv_pred.0.weight                                                                         | (256, 128, 1, 1)                                |\n",
            "| mask_head.conv_pred.1.*                     | mask_head.conv_pred.1.{bias,weight}                                                                  | (256,) (256,)                                   |\n",
            "| mask_head.convs_all_levels.0.conv0.0.weight | mask_head.convs_all_levels.0.conv0.0.weight                                                          | (128, 256, 3, 3)                                |\n",
            "| mask_head.convs_all_levels.0.conv0.1.*      | mask_head.convs_all_levels.0.conv0.1.{bias,weight}                                                   | (128,) (128,)                                   |\n",
            "| mask_head.convs_all_levels.1.conv0.0.weight | mask_head.convs_all_levels.1.conv0.0.weight                                                          | (128, 256, 3, 3)                                |\n",
            "| mask_head.convs_all_levels.1.conv0.1.*      | mask_head.convs_all_levels.1.conv0.1.{bias,weight}                                                   | (128,) (128,)                                   |\n",
            "| mask_head.convs_all_levels.2.conv0.0.weight | mask_head.convs_all_levels.2.conv0.0.weight                                                          | (128, 256, 3, 3)                                |\n",
            "| mask_head.convs_all_levels.2.conv0.1.*      | mask_head.convs_all_levels.2.conv0.1.{bias,weight}                                                   | (128,) (128,)                                   |\n",
            "| mask_head.convs_all_levels.2.conv1.0.weight | mask_head.convs_all_levels.2.conv1.0.weight                                                          | (128, 128, 3, 3)                                |\n",
            "| mask_head.convs_all_levels.2.conv1.1.*      | mask_head.convs_all_levels.2.conv1.1.{bias,weight}                                                   | (128,) (128,)                                   |\n",
            "| mask_head.convs_all_levels.3.conv0.0.weight | mask_head.convs_all_levels.3.conv0.0.weight                                                          | (128, 258, 3, 3)                                |\n",
            "| mask_head.convs_all_levels.3.conv0.1.*      | mask_head.convs_all_levels.3.conv0.1.{bias,weight}                                                   | (128,) (128,)                                   |\n",
            "| mask_head.convs_all_levels.3.conv1.0.weight | mask_head.convs_all_levels.3.conv1.0.weight                                                          | (128, 128, 3, 3)                                |\n",
            "| mask_head.convs_all_levels.3.conv1.1.*      | mask_head.convs_all_levels.3.conv1.1.{bias,weight}                                                   | (128,) (128,)                                   |\n",
            "| mask_head.convs_all_levels.3.conv2.0.weight | mask_head.convs_all_levels.3.conv2.0.weight                                                          | (128, 128, 3, 3)                                |\n",
            "| mask_head.convs_all_levels.3.conv2.1.*      | mask_head.convs_all_levels.3.conv2.1.{bias,weight}                                                   | (128,) (128,)                                   |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 07:20:08 d2.utils.events]:  eta: 3:27:05  iter: 1959  total_loss: 1.25  loss_ins: 0.9572  loss_cate: 0.2914  time: 3.7122  data_time: 2.2751  lr: 0.00076578  max_mem: 8286M\n",
            "[05/10 07:21:13 d2.utils.events]:  eta: 2:58:10  iter: 1979  total_loss: 1.426  loss_ins: 1.122  loss_cate: 0.2944  time: 3.3850  data_time: 1.6352  lr: 0.00076145  max_mem: 8419M\n",
            "[05/10 07:22:24 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/TACO-expl/data/annotations_off_0_val.json\n",
            "[05/10 07:22:24 d2.data.build]: Distribution of instances among all 10 categories:\n",
            "|  category  | #instances   |   category    | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:-------------:|:-------------|:----------:|:-------------|\n",
            "|   Bottle   | 50           |  Bottle cap   | 30           |    Can     | 21           |\n",
            "| Cigarette  | 42           |      Cup      | 21           |    Lid     | 8            |\n",
            "|   Other    | 135          | Plastic bag.. | 77           |  Pop tab   | 10           |\n",
            "|   Straw    | 16           |               |              |            |              |\n",
            "|   total    | 410          |               |              |            |              |\n",
            "[05/10 07:22:24 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/10 07:22:24 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[05/10 07:22:24 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "WARNING [05/10 07:22:24 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[05/10 07:22:24 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 07:22:58 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0019 s/iter. Inference: 0.2404 s/iter. Eval: 4.2811 s/iter. Total: 4.5234 s/iter. ETA=0:10:28\n",
            "[05/10 07:23:04 d2.evaluation.evaluator]: Inference done 12/150. Dataloading: 0.0021 s/iter. Inference: 0.2417 s/iter. Eval: 4.4006 s/iter. Total: 4.6453 s/iter. ETA=0:10:41\n",
            "[05/10 07:23:15 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0036 s/iter. Inference: 0.2469 s/iter. Eval: 4.1582 s/iter. Total: 4.4099 s/iter. ETA=0:09:55\n",
            "[05/10 07:23:23 d2.evaluation.evaluator]: Inference done 18/150. Dataloading: 0.0033 s/iter. Inference: 0.2304 s/iter. Eval: 3.7105 s/iter. Total: 3.9452 s/iter. ETA=0:08:40\n",
            "[05/10 07:23:33 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0033 s/iter. Inference: 0.2293 s/iter. Eval: 4.1802 s/iter. Total: 4.4139 s/iter. ETA=0:09:38\n",
            "[05/10 07:23:48 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0033 s/iter. Inference: 0.2462 s/iter. Eval: 4.5581 s/iter. Total: 4.8089 s/iter. ETA=0:10:20\n",
            "[05/10 07:23:55 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0033 s/iter. Inference: 0.2433 s/iter. Eval: 4.6548 s/iter. Total: 4.9026 s/iter. ETA=0:10:27\n",
            "[05/10 07:24:02 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0033 s/iter. Inference: 0.2431 s/iter. Eval: 4.8095 s/iter. Total: 5.0573 s/iter. ETA=0:10:42\n",
            "[05/10 07:24:13 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0033 s/iter. Inference: 0.2514 s/iter. Eval: 5.1147 s/iter. Total: 5.3709 s/iter. ETA=0:11:16\n",
            "[05/10 07:24:20 d2.evaluation.evaluator]: Inference done 25/150. Dataloading: 0.0033 s/iter. Inference: 0.2491 s/iter. Eval: 5.1629 s/iter. Total: 5.4168 s/iter. ETA=0:11:17\n",
            "[05/10 07:24:32 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0038 s/iter. Inference: 0.2466 s/iter. Eval: 5.2476 s/iter. Total: 5.4996 s/iter. ETA=0:11:16\n",
            "[05/10 07:24:48 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0038 s/iter. Inference: 0.2558 s/iter. Eval: 5.6865 s/iter. Total: 5.9479 s/iter. ETA=0:12:05\n",
            "[05/10 07:25:01 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0037 s/iter. Inference: 0.2638 s/iter. Eval: 5.9541 s/iter. Total: 6.2237 s/iter. ETA=0:12:33\n",
            "[05/10 07:25:14 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0037 s/iter. Inference: 0.2606 s/iter. Eval: 5.9951 s/iter. Total: 6.2614 s/iter. ETA=0:12:25\n",
            "[05/10 07:25:19 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0037 s/iter. Inference: 0.2587 s/iter. Eval: 5.9580 s/iter. Total: 6.2225 s/iter. ETA=0:12:14\n",
            "[05/10 07:25:26 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0036 s/iter. Inference: 0.2568 s/iter. Eval: 5.9783 s/iter. Total: 6.2408 s/iter. ETA=0:12:10\n",
            "[05/10 07:25:39 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0036 s/iter. Inference: 0.2553 s/iter. Eval: 6.2093 s/iter. Total: 6.4703 s/iter. ETA=0:12:30\n",
            "[05/10 07:25:50 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0036 s/iter. Inference: 0.2554 s/iter. Eval: 6.3733 s/iter. Total: 6.6344 s/iter. ETA=0:12:42\n",
            "[05/10 07:26:00 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0035 s/iter. Inference: 0.2522 s/iter. Eval: 6.2512 s/iter. Total: 6.5090 s/iter. ETA=0:12:15\n",
            "[05/10 07:26:12 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0035 s/iter. Inference: 0.2520 s/iter. Eval: 6.4444 s/iter. Total: 6.7020 s/iter. ETA=0:12:30\n",
            "[05/10 07:26:25 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0035 s/iter. Inference: 0.2517 s/iter. Eval: 6.6297 s/iter. Total: 6.8871 s/iter. ETA=0:12:44\n",
            "[05/10 07:26:44 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0036 s/iter. Inference: 0.2587 s/iter. Eval: 6.9469 s/iter. Total: 7.2115 s/iter. ETA=0:13:13\n",
            "[05/10 07:26:55 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0036 s/iter. Inference: 0.2577 s/iter. Eval: 7.0708 s/iter. Total: 7.3343 s/iter. ETA=0:13:19\n",
            "[05/10 07:27:12 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0036 s/iter. Inference: 0.2572 s/iter. Eval: 7.3195 s/iter. Total: 7.5828 s/iter. ETA=0:13:38\n",
            "[05/10 07:27:29 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0036 s/iter. Inference: 0.2550 s/iter. Eval: 7.3752 s/iter. Total: 7.6360 s/iter. ETA=0:13:29\n",
            "[05/10 07:27:43 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0037 s/iter. Inference: 0.2547 s/iter. Eval: 7.5426 s/iter. Total: 7.8032 s/iter. ETA=0:13:39\n",
            "[05/10 07:27:51 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0037 s/iter. Inference: 0.2536 s/iter. Eval: 7.5336 s/iter. Total: 7.7932 s/iter. ETA=0:13:30\n",
            "[05/10 07:28:01 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0038 s/iter. Inference: 0.2531 s/iter. Eval: 7.5960 s/iter. Total: 7.8551 s/iter. ETA=0:13:29\n",
            "[05/10 07:28:17 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0037 s/iter. Inference: 0.2569 s/iter. Eval: 7.7864 s/iter. Total: 8.0494 s/iter. ETA=0:13:41\n",
            "[05/10 07:28:25 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0037 s/iter. Inference: 0.2558 s/iter. Eval: 7.7683 s/iter. Total: 8.0302 s/iter. ETA=0:13:31\n",
            "[05/10 07:28:36 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0038 s/iter. Inference: 0.2535 s/iter. Eval: 7.6604 s/iter. Total: 7.9199 s/iter. ETA=0:13:04\n",
            "[05/10 07:28:50 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0037 s/iter. Inference: 0.2510 s/iter. Eval: 7.6278 s/iter. Total: 7.8849 s/iter. ETA=0:12:44\n",
            "[05/10 07:28:56 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0037 s/iter. Inference: 0.2492 s/iter. Eval: 7.4457 s/iter. Total: 7.7009 s/iter. ETA=0:12:11\n",
            "[05/10 07:29:08 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0037 s/iter. Inference: 0.2477 s/iter. Eval: 7.3829 s/iter. Total: 7.6366 s/iter. ETA=0:11:50\n",
            "[05/10 07:29:15 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0037 s/iter. Inference: 0.2458 s/iter. Eval: 7.2232 s/iter. Total: 7.4749 s/iter. ETA=0:11:20\n",
            "[05/10 07:29:24 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0036 s/iter. Inference: 0.2417 s/iter. Eval: 6.8629 s/iter. Total: 7.1103 s/iter. ETA=0:10:18\n",
            "[05/10 07:29:30 d2.evaluation.evaluator]: Inference done 66/150. Dataloading: 0.0036 s/iter. Inference: 0.2388 s/iter. Eval: 6.6178 s/iter. Total: 6.8623 s/iter. ETA=0:09:36\n",
            "[05/10 07:29:36 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0036 s/iter. Inference: 0.2375 s/iter. Eval: 6.5002 s/iter. Total: 6.7434 s/iter. ETA=0:09:12\n",
            "[05/10 07:29:42 d2.evaluation.evaluator]: Inference done 69/150. Dataloading: 0.0036 s/iter. Inference: 0.2372 s/iter. Eval: 6.4811 s/iter. Total: 6.7239 s/iter. ETA=0:09:04\n",
            "[05/10 07:29:50 d2.evaluation.evaluator]: Inference done 71/150. Dataloading: 0.0035 s/iter. Inference: 0.2364 s/iter. Eval: 6.4005 s/iter. Total: 6.6425 s/iter. ETA=0:08:44\n",
            "[05/10 07:29:59 d2.evaluation.evaluator]: Inference done 73/150. Dataloading: 0.0035 s/iter. Inference: 0.2349 s/iter. Eval: 6.3395 s/iter. Total: 6.5800 s/iter. ETA=0:08:26\n",
            "[05/10 07:30:04 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0035 s/iter. Inference: 0.2346 s/iter. Eval: 6.3169 s/iter. Total: 6.5571 s/iter. ETA=0:08:18\n",
            "[05/10 07:30:09 d2.evaluation.evaluator]: Inference done 75/150. Dataloading: 0.0035 s/iter. Inference: 0.2343 s/iter. Eval: 6.2976 s/iter. Total: 6.5375 s/iter. ETA=0:08:10\n",
            "[05/10 07:30:15 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0035 s/iter. Inference: 0.2326 s/iter. Eval: 6.1124 s/iter. Total: 6.3506 s/iter. ETA=0:07:37\n",
            "[05/10 07:30:22 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0035 s/iter. Inference: 0.2317 s/iter. Eval: 6.0370 s/iter. Total: 6.2742 s/iter. ETA=0:07:19\n",
            "[05/10 07:30:28 d2.evaluation.evaluator]: Inference done 83/150. Dataloading: 0.0035 s/iter. Inference: 0.2297 s/iter. Eval: 5.8776 s/iter. Total: 6.1129 s/iter. ETA=0:06:49\n",
            "[05/10 07:30:34 d2.evaluation.evaluator]: Inference done 85/150. Dataloading: 0.0035 s/iter. Inference: 0.2287 s/iter. Eval: 5.7980 s/iter. Total: 6.0322 s/iter. ETA=0:06:32\n",
            "[05/10 07:30:39 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0034 s/iter. Inference: 0.2285 s/iter. Eval: 5.7937 s/iter. Total: 6.0277 s/iter. ETA=0:06:25\n",
            "[05/10 07:30:45 d2.evaluation.evaluator]: Inference done 89/150. Dataloading: 0.0034 s/iter. Inference: 0.2260 s/iter. Eval: 5.6502 s/iter. Total: 5.8816 s/iter. ETA=0:05:58\n",
            "[05/10 07:30:52 d2.evaluation.evaluator]: Inference done 91/150. Dataloading: 0.0034 s/iter. Inference: 0.2255 s/iter. Eval: 5.5913 s/iter. Total: 5.8222 s/iter. ETA=0:05:43\n",
            "[05/10 07:30:58 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.0034 s/iter. Inference: 0.2237 s/iter. Eval: 5.4687 s/iter. Total: 5.6977 s/iter. ETA=0:05:19\n",
            "[05/10 07:31:05 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0034 s/iter. Inference: 0.2233 s/iter. Eval: 5.4149 s/iter. Total: 5.6435 s/iter. ETA=0:05:04\n",
            "[05/10 07:31:11 d2.evaluation.evaluator]: Inference done 97/150. Dataloading: 0.0034 s/iter. Inference: 0.2232 s/iter. Eval: 5.4215 s/iter. Total: 5.6501 s/iter. ETA=0:04:59\n",
            "[05/10 07:31:21 d2.evaluation.evaluator]: Inference done 99/150. Dataloading: 0.0034 s/iter. Inference: 0.2227 s/iter. Eval: 5.4119 s/iter. Total: 5.6401 s/iter. ETA=0:04:47\n",
            "[05/10 07:31:26 d2.evaluation.evaluator]: Inference done 101/150. Dataloading: 0.0034 s/iter. Inference: 0.2214 s/iter. Eval: 5.3482 s/iter. Total: 5.5749 s/iter. ETA=0:04:33\n",
            "[05/10 07:31:35 d2.evaluation.evaluator]: Inference done 103/150. Dataloading: 0.0035 s/iter. Inference: 0.2213 s/iter. Eval: 5.3217 s/iter. Total: 5.5484 s/iter. ETA=0:04:20\n",
            "[05/10 07:31:40 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0034 s/iter. Inference: 0.2201 s/iter. Eval: 5.2128 s/iter. Total: 5.4383 s/iter. ETA=0:03:59\n",
            "[05/10 07:31:50 d2.evaluation.evaluator]: Inference done 109/150. Dataloading: 0.0034 s/iter. Inference: 0.2184 s/iter. Eval: 5.1535 s/iter. Total: 5.3772 s/iter. ETA=0:03:40\n",
            "[05/10 07:31:59 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0034 s/iter. Inference: 0.2182 s/iter. Eval: 5.1310 s/iter. Total: 5.3546 s/iter. ETA=0:03:28\n",
            "[05/10 07:32:07 d2.evaluation.evaluator]: Inference done 113/150. Dataloading: 0.0034 s/iter. Inference: 0.2181 s/iter. Eval: 5.1105 s/iter. Total: 5.3340 s/iter. ETA=0:03:17\n",
            "[05/10 07:32:15 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0035 s/iter. Inference: 0.2175 s/iter. Eval: 5.0883 s/iter. Total: 5.3113 s/iter. ETA=0:03:05\n",
            "[05/10 07:32:24 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0035 s/iter. Inference: 0.2172 s/iter. Eval: 5.0671 s/iter. Total: 5.2898 s/iter. ETA=0:02:54\n",
            "[05/10 07:32:30 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0035 s/iter. Inference: 0.2166 s/iter. Eval: 5.0278 s/iter. Total: 5.2498 s/iter. ETA=0:02:42\n",
            "[05/10 07:32:38 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0035 s/iter. Inference: 0.2169 s/iter. Eval: 5.0510 s/iter. Total: 5.2733 s/iter. ETA=0:02:38\n",
            "[05/10 07:32:46 d2.evaluation.evaluator]: Inference done 122/150. Dataloading: 0.0034 s/iter. Inference: 0.2162 s/iter. Eval: 5.0328 s/iter. Total: 5.2545 s/iter. ETA=0:02:27\n",
            "[05/10 07:32:54 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0034 s/iter. Inference: 0.2155 s/iter. Eval: 4.9710 s/iter. Total: 5.1919 s/iter. ETA=0:02:09\n",
            "[05/10 07:33:00 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0035 s/iter. Inference: 0.2153 s/iter. Eval: 4.9370 s/iter. Total: 5.1578 s/iter. ETA=0:01:58\n",
            "[05/10 07:33:06 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0035 s/iter. Inference: 0.2148 s/iter. Eval: 4.9022 s/iter. Total: 5.1224 s/iter. ETA=0:01:47\n",
            "[05/10 07:33:13 d2.evaluation.evaluator]: Inference done 136/150. Dataloading: 0.0034 s/iter. Inference: 0.2109 s/iter. Eval: 4.6845 s/iter. Total: 4.9007 s/iter. ETA=0:01:08\n",
            "[05/10 07:33:21 d2.evaluation.evaluator]: Inference done 138/150. Dataloading: 0.0035 s/iter. Inference: 0.2111 s/iter. Eval: 4.6715 s/iter. Total: 4.8880 s/iter. ETA=0:00:58\n",
            "[05/10 07:33:27 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0035 s/iter. Inference: 0.2101 s/iter. Eval: 4.6028 s/iter. Total: 4.8183 s/iter. ETA=0:00:43\n",
            "[05/10 07:33:34 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0035 s/iter. Inference: 0.2082 s/iter. Eval: 4.5172 s/iter. Total: 4.7308 s/iter. ETA=0:00:23\n",
            "[05/10 07:33:39 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0035 s/iter. Inference: 0.2080 s/iter. Eval: 4.4871 s/iter. Total: 4.7004 s/iter. ETA=0:00:14\n",
            "[05/10 07:33:46 d2.evaluation.evaluator]: Inference done 150/150. Dataloading: 0.0034 s/iter. Inference: 0.2072 s/iter. Eval: 4.4400 s/iter. Total: 4.6526 s/iter. ETA=0:00:00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 07:33:46 d2.evaluation.evaluator]: Total inference time: 0:11:15.079947 (4.655724 s / iter per device, on 1 devices)\n",
            "[05/10 07:33:46 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:30 (0.207239 s / iter per device, on 1 devices)\n",
            "[05/10 07:33:47 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/10 07:33:47 d2.evaluation.coco_evaluation]: Saving results to /content/output/chkpt/inference/coco_instances_results.json\n",
            "[05/10 07:33:47 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 07:33:47 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/10 07:33:47 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.10 seconds.\n",
            "[05/10 07:33:47 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 07:33:47 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.04 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[05/10 07:33:47 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[05/10 07:33:47 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP    | category              | AP    | category   | AP    |\n",
            "|:-----------|:------|:----------------------|:------|:-----------|:------|\n",
            "| Bottle     | 0.000 | Bottle cap            | 0.000 | Can        | 0.000 |\n",
            "| Cigarette  | 0.000 | Cup                   | 0.000 | Lid        | 0.000 |\n",
            "| Other      | 0.000 | Plastic bag & wrapper | 0.000 | Pop tab    | 0.000 |\n",
            "| Straw      | 0.000 |                       |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.49s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 07:33:48 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[05/10 07:33:48 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.47 seconds.\n",
            "[05/10 07:33:48 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 07:33:48 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.04 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.077\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.169\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.068\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.015\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.104\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.219\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.224\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.047\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.281\n",
            "[05/10 07:33:49 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 7.737 | 16.911 | 6.802  | 0.000 | 1.456 | 10.448 |\n",
            "[05/10 07:33:49 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category   | AP     | category              | AP     | category   | AP    |\n",
            "|:-----------|:-------|:----------------------|:-------|:-----------|:------|\n",
            "| Bottle     | 21.822 | Bottle cap            | 15.255 | Can        | 4.151 |\n",
            "| Cigarette  | 0.010  | Cup                   | 7.223  | Lid        | 7.332 |\n",
            "| Other      | 5.988  | Plastic bag & wrapper | 14.754 | Pop tab    | 0.000 |\n",
            "| Straw      | 0.835  |                       |        |            |       |\n",
            "[05/10 07:33:49 d2.engine.defaults]: Evaluation results for TACO_val in csv format:\n",
            "[05/10 07:33:49 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[05/10 07:33:49 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 07:33:49 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[05/10 07:33:49 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[05/10 07:33:49 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 07:33:49 d2.evaluation.testing]: copypaste: 7.7369,16.9112,6.8023,0.0000,1.4561,10.4480\n",
            "[05/10 07:33:49 d2.utils.events]:  eta: 3:02:08  iter: 1999  total_loss: 1.472  loss_ins: 1.163  loss_cate: 0.3255  time: 3.4421  data_time: 1.9671  lr: 0.0007571  max_mem: 9678M\n",
            "[05/10 07:34:53 d2.utils.events]:  eta: 3:02:11  iter: 2019  total_loss: 1.308  loss_ins: 1.053  loss_cate: 0.2845  time: 3.3715  data_time: 1.6192  lr: 0.00075272  max_mem: 9678M\n",
            "[05/10 07:35:55 d2.utils.events]:  eta: 3:04:19  iter: 2039  total_loss: 1.3  loss_ins: 0.9957  loss_cate: 0.3025  time: 3.3082  data_time: 1.5307  lr: 0.00074831  max_mem: 9678M\n",
            "[05/10 07:37:04 d2.utils.events]:  eta: 3:06:19  iter: 2059  total_loss: 1.281  loss_ins: 0.9672  loss_cate: 0.322  time: 3.3393  data_time: 1.9391  lr: 0.00074389  max_mem: 9678M\n",
            "[05/10 07:38:08 d2.utils.events]:  eta: 3:04:04  iter: 2079  total_loss: 1.173  loss_ins: 0.9213  loss_cate: 0.2996  time: 3.3139  data_time: 1.6639  lr: 0.00073943  max_mem: 9678M\n",
            "[05/10 07:39:14 d2.utils.events]:  eta: 3:01:43  iter: 2099  total_loss: 1.286  loss_ins: 0.9513  loss_cate: 0.3036  time: 3.2814  data_time: 1.6493  lr: 0.00073496  max_mem: 9678M\n",
            "[05/10 07:40:20 d2.utils.events]:  eta: 3:00:21  iter: 2119  total_loss: 1.168  loss_ins: 0.8917  loss_cate: 0.2764  time: 3.2823  data_time: 1.7660  lr: 0.00073047  max_mem: 9678M\n",
            "[05/10 07:41:26 d2.utils.events]:  eta: 3:01:04  iter: 2139  total_loss: 1.129  loss_ins: 0.839  loss_cate: 0.2878  time: 3.2844  data_time: 1.7783  lr: 0.00072595  max_mem: 9678M\n",
            "[05/10 07:42:30 d2.utils.events]:  eta: 2:58:44  iter: 2159  total_loss: 1.179  loss_ins: 0.8976  loss_cate: 0.279  time: 3.2772  data_time: 1.7192  lr: 0.00072141  max_mem: 9678M\n",
            "[05/10 07:43:33 d2.utils.events]:  eta: 2:57:22  iter: 2179  total_loss: 1.238  loss_ins: 0.9812  loss_cate: 0.269  time: 3.2672  data_time: 1.6318  lr: 0.00071685  max_mem: 9678M\n",
            "[05/10 07:44:41 d2.utils.events]:  eta: 2:56:31  iter: 2199  total_loss: 1.239  loss_ins: 0.948  loss_cate: 0.2917  time: 3.2781  data_time: 1.8914  lr: 0.00071228  max_mem: 9678M\n",
            "[05/10 07:45:49 d2.utils.events]:  eta: 2:55:31  iter: 2219  total_loss: 1.269  loss_ins: 0.9701  loss_cate: 0.2756  time: 3.2846  data_time: 1.8822  lr: 0.00070768  max_mem: 9678M\n",
            "[05/10 07:46:54 d2.utils.events]:  eta: 2:54:44  iter: 2239  total_loss: 1.303  loss_ins: 0.9867  loss_cate: 0.2919  time: 3.2815  data_time: 1.8008  lr: 0.00070306  max_mem: 9678M\n",
            "[05/10 07:48:04 d2.utils.events]:  eta: 2:53:45  iter: 2259  total_loss: 1.181  loss_ins: 0.882  loss_cate: 0.2764  time: 3.2955  data_time: 1.8797  lr: 0.00069843  max_mem: 9678M\n",
            "[05/10 07:49:03 d2.utils.events]:  eta: 2:52:18  iter: 2279  total_loss: 1.062  loss_ins: 0.7971  loss_cate: 0.2592  time: 3.2727  data_time: 1.5038  lr: 0.00069377  max_mem: 9678M\n",
            "[05/10 07:50:18 d2.utils.events]:  eta: 2:52:00  iter: 2299  total_loss: 1.207  loss_ins: 0.9595  loss_cate: 0.2707  time: 3.3003  data_time: 2.1982  lr: 0.0006891  max_mem: 9678M\n",
            "[05/10 07:51:23 d2.utils.events]:  eta: 2:51:00  iter: 2319  total_loss: 1.229  loss_ins: 0.9293  loss_cate: 0.296  time: 3.2992  data_time: 1.7892  lr: 0.00068442  max_mem: 9678M\n",
            "[05/10 07:52:29 d2.utils.events]:  eta: 2:50:11  iter: 2339  total_loss: 1.041  loss_ins: 0.7833  loss_cate: 0.2555  time: 3.2981  data_time: 1.7105  lr: 0.00067972  max_mem: 9678M\n",
            "[05/10 07:53:32 d2.utils.events]:  eta: 2:51:06  iter: 2359  total_loss: 1.331  loss_ins: 1.008  loss_cate: 0.285  time: 3.2914  data_time: 1.6173  lr: 0.000675  max_mem: 9678M\n",
            "[05/10 07:54:45 d2.utils.events]:  eta: 2:51:13  iter: 2379  total_loss: 1.22  loss_ins: 0.9586  loss_cate: 0.2599  time: 3.3073  data_time: 2.0074  lr: 0.00067027  max_mem: 9678M\n",
            "[05/10 07:55:48 d2.utils.events]:  eta: 2:50:39  iter: 2399  total_loss: 1.007  loss_ins: 0.7421  loss_cate: 0.2531  time: 3.2954  data_time: 1.5659  lr: 0.00066552  max_mem: 9678M\n",
            "[05/10 07:56:52 d2.utils.events]:  eta: 2:48:08  iter: 2419  total_loss: 1.189  loss_ins: 0.9339  loss_cate: 0.2472  time: 3.2896  data_time: 1.6156  lr: 0.00066076  max_mem: 9678M\n",
            "[05/10 07:57:59 d2.utils.events]:  eta: 2:46:50  iter: 2439  total_loss: 1.255  loss_ins: 1.006  loss_cate: 0.2825  time: 3.2920  data_time: 1.7294  lr: 0.00065598  max_mem: 9678M\n",
            "[05/10 07:59:07 d2.utils.events]:  eta: 2:45:19  iter: 2459  total_loss: 1.213  loss_ins: 0.9349  loss_cate: 0.2699  time: 3.2972  data_time: 1.8672  lr: 0.0006512  max_mem: 9678M\n",
            "[05/10 08:00:12 d2.utils.events]:  eta: 2:44:49  iter: 2479  total_loss: 0.9918  loss_ins: 0.7071  loss_cate: 0.2697  time: 3.2945  data_time: 1.7647  lr: 0.0006464  max_mem: 9678M\n",
            "[05/10 08:01:19 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/TACO-expl/data/annotations_off_0_val.json\n",
            "[05/10 08:01:19 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/10 08:01:19 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[05/10 08:01:19 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "WARNING [05/10 08:01:19 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[05/10 08:01:19 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 08:01:56 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0028 s/iter. Inference: 0.2040 s/iter. Eval: 4.3886 s/iter. Total: 4.5955 s/iter. ETA=0:10:38\n",
            "[05/10 08:02:01 d2.evaluation.evaluator]: Inference done 13/150. Dataloading: 0.0027 s/iter. Inference: 0.2028 s/iter. Eval: 3.8762 s/iter. Total: 4.0821 s/iter. ETA=0:09:19\n",
            "[05/10 08:02:16 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0038 s/iter. Inference: 0.2079 s/iter. Eval: 4.5357 s/iter. Total: 4.7480 s/iter. ETA=0:10:40\n",
            "[05/10 08:02:22 d2.evaluation.evaluator]: Inference done 18/150. Dataloading: 0.0046 s/iter. Inference: 0.2041 s/iter. Eval: 3.8998 s/iter. Total: 4.1092 s/iter. ETA=0:09:02\n",
            "[05/10 08:02:27 d2.evaluation.evaluator]: Inference done 20/150. Dataloading: 0.0044 s/iter. Inference: 0.1992 s/iter. Eval: 3.7039 s/iter. Total: 3.9081 s/iter. ETA=0:08:28\n",
            "[05/10 08:02:33 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0044 s/iter. Inference: 0.1964 s/iter. Eval: 3.8379 s/iter. Total: 4.0394 s/iter. ETA=0:08:41\n",
            "[05/10 08:02:42 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0044 s/iter. Inference: 0.1986 s/iter. Eval: 3.8816 s/iter. Total: 4.0854 s/iter. ETA=0:08:38\n",
            "[05/10 08:02:49 d2.evaluation.evaluator]: Inference done 25/150. Dataloading: 0.0048 s/iter. Inference: 0.2017 s/iter. Eval: 3.7939 s/iter. Total: 4.0012 s/iter. ETA=0:08:20\n",
            "[05/10 08:03:01 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0053 s/iter. Inference: 0.2029 s/iter. Eval: 3.9736 s/iter. Total: 4.1827 s/iter. ETA=0:08:34\n",
            "[05/10 08:03:12 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0056 s/iter. Inference: 0.2041 s/iter. Eval: 4.2881 s/iter. Total: 4.4988 s/iter. ETA=0:09:08\n",
            "[05/10 08:03:21 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0055 s/iter. Inference: 0.2056 s/iter. Eval: 4.4791 s/iter. Total: 4.6915 s/iter. ETA=0:09:27\n",
            "[05/10 08:03:28 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0053 s/iter. Inference: 0.2034 s/iter. Eval: 4.3884 s/iter. Total: 4.5983 s/iter. ETA=0:09:07\n",
            "[05/10 08:03:42 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0072 s/iter. Inference: 0.2035 s/iter. Eval: 4.3736 s/iter. Total: 4.5857 s/iter. ETA=0:08:51\n",
            "[05/10 08:03:48 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0071 s/iter. Inference: 0.2041 s/iter. Eval: 4.4205 s/iter. Total: 4.6331 s/iter. ETA=0:08:52\n",
            "[05/10 08:03:55 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0068 s/iter. Inference: 0.2053 s/iter. Eval: 4.3547 s/iter. Total: 4.5683 s/iter. ETA=0:08:36\n",
            "[05/10 08:04:06 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0066 s/iter. Inference: 0.2058 s/iter. Eval: 4.4205 s/iter. Total: 4.6343 s/iter. ETA=0:08:34\n",
            "[05/10 08:04:15 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0067 s/iter. Inference: 0.2064 s/iter. Eval: 4.5467 s/iter. Total: 4.7612 s/iter. ETA=0:08:43\n",
            "[05/10 08:04:23 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0066 s/iter. Inference: 0.2064 s/iter. Eval: 4.6153 s/iter. Total: 4.8299 s/iter. ETA=0:08:46\n",
            "[05/10 08:04:30 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0065 s/iter. Inference: 0.2064 s/iter. Eval: 4.6840 s/iter. Total: 4.8984 s/iter. ETA=0:08:49\n",
            "[05/10 08:04:37 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0065 s/iter. Inference: 0.2058 s/iter. Eval: 4.6138 s/iter. Total: 4.8278 s/iter. ETA=0:08:31\n",
            "[05/10 08:04:45 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0066 s/iter. Inference: 0.2060 s/iter. Eval: 4.5686 s/iter. Total: 4.7829 s/iter. ETA=0:08:17\n",
            "[05/10 08:04:50 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0065 s/iter. Inference: 0.2063 s/iter. Eval: 4.5811 s/iter. Total: 4.7958 s/iter. ETA=0:08:13\n",
            "[05/10 08:04:58 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0065 s/iter. Inference: 0.2066 s/iter. Eval: 4.6464 s/iter. Total: 4.8613 s/iter. ETA=0:08:15\n",
            "[05/10 08:05:04 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0064 s/iter. Inference: 0.2065 s/iter. Eval: 4.5762 s/iter. Total: 4.7909 s/iter. ETA=0:07:59\n",
            "[05/10 08:05:10 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0065 s/iter. Inference: 0.2064 s/iter. Eval: 4.4845 s/iter. Total: 4.6991 s/iter. ETA=0:07:40\n",
            "[05/10 08:05:16 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0065 s/iter. Inference: 0.2066 s/iter. Eval: 4.5137 s/iter. Total: 4.7286 s/iter. ETA=0:07:38\n",
            "[05/10 08:05:23 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0062 s/iter. Inference: 0.2046 s/iter. Eval: 4.3692 s/iter. Total: 4.5817 s/iter. ETA=0:07:10\n",
            "[05/10 08:05:31 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0063 s/iter. Inference: 0.2048 s/iter. Eval: 4.3543 s/iter. Total: 4.5670 s/iter. ETA=0:07:00\n",
            "[05/10 08:05:36 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0060 s/iter. Inference: 0.2033 s/iter. Eval: 4.1308 s/iter. Total: 4.3418 s/iter. ETA=0:06:22\n",
            "[05/10 08:05:42 d2.evaluation.evaluator]: Inference done 66/150. Dataloading: 0.0058 s/iter. Inference: 0.2017 s/iter. Eval: 3.9441 s/iter. Total: 4.1533 s/iter. ETA=0:05:48\n",
            "[05/10 08:05:53 d2.evaluation.evaluator]: Inference done 69/150. Dataloading: 0.0057 s/iter. Inference: 0.2003 s/iter. Eval: 3.9206 s/iter. Total: 4.1282 s/iter. ETA=0:05:34\n",
            "[05/10 08:05:59 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0056 s/iter. Inference: 0.2001 s/iter. Eval: 3.8255 s/iter. Total: 4.0327 s/iter. ETA=0:05:14\n",
            "[05/10 08:06:04 d2.evaluation.evaluator]: Inference done 73/150. Dataloading: 0.0055 s/iter. Inference: 0.1998 s/iter. Eval: 3.8444 s/iter. Total: 4.0513 s/iter. ETA=0:05:11\n",
            "[05/10 08:06:12 d2.evaluation.evaluator]: Inference done 75/150. Dataloading: 0.0054 s/iter. Inference: 0.2000 s/iter. Eval: 3.8377 s/iter. Total: 4.0449 s/iter. ETA=0:05:03\n",
            "[05/10 08:06:22 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0052 s/iter. Inference: 0.1971 s/iter. Eval: 3.7075 s/iter. Total: 3.9115 s/iter. ETA=0:04:33\n",
            "[05/10 08:06:29 d2.evaluation.evaluator]: Inference done 83/150. Dataloading: 0.0052 s/iter. Inference: 0.1971 s/iter. Eval: 3.6458 s/iter. Total: 3.8497 s/iter. ETA=0:04:17\n",
            "[05/10 08:06:39 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0052 s/iter. Inference: 0.1968 s/iter. Eval: 3.6256 s/iter. Total: 3.8292 s/iter. ETA=0:04:05\n",
            "[05/10 08:06:46 d2.evaluation.evaluator]: Inference done 89/150. Dataloading: 0.0051 s/iter. Inference: 0.1961 s/iter. Eval: 3.5698 s/iter. Total: 3.7726 s/iter. ETA=0:03:50\n",
            "[05/10 08:06:51 d2.evaluation.evaluator]: Inference done 91/150. Dataloading: 0.0051 s/iter. Inference: 0.1965 s/iter. Eval: 3.5412 s/iter. Total: 3.7443 s/iter. ETA=0:03:40\n",
            "[05/10 08:06:57 d2.evaluation.evaluator]: Inference done 93/150. Dataloading: 0.0051 s/iter. Inference: 0.1965 s/iter. Eval: 3.5307 s/iter. Total: 3.7338 s/iter. ETA=0:03:32\n",
            "[05/10 08:07:06 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0050 s/iter. Inference: 0.1963 s/iter. Eval: 3.4982 s/iter. Total: 3.7011 s/iter. ETA=0:03:19\n",
            "[05/10 08:07:12 d2.evaluation.evaluator]: Inference done 98/150. Dataloading: 0.0050 s/iter. Inference: 0.1967 s/iter. Eval: 3.4854 s/iter. Total: 3.6887 s/iter. ETA=0:03:11\n",
            "[05/10 08:07:20 d2.evaluation.evaluator]: Inference done 99/150. Dataloading: 0.0049 s/iter. Inference: 0.1973 s/iter. Eval: 3.5372 s/iter. Total: 3.7410 s/iter. ETA=0:03:10\n",
            "[05/10 08:07:29 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0049 s/iter. Inference: 0.1973 s/iter. Eval: 3.5122 s/iter. Total: 3.7160 s/iter. ETA=0:02:58\n",
            "[05/10 08:07:35 d2.evaluation.evaluator]: Inference done 105/150. Dataloading: 0.0049 s/iter. Inference: 0.1971 s/iter. Eval: 3.4590 s/iter. Total: 3.6625 s/iter. ETA=0:02:44\n",
            "[05/10 08:07:41 d2.evaluation.evaluator]: Inference done 108/150. Dataloading: 0.0048 s/iter. Inference: 0.1952 s/iter. Eval: 3.4110 s/iter. Total: 3.6126 s/iter. ETA=0:02:31\n",
            "[05/10 08:07:47 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0048 s/iter. Inference: 0.1951 s/iter. Eval: 3.3961 s/iter. Total: 3.5975 s/iter. ETA=0:02:23\n",
            "[05/10 08:07:52 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0048 s/iter. Inference: 0.1955 s/iter. Eval: 3.4160 s/iter. Total: 3.6179 s/iter. ETA=0:02:21\n",
            "[05/10 08:08:01 d2.evaluation.evaluator]: Inference done 113/150. Dataloading: 0.0048 s/iter. Inference: 0.1957 s/iter. Eval: 3.4271 s/iter. Total: 3.6291 s/iter. ETA=0:02:14\n",
            "[05/10 08:08:09 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0048 s/iter. Inference: 0.1955 s/iter. Eval: 3.4363 s/iter. Total: 3.6381 s/iter. ETA=0:02:07\n",
            "[05/10 08:08:17 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0047 s/iter. Inference: 0.1957 s/iter. Eval: 3.4417 s/iter. Total: 3.6436 s/iter. ETA=0:02:00\n",
            "[05/10 08:08:23 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0047 s/iter. Inference: 0.1957 s/iter. Eval: 3.4269 s/iter. Total: 3.6289 s/iter. ETA=0:01:52\n",
            "[05/10 08:08:29 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0047 s/iter. Inference: 0.1961 s/iter. Eval: 3.4528 s/iter. Total: 3.6551 s/iter. ETA=0:01:49\n",
            "[05/10 08:08:36 d2.evaluation.evaluator]: Inference done 124/150. Dataloading: 0.0046 s/iter. Inference: 0.1948 s/iter. Eval: 3.3892 s/iter. Total: 3.5902 s/iter. ETA=0:01:33\n",
            "[05/10 08:08:44 d2.evaluation.evaluator]: Inference done 126/150. Dataloading: 0.0047 s/iter. Inference: 0.1945 s/iter. Eval: 3.3934 s/iter. Total: 3.5941 s/iter. ETA=0:01:26\n",
            "[05/10 08:08:52 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0046 s/iter. Inference: 0.1944 s/iter. Eval: 3.3729 s/iter. Total: 3.5734 s/iter. ETA=0:01:15\n",
            "[05/10 08:08:57 d2.evaluation.evaluator]: Inference done 132/150. Dataloading: 0.0046 s/iter. Inference: 0.1935 s/iter. Eval: 3.3291 s/iter. Total: 3.5287 s/iter. ETA=0:01:03\n",
            "[05/10 08:09:06 d2.evaluation.evaluator]: Inference done 138/150. Dataloading: 0.0045 s/iter. Inference: 0.1918 s/iter. Eval: 3.2365 s/iter. Total: 3.4342 s/iter. ETA=0:00:41\n",
            "[05/10 08:09:12 d2.evaluation.evaluator]: Inference done 142/150. Dataloading: 0.0045 s/iter. Inference: 0.1915 s/iter. Eval: 3.1797 s/iter. Total: 3.3771 s/iter. ETA=0:00:27\n",
            "[05/10 08:09:18 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0044 s/iter. Inference: 0.1902 s/iter. Eval: 3.1557 s/iter. Total: 3.3517 s/iter. ETA=0:00:16\n",
            "[05/10 08:09:25 d2.evaluation.evaluator]: Inference done 148/150. Dataloading: 0.0044 s/iter. Inference: 0.1898 s/iter. Eval: 3.1336 s/iter. Total: 3.3292 s/iter. ETA=0:00:06\n",
            "[05/10 08:09:27 d2.evaluation.evaluator]: Total inference time: 0:07:58.100213 (3.297243 s / iter per device, on 1 devices)\n",
            "[05/10 08:09:27 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:27 (0.189312 s / iter per device, on 1 devices)\n",
            "[05/10 08:09:27 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/10 08:09:27 d2.evaluation.coco_evaluation]: Saving results to /content/output/chkpt/inference/coco_instances_results.json\n",
            "[05/10 08:09:27 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 08:09:27 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/10 08:09:27 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.08 seconds.\n",
            "[05/10 08:09:27 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 08:09:28 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.04 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[05/10 08:09:28 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[05/10 08:09:28 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP    | category              | AP    | category   | AP    |\n",
            "|:-----------|:------|:----------------------|:------|:-----------|:------|\n",
            "| Bottle     | 0.000 | Bottle cap            | 0.000 | Can        | 0.000 |\n",
            "| Cigarette  | 0.000 | Cup                   | 0.000 | Lid        | 0.000 |\n",
            "| Other      | 0.000 | Plastic bag & wrapper | 0.000 | Pop tab    | 0.000 |\n",
            "| Straw      | 0.000 |                       |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.36s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 08:09:28 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[05/10 08:09:28 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.39 seconds.\n",
            "[05/10 08:09:28 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 08:09:28 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.04 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.088\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.064\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.062\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.114\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.180\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.230\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.234\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.094\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.278\n",
            "[05/10 08:09:28 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 8.817 | 19.375 | 6.412  | 0.000 | 6.170 | 11.354 |\n",
            "[05/10 08:09:28 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category   | AP     | category              | AP     | category   | AP    |\n",
            "|:-----------|:-------|:----------------------|:-------|:-----------|:------|\n",
            "| Bottle     | 18.075 | Bottle cap            | 14.042 | Can        | 6.929 |\n",
            "| Cigarette  | 0.165  | Cup                   | 11.507 | Lid        | 8.148 |\n",
            "| Other      | 7.197  | Plastic bag & wrapper | 17.369 | Pop tab    | 0.545 |\n",
            "| Straw      | 4.193  |                       |        |            |       |\n",
            "[05/10 08:09:28 d2.engine.defaults]: Evaluation results for TACO_val in csv format:\n",
            "[05/10 08:09:28 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[05/10 08:09:28 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 08:09:28 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[05/10 08:09:28 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[05/10 08:09:28 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 08:09:28 d2.evaluation.testing]: copypaste: 8.8169,19.3745,6.4117,0.0000,6.1696,11.3544\n",
            "[05/10 08:09:28 d2.utils.events]:  eta: 2:43:18  iter: 2499  total_loss: 1.203  loss_ins: 0.962  loss_cate: 0.2928  time: 3.2962  data_time: 1.8007  lr: 0.00064159  max_mem: 9678M\n",
            "[05/10 08:10:29 d2.utils.events]:  eta: 2:41:50  iter: 2519  total_loss: 1.005  loss_ins: 0.7515  loss_cate: 0.243  time: 3.2872  data_time: 1.5424  lr: 0.00063677  max_mem: 9678M\n",
            "[05/10 08:11:36 d2.utils.events]:  eta: 2:40:50  iter: 2539  total_loss: 1.128  loss_ins: 0.8399  loss_cate: 0.2665  time: 3.2879  data_time: 1.8199  lr: 0.00063193  max_mem: 9678M\n",
            "[05/10 08:12:37 d2.utils.events]:  eta: 2:39:31  iter: 2559  total_loss: 1.046  loss_ins: 0.8332  loss_cate: 0.2546  time: 3.2794  data_time: 1.6023  lr: 0.00062709  max_mem: 9678M\n",
            "[05/10 08:13:48 d2.utils.events]:  eta: 2:38:31  iter: 2579  total_loss: 1.083  loss_ins: 0.8568  loss_cate: 0.2317  time: 3.2874  data_time: 1.9943  lr: 0.00062224  max_mem: 9678M\n",
            "[05/10 08:14:53 d2.utils.events]:  eta: 2:37:31  iter: 2599  total_loss: 1.242  loss_ins: 0.9492  loss_cate: 0.2592  time: 3.2861  data_time: 1.6561  lr: 0.00061738  max_mem: 9678M\n",
            "[05/10 08:16:00 d2.utils.events]:  eta: 2:37:16  iter: 2619  total_loss: 1.232  loss_ins: 0.9394  loss_cate: 0.2712  time: 3.2877  data_time: 1.8110  lr: 0.00061251  max_mem: 9678M\n",
            "[05/10 08:17:04 d2.utils.events]:  eta: 2:36:58  iter: 2639  total_loss: 1.102  loss_ins: 0.8308  loss_cate: 0.2634  time: 3.2849  data_time: 1.5821  lr: 0.00060764  max_mem: 9678M\n",
            "[05/10 08:18:08 d2.utils.events]:  eta: 2:36:00  iter: 2659  total_loss: 1.057  loss_ins: 0.7928  loss_cate: 0.2425  time: 3.2836  data_time: 1.7452  lr: 0.00060276  max_mem: 9678M\n",
            "[05/10 08:19:13 d2.utils.events]:  eta: 2:34:44  iter: 2679  total_loss: 1.012  loss_ins: 0.7537  loss_cate: 0.2547  time: 3.2823  data_time: 1.7668  lr: 0.00059787  max_mem: 9678M\n",
            "[05/10 08:20:22 d2.utils.events]:  eta: 2:33:56  iter: 2699  total_loss: 0.99  loss_ins: 0.7662  loss_cate: 0.2274  time: 3.2845  data_time: 1.8436  lr: 0.00059298  max_mem: 9678M\n",
            "[05/10 08:21:28 d2.utils.events]:  eta: 2:32:43  iter: 2719  total_loss: 1.072  loss_ins: 0.8753  loss_cate: 0.2288  time: 3.2852  data_time: 1.7580  lr: 0.00058808  max_mem: 9678M\n",
            "[05/10 08:22:32 d2.utils.events]:  eta: 2:31:42  iter: 2739  total_loss: 1.11  loss_ins: 0.8961  loss_cate: 0.2455  time: 3.2820  data_time: 1.6302  lr: 0.00058317  max_mem: 9678M\n",
            "[05/10 08:23:37 d2.utils.events]:  eta: 2:30:54  iter: 2759  total_loss: 1.084  loss_ins: 0.8475  loss_cate: 0.2493  time: 3.2810  data_time: 1.7569  lr: 0.00057827  max_mem: 9678M\n",
            "[05/10 08:24:46 d2.utils.events]:  eta: 2:29:56  iter: 2779  total_loss: 1.013  loss_ins: 0.7398  loss_cate: 0.2433  time: 3.2854  data_time: 1.8941  lr: 0.00057335  max_mem: 9678M\n",
            "[05/10 08:25:49 d2.utils.events]:  eta: 2:29:06  iter: 2799  total_loss: 0.9794  loss_ins: 0.7623  loss_cate: 0.2183  time: 3.2830  data_time: 1.5931  lr: 0.00056844  max_mem: 9678M\n",
            "[05/10 08:26:55 d2.utils.events]:  eta: 2:27:55  iter: 2819  total_loss: 0.9813  loss_ins: 0.7702  loss_cate: 0.2317  time: 3.2824  data_time: 1.7304  lr: 0.00056353  max_mem: 9678M\n",
            "[05/10 08:27:57 d2.utils.events]:  eta: 2:26:52  iter: 2839  total_loss: 1.22  loss_ins: 0.9162  loss_cate: 0.2589  time: 3.2782  data_time: 1.4847  lr: 0.00055861  max_mem: 9678M\n",
            "[05/10 08:29:08 d2.utils.events]:  eta: 2:25:54  iter: 2859  total_loss: 1.021  loss_ins: 0.7949  loss_cate: 0.2327  time: 3.2835  data_time: 1.9498  lr: 0.00055369  max_mem: 9678M\n",
            "[05/10 08:30:12 d2.utils.events]:  eta: 2:24:12  iter: 2879  total_loss: 0.9429  loss_ins: 0.7488  loss_cate: 0.2105  time: 3.2816  data_time: 1.7160  lr: 0.00054877  max_mem: 9678M\n",
            "[05/10 08:31:14 d2.utils.events]:  eta: 2:22:47  iter: 2899  total_loss: 0.808  loss_ins: 0.5921  loss_cate: 0.2188  time: 3.2784  data_time: 1.7854  lr: 0.00054385  max_mem: 9678M\n",
            "[05/10 08:32:18 d2.utils.events]:  eta: 2:21:47  iter: 2919  total_loss: 1.101  loss_ins: 0.8361  loss_cate: 0.2422  time: 3.2764  data_time: 1.6447  lr: 0.00053893  max_mem: 9678M\n",
            "[05/10 08:33:27 d2.utils.events]:  eta: 2:20:47  iter: 2939  total_loss: 1.023  loss_ins: 0.8221  loss_cate: 0.2165  time: 3.2801  data_time: 1.8331  lr: 0.00053402  max_mem: 9678M\n",
            "[05/10 08:34:32 d2.utils.events]:  eta: 2:19:30  iter: 2959  total_loss: 1.051  loss_ins: 0.8258  loss_cate: 0.2508  time: 3.2794  data_time: 1.7321  lr: 0.0005291  max_mem: 9678M\n",
            "[05/10 08:35:39 d2.utils.events]:  eta: 2:18:46  iter: 2979  total_loss: 1.138  loss_ins: 0.8578  loss_cate: 0.2639  time: 3.2807  data_time: 1.7239  lr: 0.00052419  max_mem: 9678M\n",
            "[05/10 08:36:48 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/TACO-expl/data/annotations_off_0_val.json\n",
            "[05/10 08:36:48 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/10 08:36:48 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[05/10 08:36:48 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "WARNING [05/10 08:36:48 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[05/10 08:36:48 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 08:37:21 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0023 s/iter. Inference: 0.2129 s/iter. Eval: 4.2606 s/iter. Total: 4.4759 s/iter. ETA=0:10:22\n",
            "[05/10 08:37:26 d2.evaluation.evaluator]: Inference done 12/150. Dataloading: 0.0025 s/iter. Inference: 0.2134 s/iter. Eval: 4.3381 s/iter. Total: 4.5550 s/iter. ETA=0:10:28\n",
            "[05/10 08:37:40 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0042 s/iter. Inference: 0.2162 s/iter. Eval: 4.3072 s/iter. Total: 4.5287 s/iter. ETA=0:10:11\n",
            "[05/10 08:37:49 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0040 s/iter. Inference: 0.2142 s/iter. Eval: 3.6855 s/iter. Total: 3.9048 s/iter. ETA=0:08:31\n",
            "[05/10 08:37:59 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0038 s/iter. Inference: 0.2106 s/iter. Eval: 3.7991 s/iter. Total: 4.0145 s/iter. ETA=0:08:37\n",
            "[05/10 08:38:07 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0039 s/iter. Inference: 0.2118 s/iter. Eval: 4.0601 s/iter. Total: 4.2770 s/iter. ETA=0:09:07\n",
            "[05/10 08:38:13 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0038 s/iter. Inference: 0.2118 s/iter. Eval: 4.1560 s/iter. Total: 4.3732 s/iter. ETA=0:09:15\n",
            "[05/10 08:38:20 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0038 s/iter. Inference: 0.2125 s/iter. Eval: 4.2584 s/iter. Total: 4.4762 s/iter. ETA=0:09:24\n",
            "[05/10 08:38:27 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0036 s/iter. Inference: 0.2135 s/iter. Eval: 4.1930 s/iter. Total: 4.4115 s/iter. ETA=0:09:07\n",
            "[05/10 08:38:35 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0036 s/iter. Inference: 0.2153 s/iter. Eval: 4.3414 s/iter. Total: 4.5620 s/iter. ETA=0:09:21\n",
            "[05/10 08:38:46 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0036 s/iter. Inference: 0.2162 s/iter. Eval: 4.6477 s/iter. Total: 4.8693 s/iter. ETA=0:09:54\n",
            "[05/10 08:38:56 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0037 s/iter. Inference: 0.2172 s/iter. Eval: 4.8588 s/iter. Total: 5.0815 s/iter. ETA=0:10:14\n",
            "[05/10 08:39:06 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0044 s/iter. Inference: 0.2193 s/iter. Eval: 4.8333 s/iter. Total: 5.0588 s/iter. ETA=0:10:01\n",
            "[05/10 08:39:12 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0042 s/iter. Inference: 0.2170 s/iter. Eval: 4.6721 s/iter. Total: 4.8950 s/iter. ETA=0:09:32\n",
            "[05/10 08:39:22 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0045 s/iter. Inference: 0.2174 s/iter. Eval: 4.8531 s/iter. Total: 5.0767 s/iter. ETA=0:09:48\n",
            "[05/10 08:39:27 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0045 s/iter. Inference: 0.2168 s/iter. Eval: 4.8658 s/iter. Total: 5.0888 s/iter. ETA=0:09:45\n",
            "[05/10 08:39:37 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0044 s/iter. Inference: 0.2185 s/iter. Eval: 4.8465 s/iter. Total: 5.0713 s/iter. ETA=0:09:33\n",
            "[05/10 08:39:47 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0043 s/iter. Inference: 0.2245 s/iter. Eval: 4.9918 s/iter. Total: 5.2225 s/iter. ETA=0:09:44\n",
            "[05/10 08:39:55 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0043 s/iter. Inference: 0.2250 s/iter. Eval: 5.0706 s/iter. Total: 5.3019 s/iter. ETA=0:09:48\n",
            "[05/10 08:40:08 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0042 s/iter. Inference: 0.2311 s/iter. Eval: 5.2890 s/iter. Total: 5.5264 s/iter. ETA=0:10:07\n",
            "[05/10 08:40:15 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0042 s/iter. Inference: 0.2307 s/iter. Eval: 5.3332 s/iter. Total: 5.5701 s/iter. ETA=0:10:07\n",
            "[05/10 08:40:25 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0042 s/iter. Inference: 0.2313 s/iter. Eval: 5.4555 s/iter. Total: 5.6932 s/iter. ETA=0:10:14\n",
            "[05/10 08:40:33 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0041 s/iter. Inference: 0.2305 s/iter. Eval: 5.3751 s/iter. Total: 5.6119 s/iter. ETA=0:09:54\n",
            "[05/10 08:40:41 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0040 s/iter. Inference: 0.2291 s/iter. Eval: 5.2924 s/iter. Total: 5.5277 s/iter. ETA=0:09:34\n",
            "[05/10 08:40:50 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0040 s/iter. Inference: 0.2290 s/iter. Eval: 5.3726 s/iter. Total: 5.6077 s/iter. ETA=0:09:37\n",
            "[05/10 08:41:02 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0040 s/iter. Inference: 0.2292 s/iter. Eval: 5.5176 s/iter. Total: 5.7531 s/iter. ETA=0:09:46\n",
            "[05/10 08:41:09 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0039 s/iter. Inference: 0.2278 s/iter. Eval: 5.4248 s/iter. Total: 5.6587 s/iter. ETA=0:09:25\n",
            "[05/10 08:41:19 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0041 s/iter. Inference: 0.2261 s/iter. Eval: 5.2843 s/iter. Total: 5.5166 s/iter. ETA=0:08:55\n",
            "[05/10 08:41:25 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0040 s/iter. Inference: 0.2251 s/iter. Eval: 5.1805 s/iter. Total: 5.4117 s/iter. ETA=0:08:34\n",
            "[05/10 08:41:34 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0040 s/iter. Inference: 0.2244 s/iter. Eval: 5.1372 s/iter. Total: 5.3678 s/iter. ETA=0:08:19\n",
            "[05/10 08:41:40 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0040 s/iter. Inference: 0.2243 s/iter. Eval: 5.1569 s/iter. Total: 5.3872 s/iter. ETA=0:08:15\n",
            "[05/10 08:41:47 d2.evaluation.evaluator]: Inference done 61/150. Dataloading: 0.0042 s/iter. Inference: 0.2235 s/iter. Eval: 4.9884 s/iter. Total: 5.2181 s/iter. ETA=0:07:44\n",
            "[05/10 08:41:52 d2.evaluation.evaluator]: Inference done 65/150. Dataloading: 0.0044 s/iter. Inference: 0.2244 s/iter. Eval: 4.7271 s/iter. Total: 4.9580 s/iter. ETA=0:07:01\n",
            "[05/10 08:42:01 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0043 s/iter. Inference: 0.2239 s/iter. Eval: 4.6280 s/iter. Total: 4.8583 s/iter. ETA=0:06:38\n",
            "[05/10 08:42:08 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0043 s/iter. Inference: 0.2231 s/iter. Eval: 4.5953 s/iter. Total: 4.8247 s/iter. ETA=0:06:25\n",
            "[05/10 08:42:17 d2.evaluation.evaluator]: Inference done 73/150. Dataloading: 0.0042 s/iter. Inference: 0.2222 s/iter. Eval: 4.5119 s/iter. Total: 4.7403 s/iter. ETA=0:06:05\n",
            "[05/10 08:42:24 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0042 s/iter. Inference: 0.2227 s/iter. Eval: 4.5434 s/iter. Total: 4.7722 s/iter. ETA=0:06:02\n",
            "[05/10 08:42:30 d2.evaluation.evaluator]: Inference done 77/150. Dataloading: 0.0041 s/iter. Inference: 0.2213 s/iter. Eval: 4.4268 s/iter. Total: 4.6542 s/iter. ETA=0:05:39\n",
            "[05/10 08:42:37 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0041 s/iter. Inference: 0.2188 s/iter. Eval: 4.3377 s/iter. Total: 4.5625 s/iter. ETA=0:05:19\n",
            "[05/10 08:42:44 d2.evaluation.evaluator]: Inference done 83/150. Dataloading: 0.0040 s/iter. Inference: 0.2172 s/iter. Eval: 4.2547 s/iter. Total: 4.4779 s/iter. ETA=0:05:00\n",
            "[05/10 08:42:49 d2.evaluation.evaluator]: Inference done 85/150. Dataloading: 0.0040 s/iter. Inference: 0.2173 s/iter. Eval: 4.2132 s/iter. Total: 4.4364 s/iter. ETA=0:04:48\n",
            "[05/10 08:42:56 d2.evaluation.evaluator]: Inference done 87/150. Dataloading: 0.0041 s/iter. Inference: 0.2175 s/iter. Eval: 4.1905 s/iter. Total: 4.4140 s/iter. ETA=0:04:38\n",
            "[05/10 08:43:03 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0041 s/iter. Inference: 0.2160 s/iter. Eval: 4.1180 s/iter. Total: 4.3400 s/iter. ETA=0:04:20\n",
            "[05/10 08:43:11 d2.evaluation.evaluator]: Inference done 93/150. Dataloading: 0.0040 s/iter. Inference: 0.2156 s/iter. Eval: 4.0588 s/iter. Total: 4.2803 s/iter. ETA=0:04:03\n",
            "[05/10 08:43:18 d2.evaluation.evaluator]: Inference done 95/150. Dataloading: 0.0040 s/iter. Inference: 0.2151 s/iter. Eval: 4.0416 s/iter. Total: 4.2626 s/iter. ETA=0:03:54\n",
            "[05/10 08:43:26 d2.evaluation.evaluator]: Inference done 97/150. Dataloading: 0.0040 s/iter. Inference: 0.2150 s/iter. Eval: 4.0315 s/iter. Total: 4.2523 s/iter. ETA=0:03:45\n",
            "[05/10 08:43:37 d2.evaluation.evaluator]: Inference done 99/150. Dataloading: 0.0039 s/iter. Inference: 0.2145 s/iter. Eval: 4.0602 s/iter. Total: 4.2806 s/iter. ETA=0:03:38\n",
            "[05/10 08:43:45 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0039 s/iter. Inference: 0.2140 s/iter. Eval: 4.0079 s/iter. Total: 4.2277 s/iter. ETA=0:03:22\n",
            "[05/10 08:43:51 d2.evaluation.evaluator]: Inference done 105/150. Dataloading: 0.0039 s/iter. Inference: 0.2130 s/iter. Eval: 3.9448 s/iter. Total: 4.1634 s/iter. ETA=0:03:07\n",
            "[05/10 08:44:00 d2.evaluation.evaluator]: Inference done 109/150. Dataloading: 0.0038 s/iter. Inference: 0.2112 s/iter. Eval: 3.8783 s/iter. Total: 4.0951 s/iter. ETA=0:02:47\n",
            "[05/10 08:44:06 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0039 s/iter. Inference: 0.2111 s/iter. Eval: 3.8536 s/iter. Total: 4.0703 s/iter. ETA=0:02:38\n",
            "[05/10 08:44:14 d2.evaluation.evaluator]: Inference done 113/150. Dataloading: 0.0039 s/iter. Inference: 0.2111 s/iter. Eval: 3.8486 s/iter. Total: 4.0653 s/iter. ETA=0:02:30\n",
            "[05/10 08:44:23 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0038 s/iter. Inference: 0.2104 s/iter. Eval: 3.8588 s/iter. Total: 4.0749 s/iter. ETA=0:02:22\n",
            "[05/10 08:44:33 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0038 s/iter. Inference: 0.2106 s/iter. Eval: 3.8803 s/iter. Total: 4.0965 s/iter. ETA=0:02:15\n",
            "[05/10 08:44:39 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0038 s/iter. Inference: 0.2106 s/iter. Eval: 3.8574 s/iter. Total: 4.0737 s/iter. ETA=0:02:06\n",
            "[05/10 08:44:47 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0038 s/iter. Inference: 0.2106 s/iter. Eval: 3.8886 s/iter. Total: 4.1048 s/iter. ETA=0:02:03\n",
            "[05/10 08:44:52 d2.evaluation.evaluator]: Inference done 122/150. Dataloading: 0.0038 s/iter. Inference: 0.2103 s/iter. Eval: 3.8683 s/iter. Total: 4.0843 s/iter. ETA=0:01:54\n",
            "[05/10 08:45:00 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0038 s/iter. Inference: 0.2102 s/iter. Eval: 3.8314 s/iter. Total: 4.0472 s/iter. ETA=0:01:41\n",
            "[05/10 08:45:05 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0038 s/iter. Inference: 0.2099 s/iter. Eval: 3.8092 s/iter. Total: 4.0248 s/iter. ETA=0:01:32\n",
            "[05/10 08:45:11 d2.evaluation.evaluator]: Inference done 130/150. Dataloading: 0.0038 s/iter. Inference: 0.2089 s/iter. Eval: 3.7548 s/iter. Total: 3.9694 s/iter. ETA=0:01:19\n",
            "[05/10 08:45:16 d2.evaluation.evaluator]: Inference done 134/150. Dataloading: 0.0038 s/iter. Inference: 0.2080 s/iter. Eval: 3.6759 s/iter. Total: 3.8895 s/iter. ETA=0:01:02\n",
            "[05/10 08:45:23 d2.evaluation.evaluator]: Inference done 137/150. Dataloading: 0.0039 s/iter. Inference: 0.2088 s/iter. Eval: 3.6369 s/iter. Total: 3.8514 s/iter. ETA=0:00:50\n",
            "[05/10 08:45:29 d2.evaluation.evaluator]: Inference done 139/150. Dataloading: 0.0039 s/iter. Inference: 0.2092 s/iter. Eval: 3.6251 s/iter. Total: 3.8401 s/iter. ETA=0:00:42\n",
            "[05/10 08:45:36 d2.evaluation.evaluator]: Inference done 144/150. Dataloading: 0.0039 s/iter. Inference: 0.2074 s/iter. Eval: 3.5363 s/iter. Total: 3.7495 s/iter. ETA=0:00:22\n",
            "[05/10 08:45:41 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0039 s/iter. Inference: 0.2074 s/iter. Eval: 3.5460 s/iter. Total: 3.7591 s/iter. ETA=0:00:18\n",
            "[05/10 08:45:47 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0039 s/iter. Inference: 0.2071 s/iter. Eval: 3.5344 s/iter. Total: 3.7472 s/iter. ETA=0:00:11\n",
            "[05/10 08:45:52 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0039 s/iter. Inference: 0.2069 s/iter. Eval: 3.5224 s/iter. Total: 3.7350 s/iter. ETA=0:00:03\n",
            "[05/10 08:45:55 d2.evaluation.evaluator]: Total inference time: 0:09:00.397588 (3.726880 s / iter per device, on 1 devices)\n",
            "[05/10 08:45:55 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:29 (0.206471 s / iter per device, on 1 devices)\n",
            "[05/10 08:45:55 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/10 08:45:55 d2.evaluation.coco_evaluation]: Saving results to /content/output/chkpt/inference/coco_instances_results.json\n",
            "[05/10 08:45:56 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 08:45:56 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/10 08:45:56 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.17 seconds.\n",
            "[05/10 08:45:56 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 08:45:56 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.07 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[05/10 08:45:56 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[05/10 08:45:56 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP    | category              | AP    | category   | AP    |\n",
            "|:-----------|:------|:----------------------|:------|:-----------|:------|\n",
            "| Bottle     | 0.000 | Bottle cap            | 0.000 | Can        | 0.000 |\n",
            "| Cigarette  | 0.000 | Cup                   | 0.000 | Lid        | 0.000 |\n",
            "| Other      | 0.000 | Plastic bag & wrapper | 0.000 | Pop tab    | 0.000 |\n",
            "| Straw      | 0.000 |                       |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.81s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 08:45:57 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[05/10 08:45:58 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.71 seconds.\n",
            "[05/10 08:45:58 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 08:45:58 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.07 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.113\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.104\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.034\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.158\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.202\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.260\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.268\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.082\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.351\n",
            "[05/10 08:45:58 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 11.259 | 21.301 | 10.423 | 0.000 | 3.431 | 15.772 |\n",
            "[05/10 08:45:58 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category   | AP     | category              | AP     | category   | AP     |\n",
            "|:-----------|:-------|:----------------------|:-------|:-----------|:-------|\n",
            "| Bottle     | 23.805 | Bottle cap            | 24.180 | Can        | 10.526 |\n",
            "| Cigarette  | 1.031  | Cup                   | 14.802 | Lid        | 9.588  |\n",
            "| Other      | 6.920  | Plastic bag & wrapper | 18.451 | Pop tab    | 1.089  |\n",
            "| Straw      | 2.201  |                       |        |            |        |\n",
            "[05/10 08:45:58 d2.engine.defaults]: Evaluation results for TACO_val in csv format:\n",
            "[05/10 08:45:58 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[05/10 08:45:58 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 08:45:58 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[05/10 08:45:58 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[05/10 08:45:58 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 08:45:58 d2.evaluation.testing]: copypaste: 11.2592,21.3006,10.4227,0.0000,3.4313,15.7721\n",
            "[05/10 08:45:58 d2.utils.events]:  eta: 2:17:30  iter: 2999  total_loss: 0.89  loss_ins: 0.6638  loss_cate: 0.2254  time: 3.2828  data_time: 1.8523  lr: 0.00051928  max_mem: 9678M\n",
            "[05/10 08:47:06 d2.utils.events]:  eta: 2:16:46  iter: 3019  total_loss: 0.9785  loss_ins: 0.7669  loss_cate: 0.2346  time: 3.2846  data_time: 1.7103  lr: 0.00051437  max_mem: 9678M\n",
            "[05/10 08:48:09 d2.utils.events]:  eta: 2:15:30  iter: 3039  total_loss: 0.7796  loss_ins: 0.5646  loss_cate: 0.2154  time: 3.2823  data_time: 1.6730  lr: 0.00050947  max_mem: 9678M\n",
            "[05/10 08:49:11 d2.utils.events]:  eta: 2:13:51  iter: 3059  total_loss: 0.8766  loss_ins: 0.6666  loss_cate: 0.217  time: 3.2789  data_time: 1.5938  lr: 0.00050458  max_mem: 9678M\n",
            "[05/10 08:50:20 d2.utils.events]:  eta: 2:13:17  iter: 3079  total_loss: 0.9795  loss_ins: 0.7831  loss_cate: 0.2116  time: 3.2823  data_time: 1.8211  lr: 0.00049969  max_mem: 9678M\n",
            "[05/10 08:51:29 d2.utils.events]:  eta: 2:13:15  iter: 3099  total_loss: 1.009  loss_ins: 0.7739  loss_cate: 0.2163  time: 3.2849  data_time: 1.8226  lr: 0.0004948  max_mem: 9678M\n",
            "[05/10 08:52:38 d2.utils.events]:  eta: 2:12:53  iter: 3119  total_loss: 0.9542  loss_ins: 0.7272  loss_cate: 0.2314  time: 3.2881  data_time: 2.0080  lr: 0.00048992  max_mem: 9678M\n",
            "[05/10 08:53:44 d2.utils.events]:  eta: 2:11:14  iter: 3139  total_loss: 1.057  loss_ins: 0.8446  loss_cate: 0.2349  time: 3.2879  data_time: 1.7183  lr: 0.00048505  max_mem: 9678M\n",
            "[05/10 08:54:59 d2.utils.events]:  eta: 2:11:09  iter: 3159  total_loss: 1.025  loss_ins: 0.8337  loss_cate: 0.2072  time: 3.2909  data_time: 1.8295  lr: 0.00048019  max_mem: 9678M\n",
            "[05/10 08:56:03 d2.utils.events]:  eta: 2:10:26  iter: 3179  total_loss: 1.045  loss_ins: 0.7702  loss_cate: 0.2207  time: 3.2897  data_time: 1.6369  lr: 0.00047533  max_mem: 9678M\n",
            "[05/10 08:57:08 d2.utils.events]:  eta: 2:09:16  iter: 3199  total_loss: 0.8226  loss_ins: 0.6533  loss_cate: 0.2167  time: 3.2890  data_time: 1.6500  lr: 0.00047049  max_mem: 9678M\n",
            "[05/10 08:58:15 d2.utils.events]:  eta: 2:08:15  iter: 3219  total_loss: 0.9409  loss_ins: 0.7403  loss_cate: 0.2051  time: 3.2899  data_time: 1.7904  lr: 0.00046565  max_mem: 9678M\n",
            "[05/10 08:59:23 d2.utils.events]:  eta: 2:07:05  iter: 3239  total_loss: 0.8832  loss_ins: 0.657  loss_cate: 0.2196  time: 3.2915  data_time: 1.8002  lr: 0.00046082  max_mem: 9678M\n",
            "[05/10 09:00:28 d2.utils.events]:  eta: 2:06:04  iter: 3259  total_loss: 0.9059  loss_ins: 0.6702  loss_cate: 0.2148  time: 3.2909  data_time: 1.7331  lr: 0.00045601  max_mem: 9678M\n",
            "[05/10 09:01:37 d2.utils.events]:  eta: 2:05:21  iter: 3279  total_loss: 1.128  loss_ins: 0.875  loss_cate: 0.2245  time: 3.2930  data_time: 1.9142  lr: 0.0004512  max_mem: 9678M\n",
            "[05/10 09:02:43 d2.utils.events]:  eta: 2:04:20  iter: 3299  total_loss: 0.8875  loss_ins: 0.6902  loss_cate: 0.2076  time: 3.2908  data_time: 1.5873  lr: 0.00044641  max_mem: 9678M\n",
            "[05/10 09:03:52 d2.utils.events]:  eta: 2:03:11  iter: 3319  total_loss: 0.8319  loss_ins: 0.645  loss_cate: 0.1989  time: 3.2922  data_time: 1.8253  lr: 0.00044163  max_mem: 9678M\n",
            "[05/10 09:04:56 d2.utils.events]:  eta: 2:01:53  iter: 3339  total_loss: 0.83  loss_ins: 0.6364  loss_cate: 0.2048  time: 3.2910  data_time: 1.5544  lr: 0.00043686  max_mem: 9678M\n",
            "[05/10 09:06:01 d2.utils.events]:  eta: 2:00:19  iter: 3359  total_loss: 0.935  loss_ins: 0.6981  loss_cate: 0.2085  time: 3.2903  data_time: 1.6678  lr: 0.00043211  max_mem: 9678M\n",
            "[05/10 09:07:04 d2.utils.events]:  eta: 1:59:06  iter: 3379  total_loss: 0.9983  loss_ins: 0.7795  loss_cate: 0.2142  time: 3.2888  data_time: 1.6098  lr: 0.00042737  max_mem: 9678M\n",
            "[05/10 09:08:20 d2.utils.events]:  eta: 1:58:18  iter: 3399  total_loss: 0.9082  loss_ins: 0.6862  loss_cate: 0.2125  time: 3.2954  data_time: 2.1806  lr: 0.00042264  max_mem: 9678M\n",
            "[05/10 09:09:22 d2.utils.events]:  eta: 1:57:28  iter: 3419  total_loss: 0.7322  loss_ins: 0.5347  loss_cate: 0.2036  time: 3.2930  data_time: 1.7424  lr: 0.00041793  max_mem: 9678M\n",
            "[05/10 09:10:27 d2.utils.events]:  eta: 1:56:57  iter: 3439  total_loss: 0.8562  loss_ins: 0.6618  loss_cate: 0.2011  time: 3.2922  data_time: 1.6533  lr: 0.00041324  max_mem: 9678M\n",
            "[05/10 09:11:35 d2.utils.events]:  eta: 1:56:04  iter: 3459  total_loss: 0.832  loss_ins: 0.6213  loss_cate: 0.2098  time: 3.2902  data_time: 1.5735  lr: 0.00040856  max_mem: 9678M\n",
            "[05/10 09:12:39 d2.utils.events]:  eta: 1:55:04  iter: 3479  total_loss: 0.7482  loss_ins: 0.5818  loss_cate: 0.1842  time: 3.2895  data_time: 1.6424  lr: 0.0004039  max_mem: 9678M\n",
            "[05/10 09:13:46 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/TACO-expl/data/annotations_off_0_val.json\n",
            "[05/10 09:13:46 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/10 09:13:46 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[05/10 09:13:46 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "WARNING [05/10 09:13:46 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[05/10 09:13:46 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 09:14:20 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0024 s/iter. Inference: 0.1846 s/iter. Eval: 4.1170 s/iter. Total: 4.3040 s/iter. ETA=0:09:58\n",
            "[05/10 09:14:25 d2.evaluation.evaluator]: Inference done 13/150. Dataloading: 0.0024 s/iter. Inference: 0.1871 s/iter. Eval: 3.7135 s/iter. Total: 3.9034 s/iter. ETA=0:08:54\n",
            "[05/10 09:14:34 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0024 s/iter. Inference: 0.1938 s/iter. Eval: 3.7927 s/iter. Total: 3.9894 s/iter. ETA=0:08:58\n",
            "[05/10 09:14:40 d2.evaluation.evaluator]: Inference done 18/150. Dataloading: 0.0031 s/iter. Inference: 0.1982 s/iter. Eval: 3.2827 s/iter. Total: 3.4848 s/iter. ETA=0:07:39\n",
            "[05/10 09:14:46 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0032 s/iter. Inference: 0.1999 s/iter. Eval: 3.4561 s/iter. Total: 3.6604 s/iter. ETA=0:07:59\n",
            "[05/10 09:14:54 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0031 s/iter. Inference: 0.1963 s/iter. Eval: 3.5432 s/iter. Total: 3.7438 s/iter. ETA=0:08:02\n",
            "[05/10 09:15:00 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0030 s/iter. Inference: 0.1971 s/iter. Eval: 3.6742 s/iter. Total: 3.8758 s/iter. ETA=0:08:16\n",
            "[05/10 09:15:08 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0035 s/iter. Inference: 0.1993 s/iter. Eval: 3.8682 s/iter. Total: 4.0727 s/iter. ETA=0:08:37\n",
            "[05/10 09:15:15 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0034 s/iter. Inference: 0.2015 s/iter. Eval: 4.0238 s/iter. Total: 4.2306 s/iter. ETA=0:08:53\n",
            "[05/10 09:15:22 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0038 s/iter. Inference: 0.2023 s/iter. Eval: 3.9879 s/iter. Total: 4.1959 s/iter. ETA=0:08:40\n",
            "[05/10 09:15:28 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0042 s/iter. Inference: 0.2040 s/iter. Eval: 4.0592 s/iter. Total: 4.2696 s/iter. ETA=0:08:45\n",
            "[05/10 09:15:39 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0042 s/iter. Inference: 0.2063 s/iter. Eval: 4.3500 s/iter. Total: 4.5628 s/iter. ETA=0:09:16\n",
            "[05/10 09:15:49 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0044 s/iter. Inference: 0.2076 s/iter. Eval: 4.5596 s/iter. Total: 4.7738 s/iter. ETA=0:09:37\n",
            "[05/10 09:15:58 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0050 s/iter. Inference: 0.2086 s/iter. Eval: 4.5589 s/iter. Total: 4.7749 s/iter. ETA=0:09:28\n",
            "[05/10 09:16:07 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0057 s/iter. Inference: 0.2091 s/iter. Eval: 4.5246 s/iter. Total: 4.7416 s/iter. ETA=0:09:14\n",
            "[05/10 09:16:16 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0061 s/iter. Inference: 0.2098 s/iter. Eval: 4.6807 s/iter. Total: 4.8988 s/iter. ETA=0:09:28\n",
            "[05/10 09:16:24 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0059 s/iter. Inference: 0.2104 s/iter. Eval: 4.7747 s/iter. Total: 4.9933 s/iter. ETA=0:09:34\n",
            "[05/10 09:16:35 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0060 s/iter. Inference: 0.2119 s/iter. Eval: 4.8042 s/iter. Total: 5.0244 s/iter. ETA=0:09:27\n",
            "[05/10 09:16:42 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0061 s/iter. Inference: 0.2126 s/iter. Eval: 4.8623 s/iter. Total: 5.0835 s/iter. ETA=0:09:29\n",
            "[05/10 09:16:48 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0060 s/iter. Inference: 0.2129 s/iter. Eval: 4.8946 s/iter. Total: 5.1160 s/iter. ETA=0:09:27\n",
            "[05/10 09:16:59 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0062 s/iter. Inference: 0.2149 s/iter. Eval: 5.0543 s/iter. Total: 5.2778 s/iter. ETA=0:09:40\n",
            "[05/10 09:17:08 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0063 s/iter. Inference: 0.2158 s/iter. Eval: 5.1601 s/iter. Total: 5.3848 s/iter. ETA=0:09:46\n",
            "[05/10 09:17:18 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0063 s/iter. Inference: 0.2163 s/iter. Eval: 5.2689 s/iter. Total: 5.4942 s/iter. ETA=0:09:53\n",
            "[05/10 09:17:28 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0061 s/iter. Inference: 0.2163 s/iter. Eval: 5.2680 s/iter. Total: 5.4929 s/iter. ETA=0:09:42\n",
            "[05/10 09:17:36 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0060 s/iter. Inference: 0.2161 s/iter. Eval: 5.3130 s/iter. Total: 5.5377 s/iter. ETA=0:09:41\n",
            "[05/10 09:17:41 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0060 s/iter. Inference: 0.2162 s/iter. Eval: 5.3043 s/iter. Total: 5.5291 s/iter. ETA=0:09:35\n",
            "[05/10 09:17:47 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0059 s/iter. Inference: 0.2160 s/iter. Eval: 5.3064 s/iter. Total: 5.5309 s/iter. ETA=0:09:29\n",
            "[05/10 09:17:57 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0058 s/iter. Inference: 0.2160 s/iter. Eval: 5.4254 s/iter. Total: 5.6499 s/iter. ETA=0:09:36\n",
            "[05/10 09:18:02 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0059 s/iter. Inference: 0.2166 s/iter. Eval: 5.4164 s/iter. Total: 5.6414 s/iter. ETA=0:09:29\n",
            "[05/10 09:18:08 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0058 s/iter. Inference: 0.2154 s/iter. Eval: 5.2965 s/iter. Total: 5.5203 s/iter. ETA=0:09:06\n",
            "[05/10 09:18:16 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0057 s/iter. Inference: 0.2137 s/iter. Eval: 5.2298 s/iter. Total: 5.4518 s/iter. ETA=0:08:48\n",
            "[05/10 09:18:22 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0057 s/iter. Inference: 0.2130 s/iter. Eval: 5.0320 s/iter. Total: 5.2532 s/iter. ETA=0:08:13\n",
            "[05/10 09:18:29 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0056 s/iter. Inference: 0.2124 s/iter. Eval: 5.0556 s/iter. Total: 5.2763 s/iter. ETA=0:08:10\n",
            "[05/10 09:18:34 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0055 s/iter. Inference: 0.2119 s/iter. Eval: 4.9615 s/iter. Total: 5.1814 s/iter. ETA=0:07:51\n",
            "[05/10 09:18:40 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0053 s/iter. Inference: 0.2066 s/iter. Eval: 4.7151 s/iter. Total: 4.9295 s/iter. ETA=0:07:08\n",
            "[05/10 09:18:46 d2.evaluation.evaluator]: Inference done 66/150. Dataloading: 0.0052 s/iter. Inference: 0.2050 s/iter. Eval: 4.5628 s/iter. Total: 4.7754 s/iter. ETA=0:06:41\n",
            "[05/10 09:18:52 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0051 s/iter. Inference: 0.2051 s/iter. Eval: 4.5184 s/iter. Total: 4.7309 s/iter. ETA=0:06:27\n",
            "[05/10 09:18:58 d2.evaluation.evaluator]: Inference done 69/150. Dataloading: 0.0052 s/iter. Inference: 0.2057 s/iter. Eval: 4.5279 s/iter. Total: 4.7412 s/iter. ETA=0:06:24\n",
            "[05/10 09:19:04 d2.evaluation.evaluator]: Inference done 71/150. Dataloading: 0.0051 s/iter. Inference: 0.2053 s/iter. Eval: 4.4730 s/iter. Total: 4.6858 s/iter. ETA=0:06:10\n",
            "[05/10 09:19:11 d2.evaluation.evaluator]: Inference done 73/150. Dataloading: 0.0051 s/iter. Inference: 0.2044 s/iter. Eval: 4.4443 s/iter. Total: 4.6561 s/iter. ETA=0:05:58\n",
            "[05/10 09:19:18 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0051 s/iter. Inference: 0.2048 s/iter. Eval: 4.4791 s/iter. Total: 4.6915 s/iter. ETA=0:05:56\n",
            "[05/10 09:19:24 d2.evaluation.evaluator]: Inference done 77/150. Dataloading: 0.0050 s/iter. Inference: 0.2046 s/iter. Eval: 4.3630 s/iter. Total: 4.5749 s/iter. ETA=0:05:33\n",
            "[05/10 09:19:30 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0049 s/iter. Inference: 0.2017 s/iter. Eval: 4.2734 s/iter. Total: 4.4822 s/iter. ETA=0:05:13\n",
            "[05/10 09:19:37 d2.evaluation.evaluator]: Inference done 83/150. Dataloading: 0.0048 s/iter. Inference: 0.2013 s/iter. Eval: 4.1828 s/iter. Total: 4.3911 s/iter. ETA=0:04:54\n",
            "[05/10 09:19:42 d2.evaluation.evaluator]: Inference done 85/150. Dataloading: 0.0048 s/iter. Inference: 0.2013 s/iter. Eval: 4.1381 s/iter. Total: 4.3463 s/iter. ETA=0:04:42\n",
            "[05/10 09:19:48 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0048 s/iter. Inference: 0.2013 s/iter. Eval: 4.1615 s/iter. Total: 4.3698 s/iter. ETA=0:04:39\n",
            "[05/10 09:19:54 d2.evaluation.evaluator]: Inference done 89/150. Dataloading: 0.0047 s/iter. Inference: 0.2000 s/iter. Eval: 4.0718 s/iter. Total: 4.2787 s/iter. ETA=0:04:20\n",
            "[05/10 09:20:00 d2.evaluation.evaluator]: Inference done 91/150. Dataloading: 0.0046 s/iter. Inference: 0.1999 s/iter. Eval: 4.0473 s/iter. Total: 4.2540 s/iter. ETA=0:04:10\n",
            "[05/10 09:20:07 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.0046 s/iter. Inference: 0.1996 s/iter. Eval: 3.9820 s/iter. Total: 4.1884 s/iter. ETA=0:03:54\n",
            "[05/10 09:20:14 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0046 s/iter. Inference: 0.2001 s/iter. Eval: 3.9676 s/iter. Total: 4.1744 s/iter. ETA=0:03:45\n",
            "[05/10 09:20:20 d2.evaluation.evaluator]: Inference done 98/150. Dataloading: 0.0046 s/iter. Inference: 0.2005 s/iter. Eval: 3.9369 s/iter. Total: 4.1441 s/iter. ETA=0:03:35\n",
            "[05/10 09:20:29 d2.evaluation.evaluator]: Inference done 99/150. Dataloading: 0.0045 s/iter. Inference: 0.2010 s/iter. Eval: 3.9933 s/iter. Total: 4.2010 s/iter. ETA=0:03:34\n",
            "[05/10 09:20:36 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0045 s/iter. Inference: 0.2004 s/iter. Eval: 3.9379 s/iter. Total: 4.1449 s/iter. ETA=0:03:18\n",
            "[05/10 09:20:42 d2.evaluation.evaluator]: Inference done 104/150. Dataloading: 0.0046 s/iter. Inference: 0.2006 s/iter. Eval: 3.9107 s/iter. Total: 4.1180 s/iter. ETA=0:03:09\n",
            "[05/10 09:20:48 d2.evaluation.evaluator]: Inference done 107/150. Dataloading: 0.0045 s/iter. Inference: 0.1992 s/iter. Eval: 3.8455 s/iter. Total: 4.0513 s/iter. ETA=0:02:54\n",
            "[05/10 09:20:54 d2.evaluation.evaluator]: Inference done 109/150. Dataloading: 0.0045 s/iter. Inference: 0.1987 s/iter. Eval: 3.8326 s/iter. Total: 4.0378 s/iter. ETA=0:02:45\n",
            "[05/10 09:21:01 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0045 s/iter. Inference: 0.1985 s/iter. Eval: 3.8171 s/iter. Total: 4.0221 s/iter. ETA=0:02:36\n",
            "[05/10 09:21:08 d2.evaluation.evaluator]: Inference done 113/150. Dataloading: 0.0044 s/iter. Inference: 0.1989 s/iter. Eval: 3.8105 s/iter. Total: 4.0160 s/iter. ETA=0:02:28\n",
            "[05/10 09:21:18 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0044 s/iter. Inference: 0.1985 s/iter. Eval: 3.8252 s/iter. Total: 4.0302 s/iter. ETA=0:02:21\n",
            "[05/10 09:21:28 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0044 s/iter. Inference: 0.1993 s/iter. Eval: 3.8439 s/iter. Total: 4.0497 s/iter. ETA=0:02:13\n",
            "[05/10 09:21:34 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0044 s/iter. Inference: 0.1994 s/iter. Eval: 3.8240 s/iter. Total: 4.0299 s/iter. ETA=0:02:04\n",
            "[05/10 09:21:42 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0044 s/iter. Inference: 0.1996 s/iter. Eval: 3.8593 s/iter. Total: 4.0654 s/iter. ETA=0:02:01\n",
            "[05/10 09:21:48 d2.evaluation.evaluator]: Inference done 122/150. Dataloading: 0.0044 s/iter. Inference: 0.1990 s/iter. Eval: 3.8447 s/iter. Total: 4.0502 s/iter. ETA=0:01:53\n",
            "[05/10 09:21:55 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0044 s/iter. Inference: 0.1992 s/iter. Eval: 3.8032 s/iter. Total: 4.0088 s/iter. ETA=0:01:40\n",
            "[05/10 09:22:02 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0044 s/iter. Inference: 0.1991 s/iter. Eval: 3.7902 s/iter. Total: 3.9958 s/iter. ETA=0:01:31\n",
            "[05/10 09:22:07 d2.evaluation.evaluator]: Inference done 130/150. Dataloading: 0.0044 s/iter. Inference: 0.1983 s/iter. Eval: 3.7390 s/iter. Total: 3.9436 s/iter. ETA=0:01:18\n",
            "[05/10 09:22:13 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0044 s/iter. Inference: 0.1976 s/iter. Eval: 3.6320 s/iter. Total: 3.8358 s/iter. ETA=0:00:57\n",
            "[05/10 09:22:22 d2.evaluation.evaluator]: Inference done 138/150. Dataloading: 0.0044 s/iter. Inference: 0.1977 s/iter. Eval: 3.6108 s/iter. Total: 3.8147 s/iter. ETA=0:00:45\n",
            "[05/10 09:22:27 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0045 s/iter. Inference: 0.1984 s/iter. Eval: 3.5687 s/iter. Total: 3.7735 s/iter. ETA=0:00:33\n",
            "[05/10 09:22:37 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0045 s/iter. Inference: 0.1972 s/iter. Eval: 3.5287 s/iter. Total: 3.7323 s/iter. ETA=0:00:18\n",
            "[05/10 09:22:43 d2.evaluation.evaluator]: Inference done 148/150. Dataloading: 0.0045 s/iter. Inference: 0.1966 s/iter. Eval: 3.4975 s/iter. Total: 3.7004 s/iter. ETA=0:00:07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 09:22:47 d2.evaluation.evaluator]: Total inference time: 0:08:52.923383 (3.675334 s / iter per device, on 1 devices)\n",
            "[05/10 09:22:47 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:28 (0.196062 s / iter per device, on 1 devices)\n",
            "[05/10 09:22:47 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/10 09:22:47 d2.evaluation.coco_evaluation]: Saving results to /content/output/chkpt/inference/coco_instances_results.json\n",
            "[05/10 09:22:48 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 09:22:48 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/10 09:22:48 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.07 seconds.\n",
            "[05/10 09:22:48 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 09:22:48 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.04 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[05/10 09:22:48 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[05/10 09:22:48 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP    | category              | AP    | category   | AP    |\n",
            "|:-----------|:------|:----------------------|:------|:-----------|:------|\n",
            "| Bottle     | 0.000 | Bottle cap            | 0.000 | Can        | 0.000 |\n",
            "| Cigarette  | 0.000 | Cup                   | 0.000 | Lid        | 0.000 |\n",
            "| Other      | 0.000 | Plastic bag & wrapper | 0.000 | Pop tab    | 0.000 |\n",
            "| Straw      | 0.000 |                       |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.38s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 09:22:48 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[05/10 09:22:49 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.36 seconds.\n",
            "[05/10 09:22:49 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 09:22:49 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.04 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.106\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.090\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.036\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.151\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.276\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.282\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.112\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            "[05/10 09:22:49 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 10.633 | 20.491 | 9.008  | 0.000 | 3.567 | 15.123 |\n",
            "[05/10 09:22:49 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category   | AP     | category              | AP     | category   | AP    |\n",
            "|:-----------|:-------|:----------------------|:-------|:-----------|:------|\n",
            "| Bottle     | 24.771 | Bottle cap            | 24.684 | Can        | 9.739 |\n",
            "| Cigarette  | 0.491  | Cup                   | 12.448 | Lid        | 4.299 |\n",
            "| Other      | 6.878  | Plastic bag & wrapper | 20.772 | Pop tab    | 0.182 |\n",
            "| Straw      | 2.063  |                       |        |            |       |\n",
            "[05/10 09:22:49 d2.engine.defaults]: Evaluation results for TACO_val in csv format:\n",
            "[05/10 09:22:49 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[05/10 09:22:49 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 09:22:49 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[05/10 09:22:49 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[05/10 09:22:49 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 09:22:49 d2.evaluation.testing]: copypaste: 10.6326,20.4906,9.0080,0.0000,3.5674,15.1232\n",
            "[05/10 09:22:49 d2.utils.events]:  eta: 1:54:03  iter: 3499  total_loss: 1.123  loss_ins: 0.864  loss_cate: 0.2225  time: 3.2899  data_time: 1.7623  lr: 0.00039925  max_mem: 9678M\n",
            "[05/10 09:23:54 d2.utils.events]:  eta: 1:53:10  iter: 3519  total_loss: 0.651  loss_ins: 0.4657  loss_cate: 0.1686  time: 3.2897  data_time: 1.7507  lr: 0.00039463  max_mem: 9678M\n",
            "[05/10 09:25:02 d2.utils.events]:  eta: 1:52:09  iter: 3539  total_loss: 0.8826  loss_ins: 0.6784  loss_cate: 0.1958  time: 3.2910  data_time: 1.8850  lr: 0.00039002  max_mem: 9678M\n",
            "[05/10 09:26:06 d2.utils.events]:  eta: 1:50:57  iter: 3559  total_loss: 0.9608  loss_ins: 0.7195  loss_cate: 0.1941  time: 3.2893  data_time: 1.6907  lr: 0.00038543  max_mem: 9678M\n",
            "[05/10 09:27:11 d2.utils.events]:  eta: 1:50:07  iter: 3579  total_loss: 0.903  loss_ins: 0.6487  loss_cate: 0.2103  time: 3.2892  data_time: 1.7287  lr: 0.00038086  max_mem: 9678M\n",
            "[05/10 09:28:20 d2.utils.events]:  eta: 1:48:55  iter: 3599  total_loss: 0.8501  loss_ins: 0.6443  loss_cate: 0.2121  time: 3.2878  data_time: 1.5867  lr: 0.00037632  max_mem: 9678M\n",
            "[05/10 09:29:26 d2.utils.events]:  eta: 1:47:54  iter: 3619  total_loss: 0.8468  loss_ins: 0.6502  loss_cate: 0.2095  time: 3.2877  data_time: 1.6639  lr: 0.00037179  max_mem: 9678M\n",
            "[05/10 09:30:28 d2.utils.events]:  eta: 1:46:21  iter: 3639  total_loss: 0.8295  loss_ins: 0.6039  loss_cate: 0.2085  time: 3.2858  data_time: 1.6176  lr: 0.00036728  max_mem: 9678M\n",
            "[05/10 09:31:32 d2.utils.events]:  eta: 1:45:09  iter: 3659  total_loss: 0.738  loss_ins: 0.5542  loss_cate: 0.1755  time: 3.2843  data_time: 1.6533  lr: 0.0003628  max_mem: 9678M\n",
            "[05/10 09:32:33 d2.utils.events]:  eta: 1:44:16  iter: 3679  total_loss: 1.001  loss_ins: 0.7979  loss_cate: 0.2103  time: 3.2820  data_time: 1.6333  lr: 0.00035834  max_mem: 9678M\n",
            "[05/10 09:33:44 d2.utils.events]:  eta: 1:43:08  iter: 3699  total_loss: 0.7724  loss_ins: 0.5942  loss_cate: 0.1838  time: 3.2846  data_time: 1.8229  lr: 0.0003539  max_mem: 9678M\n",
            "[05/10 09:34:53 d2.utils.events]:  eta: 1:42:39  iter: 3719  total_loss: 0.9389  loss_ins: 0.7158  loss_cate: 0.2001  time: 3.2867  data_time: 1.7929  lr: 0.00034948  max_mem: 9678M\n",
            "[05/10 09:36:01 d2.utils.events]:  eta: 1:41:44  iter: 3739  total_loss: 0.7962  loss_ins: 0.6025  loss_cate: 0.2086  time: 3.2878  data_time: 1.9011  lr: 0.00034509  max_mem: 9678M\n",
            "[05/10 09:37:07 d2.utils.events]:  eta: 1:40:52  iter: 3759  total_loss: 0.9102  loss_ins: 0.7309  loss_cate: 0.2032  time: 3.2868  data_time: 1.5780  lr: 0.00034072  max_mem: 9678M\n",
            "[05/10 09:38:17 d2.utils.events]:  eta: 1:40:04  iter: 3779  total_loss: 0.7962  loss_ins: 0.6214  loss_cate: 0.184  time: 3.2892  data_time: 1.9209  lr: 0.00033638  max_mem: 9678M\n",
            "[05/10 09:39:20 d2.utils.events]:  eta: 1:38:56  iter: 3799  total_loss: 0.8795  loss_ins: 0.6649  loss_cate: 0.2012  time: 3.2873  data_time: 1.5158  lr: 0.00033207  max_mem: 9678M\n",
            "[05/10 09:40:28 d2.utils.events]:  eta: 1:37:50  iter: 3819  total_loss: 0.8057  loss_ins: 0.6173  loss_cate: 0.1884  time: 3.2887  data_time: 1.9211  lr: 0.00032777  max_mem: 9678M\n",
            "[05/10 09:41:30 d2.utils.events]:  eta: 1:36:49  iter: 3839  total_loss: 0.7873  loss_ins: 0.5964  loss_cate: 0.1822  time: 3.2869  data_time: 1.5141  lr: 0.00032351  max_mem: 9678M\n",
            "[05/10 09:42:38 d2.utils.events]:  eta: 1:36:00  iter: 3859  total_loss: 0.7686  loss_ins: 0.5778  loss_cate: 0.1961  time: 3.2880  data_time: 1.7401  lr: 0.00031927  max_mem: 9678M\n",
            "[05/10 09:43:44 d2.utils.events]:  eta: 1:35:31  iter: 3879  total_loss: 0.81  loss_ins: 0.6283  loss_cate: 0.1793  time: 3.2877  data_time: 1.7347  lr: 0.00031506  max_mem: 9678M\n",
            "[05/10 09:44:54 d2.utils.events]:  eta: 1:34:57  iter: 3899  total_loss: 0.9229  loss_ins: 0.7184  loss_cate: 0.2028  time: 3.2873  data_time: 1.5590  lr: 0.00031088  max_mem: 9678M\n",
            "[05/10 09:46:06 d2.utils.events]:  eta: 1:34:00  iter: 3919  total_loss: 0.7303  loss_ins: 0.5911  loss_cate: 0.1946  time: 3.2902  data_time: 2.0324  lr: 0.00030673  max_mem: 9678M\n",
            "[05/10 09:47:08 d2.utils.events]:  eta: 1:32:54  iter: 3939  total_loss: 0.9226  loss_ins: 0.7235  loss_cate: 0.1935  time: 3.2887  data_time: 1.4670  lr: 0.00030261  max_mem: 9678M\n",
            "[05/10 09:48:12 d2.utils.events]:  eta: 1:32:09  iter: 3959  total_loss: 0.7302  loss_ins: 0.5497  loss_cate: 0.1775  time: 3.2876  data_time: 1.6543  lr: 0.00029851  max_mem: 9678M\n",
            "[05/10 09:49:18 d2.utils.events]:  eta: 1:31:42  iter: 3979  total_loss: 0.8141  loss_ins: 0.6234  loss_cate: 0.2018  time: 3.2879  data_time: 1.7168  lr: 0.00029445  max_mem: 9678M\n",
            "[05/10 09:50:24 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/TACO-expl/data/annotations_off_0_val.json\n",
            "[05/10 09:50:24 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/10 09:50:24 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[05/10 09:50:24 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "WARNING [05/10 09:50:24 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[05/10 09:50:24 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 09:50:53 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0046 s/iter. Inference: 0.1952 s/iter. Eval: 3.4714 s/iter. Total: 3.6712 s/iter. ETA=0:08:30\n",
            "[05/10 09:51:03 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0084 s/iter. Inference: 0.1992 s/iter. Eval: 3.0137 s/iter. Total: 3.2216 s/iter. ETA=0:07:14\n",
            "[05/10 09:51:15 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0082 s/iter. Inference: 0.2074 s/iter. Eval: 2.9169 s/iter. Total: 3.1329 s/iter. ETA=0:06:50\n",
            "[05/10 09:51:23 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0078 s/iter. Inference: 0.2057 s/iter. Eval: 3.0314 s/iter. Total: 3.2456 s/iter. ETA=0:06:58\n",
            "[05/10 09:51:28 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0075 s/iter. Inference: 0.2060 s/iter. Eval: 3.1352 s/iter. Total: 3.3495 s/iter. ETA=0:07:08\n",
            "[05/10 09:51:37 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0073 s/iter. Inference: 0.2079 s/iter. Eval: 3.2400 s/iter. Total: 3.4559 s/iter. ETA=0:07:15\n",
            "[05/10 09:51:44 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0074 s/iter. Inference: 0.2096 s/iter. Eval: 3.2385 s/iter. Total: 3.4565 s/iter. ETA=0:07:08\n",
            "[05/10 09:51:54 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0071 s/iter. Inference: 0.2096 s/iter. Eval: 3.3798 s/iter. Total: 3.5975 s/iter. ETA=0:07:18\n",
            "[05/10 09:52:04 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0069 s/iter. Inference: 0.2106 s/iter. Eval: 3.6532 s/iter. Total: 3.8717 s/iter. ETA=0:07:48\n",
            "[05/10 09:52:13 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0076 s/iter. Inference: 0.2166 s/iter. Eval: 3.6865 s/iter. Total: 3.9118 s/iter. ETA=0:07:45\n",
            "[05/10 09:52:18 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0077 s/iter. Inference: 0.2157 s/iter. Eval: 3.6062 s/iter. Total: 3.8306 s/iter. ETA=0:07:28\n",
            "[05/10 09:52:25 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0075 s/iter. Inference: 0.2154 s/iter. Eval: 3.7144 s/iter. Total: 3.9383 s/iter. ETA=0:07:36\n",
            "[05/10 09:52:32 d2.evaluation.evaluator]: Inference done 36/150. Dataloading: 0.0074 s/iter. Inference: 0.2152 s/iter. Eval: 3.6827 s/iter. Total: 3.9065 s/iter. ETA=0:07:25\n",
            "[05/10 09:52:38 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0073 s/iter. Inference: 0.2156 s/iter. Eval: 3.7278 s/iter. Total: 3.9520 s/iter. ETA=0:07:26\n",
            "[05/10 09:52:45 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0075 s/iter. Inference: 0.2166 s/iter. Eval: 3.8154 s/iter. Total: 4.0409 s/iter. ETA=0:07:32\n",
            "[05/10 09:52:50 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0075 s/iter. Inference: 0.2161 s/iter. Eval: 3.8561 s/iter. Total: 4.0811 s/iter. ETA=0:07:33\n",
            "[05/10 09:52:58 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0075 s/iter. Inference: 0.2158 s/iter. Eval: 3.9756 s/iter. Total: 4.2004 s/iter. ETA=0:07:42\n",
            "[05/10 09:53:04 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0077 s/iter. Inference: 0.2154 s/iter. Eval: 4.0193 s/iter. Total: 4.2440 s/iter. ETA=0:07:42\n",
            "[05/10 09:53:10 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0076 s/iter. Inference: 0.2163 s/iter. Eval: 4.0623 s/iter. Total: 4.2879 s/iter. ETA=0:07:43\n",
            "[05/10 09:53:16 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0073 s/iter. Inference: 0.2146 s/iter. Eval: 4.0121 s/iter. Total: 4.2357 s/iter. ETA=0:07:28\n",
            "[05/10 09:53:22 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0072 s/iter. Inference: 0.2146 s/iter. Eval: 4.0453 s/iter. Total: 4.2688 s/iter. ETA=0:07:28\n",
            "[05/10 09:53:31 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0071 s/iter. Inference: 0.2162 s/iter. Eval: 4.0535 s/iter. Total: 4.2784 s/iter. ETA=0:07:20\n",
            "[05/10 09:53:37 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0070 s/iter. Inference: 0.2163 s/iter. Eval: 4.0885 s/iter. Total: 4.3134 s/iter. ETA=0:07:19\n",
            "[05/10 09:53:43 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0070 s/iter. Inference: 0.2162 s/iter. Eval: 4.0400 s/iter. Total: 4.2650 s/iter. ETA=0:07:06\n",
            "[05/10 09:53:51 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0067 s/iter. Inference: 0.2130 s/iter. Eval: 3.9457 s/iter. Total: 4.1672 s/iter. ETA=0:06:44\n",
            "[05/10 09:54:02 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0064 s/iter. Inference: 0.2137 s/iter. Eval: 3.8373 s/iter. Total: 4.0590 s/iter. ETA=0:06:17\n",
            "[05/10 09:54:08 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0064 s/iter. Inference: 0.2121 s/iter. Eval: 3.7190 s/iter. Total: 3.9391 s/iter. ETA=0:05:54\n",
            "[05/10 09:54:15 d2.evaluation.evaluator]: Inference done 64/150. Dataloading: 0.0064 s/iter. Inference: 0.2136 s/iter. Eval: 3.5649 s/iter. Total: 3.7865 s/iter. ETA=0:05:25\n",
            "[05/10 09:54:20 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0068 s/iter. Inference: 0.2156 s/iter. Eval: 3.4658 s/iter. Total: 3.6897 s/iter. ETA=0:05:06\n",
            "[05/10 09:54:28 d2.evaluation.evaluator]: Inference done 69/150. Dataloading: 0.0070 s/iter. Inference: 0.2172 s/iter. Eval: 3.4789 s/iter. Total: 3.7046 s/iter. ETA=0:05:00\n",
            "[05/10 09:54:34 d2.evaluation.evaluator]: Inference done 71/150. Dataloading: 0.0068 s/iter. Inference: 0.2176 s/iter. Eval: 3.4601 s/iter. Total: 3.6861 s/iter. ETA=0:04:51\n",
            "[05/10 09:54:44 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0066 s/iter. Inference: 0.2184 s/iter. Eval: 3.4443 s/iter. Total: 3.6709 s/iter. ETA=0:04:38\n",
            "[05/10 09:54:51 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0065 s/iter. Inference: 0.2166 s/iter. Eval: 3.3284 s/iter. Total: 3.5531 s/iter. ETA=0:04:15\n",
            "[05/10 09:54:56 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0064 s/iter. Inference: 0.2171 s/iter. Eval: 3.3051 s/iter. Total: 3.5301 s/iter. ETA=0:04:07\n",
            "[05/10 09:55:01 d2.evaluation.evaluator]: Inference done 83/150. Dataloading: 0.0063 s/iter. Inference: 0.2162 s/iter. Eval: 3.2403 s/iter. Total: 3.4644 s/iter. ETA=0:03:52\n",
            "[05/10 09:55:08 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0062 s/iter. Inference: 0.2155 s/iter. Eval: 3.2005 s/iter. Total: 3.4237 s/iter. ETA=0:03:39\n",
            "[05/10 09:55:15 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0060 s/iter. Inference: 0.2122 s/iter. Eval: 3.1157 s/iter. Total: 3.3354 s/iter. ETA=0:03:20\n",
            "[05/10 09:55:21 d2.evaluation.evaluator]: Inference done 93/150. Dataloading: 0.0059 s/iter. Inference: 0.2120 s/iter. Eval: 3.0747 s/iter. Total: 3.2941 s/iter. ETA=0:03:07\n",
            "[05/10 09:55:28 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0059 s/iter. Inference: 0.2123 s/iter. Eval: 3.0407 s/iter. Total: 3.2604 s/iter. ETA=0:02:56\n",
            "[05/10 09:55:40 d2.evaluation.evaluator]: Inference done 99/150. Dataloading: 0.0058 s/iter. Inference: 0.2120 s/iter. Eval: 3.0670 s/iter. Total: 3.2863 s/iter. ETA=0:02:47\n",
            "[05/10 09:55:47 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0057 s/iter. Inference: 0.2121 s/iter. Eval: 3.0351 s/iter. Total: 3.2544 s/iter. ETA=0:02:36\n",
            "[05/10 09:55:53 d2.evaluation.evaluator]: Inference done 105/150. Dataloading: 0.0056 s/iter. Inference: 0.2118 s/iter. Eval: 2.9972 s/iter. Total: 3.2161 s/iter. ETA=0:02:24\n",
            "[05/10 09:56:00 d2.evaluation.evaluator]: Inference done 109/150. Dataloading: 0.0055 s/iter. Inference: 0.2098 s/iter. Eval: 2.9432 s/iter. Total: 3.1600 s/iter. ETA=0:02:09\n",
            "[05/10 09:56:06 d2.evaluation.evaluator]: Inference done 112/150. Dataloading: 0.0054 s/iter. Inference: 0.2088 s/iter. Eval: 2.9134 s/iter. Total: 3.1292 s/iter. ETA=0:01:58\n",
            "[05/10 09:56:18 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0055 s/iter. Inference: 0.2090 s/iter. Eval: 2.9409 s/iter. Total: 3.1569 s/iter. ETA=0:01:50\n",
            "[05/10 09:56:25 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0054 s/iter. Inference: 0.2101 s/iter. Eval: 2.9429 s/iter. Total: 3.1599 s/iter. ETA=0:01:44\n",
            "[05/10 09:56:33 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0054 s/iter. Inference: 0.2092 s/iter. Eval: 2.9347 s/iter. Total: 3.1507 s/iter. ETA=0:01:34\n",
            "[05/10 09:56:40 d2.evaluation.evaluator]: Inference done 122/150. Dataloading: 0.0054 s/iter. Inference: 0.2089 s/iter. Eval: 2.9370 s/iter. Total: 3.1528 s/iter. ETA=0:01:28\n",
            "[05/10 09:56:47 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0054 s/iter. Inference: 0.2091 s/iter. Eval: 2.9121 s/iter. Total: 3.1281 s/iter. ETA=0:01:18\n",
            "[05/10 09:56:52 d2.evaluation.evaluator]: Inference done 128/150. Dataloading: 0.0054 s/iter. Inference: 0.2081 s/iter. Eval: 2.8815 s/iter. Total: 3.0965 s/iter. ETA=0:01:08\n",
            "[05/10 09:56:58 d2.evaluation.evaluator]: Inference done 132/150. Dataloading: 0.0053 s/iter. Inference: 0.2062 s/iter. Eval: 2.8343 s/iter. Total: 3.0473 s/iter. ETA=0:00:54\n",
            "[05/10 09:57:04 d2.evaluation.evaluator]: Inference done 136/150. Dataloading: 0.0053 s/iter. Inference: 0.2063 s/iter. Eval: 2.7892 s/iter. Total: 3.0022 s/iter. ETA=0:00:42\n",
            "[05/10 09:57:11 d2.evaluation.evaluator]: Inference done 139/150. Dataloading: 0.0053 s/iter. Inference: 0.2070 s/iter. Eval: 2.7682 s/iter. Total: 2.9820 s/iter. ETA=0:00:32\n",
            "[05/10 09:57:17 d2.evaluation.evaluator]: Inference done 144/150. Dataloading: 0.0052 s/iter. Inference: 0.2050 s/iter. Eval: 2.7057 s/iter. Total: 2.9174 s/iter. ETA=0:00:17\n",
            "[05/10 09:57:23 d2.evaluation.evaluator]: Inference done 146/150. Dataloading: 0.0052 s/iter. Inference: 0.2044 s/iter. Eval: 2.7068 s/iter. Total: 2.9180 s/iter. ETA=0:00:11\n",
            "[05/10 09:57:28 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0051 s/iter. Inference: 0.2037 s/iter. Eval: 2.6862 s/iter. Total: 2.8965 s/iter. ETA=0:00:02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 09:57:31 d2.evaluation.evaluator]: Total inference time: 0:06:59.909442 (2.895927 s / iter per device, on 1 devices)\n",
            "[05/10 09:57:31 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:29 (0.203233 s / iter per device, on 1 devices)\n",
            "[05/10 09:57:31 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/10 09:57:31 d2.evaluation.coco_evaluation]: Saving results to /content/output/chkpt/inference/coco_instances_results.json\n",
            "[05/10 09:57:32 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 09:57:32 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/10 09:57:32 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.11 seconds.\n",
            "[05/10 09:57:32 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 09:57:32 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.06 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[05/10 09:57:32 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[05/10 09:57:32 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP    | category              | AP    | category   | AP    |\n",
            "|:-----------|:------|:----------------------|:------|:-----------|:------|\n",
            "| Bottle     | 0.000 | Bottle cap            | 0.000 | Can        | 0.000 |\n",
            "| Cigarette  | 0.000 | Cup                   | 0.000 | Lid        | 0.000 |\n",
            "| Other      | 0.000 | Plastic bag & wrapper | 0.000 | Pop tab    | 0.000 |\n",
            "| Straw      | 0.000 |                       |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.53s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 09:57:33 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[05/10 09:57:33 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.47 seconds.\n",
            "[05/10 09:57:33 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 09:57:33 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.04 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.123\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.243\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.099\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.054\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.216\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.272\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.277\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.003\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.107\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.359\n",
            "[05/10 09:57:33 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 12.313 | 24.308 | 9.947  | 0.347 | 5.439 | 16.445 |\n",
            "[05/10 09:57:33 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category   | AP     | category              | AP     | category   | AP     |\n",
            "|:-----------|:-------|:----------------------|:-------|:-----------|:-------|\n",
            "| Bottle     | 24.439 | Bottle cap            | 24.886 | Can        | 8.327  |\n",
            "| Cigarette  | 2.632  | Cup                   | 16.716 | Lid        | 11.809 |\n",
            "| Other      | 7.681  | Plastic bag & wrapper | 22.700 | Pop tab    | 1.634  |\n",
            "| Straw      | 2.313  |                       |        |            |        |\n",
            "[05/10 09:57:33 d2.engine.defaults]: Evaluation results for TACO_val in csv format:\n",
            "[05/10 09:57:33 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[05/10 09:57:33 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 09:57:33 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[05/10 09:57:33 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[05/10 09:57:33 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 09:57:33 d2.evaluation.testing]: copypaste: 12.3135,24.3080,9.9473,0.3465,5.4390,16.4447\n",
            "[05/10 09:57:33 d2.utils.events]:  eta: 1:30:28  iter: 3999  total_loss: 0.7837  loss_ins: 0.5462  loss_cate: 0.1931  time: 3.2876  data_time: 1.7243  lr: 0.00029042  max_mem: 9678M\n",
            "[05/10 09:58:38 d2.utils.events]:  eta: 1:29:38  iter: 4019  total_loss: 0.7786  loss_ins: 0.6058  loss_cate: 0.2028  time: 3.2872  data_time: 1.5725  lr: 0.00028641  max_mem: 9678M\n",
            "[05/10 09:59:47 d2.utils.events]:  eta: 1:28:35  iter: 4039  total_loss: 0.7965  loss_ins: 0.6248  loss_cate: 0.1806  time: 3.2885  data_time: 1.8947  lr: 0.00028244  max_mem: 9678M\n",
            "[05/10 10:00:54 d2.utils.events]:  eta: 1:27:55  iter: 4059  total_loss: 0.7802  loss_ins: 0.5912  loss_cate: 0.1768  time: 3.2864  data_time: 1.5968  lr: 0.0002785  max_mem: 9678M\n",
            "[05/10 10:01:57 d2.utils.events]:  eta: 1:26:42  iter: 4079  total_loss: 0.8001  loss_ins: 0.603  loss_cate: 0.1881  time: 3.2854  data_time: 1.4633  lr: 0.0002746  max_mem: 9678M\n",
            "[05/10 10:03:08 d2.utils.events]:  eta: 1:25:08  iter: 4099  total_loss: 0.784  loss_ins: 0.5996  loss_cate: 0.1735  time: 3.2877  data_time: 1.9579  lr: 0.00027072  max_mem: 9678M\n",
            "[05/10 10:04:12 d2.utils.events]:  eta: 1:23:54  iter: 4119  total_loss: 0.7457  loss_ins: 0.5489  loss_cate: 0.1856  time: 3.2868  data_time: 1.6830  lr: 0.00026688  max_mem: 9678M\n",
            "[05/10 10:05:16 d2.utils.events]:  eta: 1:22:53  iter: 4139  total_loss: 0.8365  loss_ins: 0.6702  loss_cate: 0.1759  time: 3.2862  data_time: 1.5730  lr: 0.00026308  max_mem: 9678M\n",
            "[05/10 10:06:20 d2.utils.events]:  eta: 1:21:12  iter: 4159  total_loss: 0.7733  loss_ins: 0.5917  loss_cate: 0.1833  time: 3.2852  data_time: 1.7205  lr: 0.0002593  max_mem: 9678M\n",
            "[05/10 10:07:33 d2.utils.events]:  eta: 1:20:28  iter: 4179  total_loss: 0.9073  loss_ins: 0.7108  loss_cate: 0.195  time: 3.2887  data_time: 1.9582  lr: 0.00025557  max_mem: 9678M\n",
            "[05/10 10:08:39 d2.utils.events]:  eta: 1:19:33  iter: 4199  total_loss: 0.7619  loss_ins: 0.5905  loss_cate: 0.1785  time: 3.2874  data_time: 1.5760  lr: 0.00025186  max_mem: 9678M\n",
            "[05/10 10:09:41 d2.utils.events]:  eta: 1:18:31  iter: 4219  total_loss: 0.8186  loss_ins: 0.6196  loss_cate: 0.1795  time: 3.2858  data_time: 1.5159  lr: 0.0002482  max_mem: 9678M\n",
            "[05/10 10:10:47 d2.utils.events]:  eta: 1:17:54  iter: 4239  total_loss: 0.7183  loss_ins: 0.5656  loss_cate: 0.1708  time: 3.2862  data_time: 1.7632  lr: 0.00024457  max_mem: 9678M\n",
            "[05/10 10:11:56 d2.utils.events]:  eta: 1:17:11  iter: 4259  total_loss: 0.7758  loss_ins: 0.5835  loss_cate: 0.1755  time: 3.2875  data_time: 1.7233  lr: 0.00024097  max_mem: 9678M\n",
            "[05/10 10:13:01 d2.utils.events]:  eta: 1:16:16  iter: 4279  total_loss: 0.7411  loss_ins: 0.5287  loss_cate: 0.195  time: 3.2872  data_time: 1.7041  lr: 0.00023742  max_mem: 9678M\n",
            "[05/10 10:14:06 d2.utils.events]:  eta: 1:14:51  iter: 4299  total_loss: 0.7234  loss_ins: 0.534  loss_cate: 0.1706  time: 3.2868  data_time: 1.7409  lr: 0.0002339  max_mem: 9678M\n",
            "[05/10 10:15:09 d2.utils.events]:  eta: 1:13:46  iter: 4319  total_loss: 0.7839  loss_ins: 0.5865  loss_cate: 0.1842  time: 3.2857  data_time: 1.5838  lr: 0.00023041  max_mem: 9678M\n",
            "[05/10 10:16:20 d2.utils.events]:  eta: 1:12:58  iter: 4339  total_loss: 0.7025  loss_ins: 0.527  loss_cate: 0.175  time: 3.2876  data_time: 1.8849  lr: 0.00022697  max_mem: 9678M\n",
            "[05/10 10:17:26 d2.utils.events]:  eta: 1:11:55  iter: 4359  total_loss: 0.6824  loss_ins: 0.5019  loss_cate: 0.184  time: 3.2875  data_time: 1.6746  lr: 0.00022356  max_mem: 9678M\n",
            "[05/10 10:18:27 d2.utils.events]:  eta: 1:10:57  iter: 4379  total_loss: 0.718  loss_ins: 0.5349  loss_cate: 0.1731  time: 3.2853  data_time: 1.4909  lr: 0.0002202  max_mem: 9678M\n",
            "[05/10 10:19:30 d2.utils.events]:  eta: 1:09:40  iter: 4399  total_loss: 0.7133  loss_ins: 0.5462  loss_cate: 0.1693  time: 3.2843  data_time: 1.6197  lr: 0.00021687  max_mem: 9678M\n",
            "[05/10 10:20:39 d2.utils.events]:  eta: 1:08:44  iter: 4419  total_loss: 0.6169  loss_ins: 0.4468  loss_cate: 0.1577  time: 3.2856  data_time: 1.8784  lr: 0.00021358  max_mem: 9678M\n",
            "[05/10 10:21:43 d2.utils.events]:  eta: 1:07:36  iter: 4439  total_loss: 0.7297  loss_ins: 0.5533  loss_cate: 0.1701  time: 3.2851  data_time: 1.6561  lr: 0.00021034  max_mem: 9678M\n",
            "[05/10 10:22:52 d2.utils.events]:  eta: 1:06:32  iter: 4459  total_loss: 0.8003  loss_ins: 0.607  loss_cate: 0.1816  time: 3.2862  data_time: 1.9014  lr: 0.00020713  max_mem: 9678M\n",
            "[05/10 10:24:03 d2.utils.events]:  eta: 1:05:09  iter: 4479  total_loss: 0.8193  loss_ins: 0.6314  loss_cate: 0.1846  time: 3.2882  data_time: 1.8543  lr: 0.00020397  max_mem: 9678M\n",
            "[05/10 10:25:07 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/TACO-expl/data/annotations_off_0_val.json\n",
            "[05/10 10:25:07 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/10 10:25:07 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[05/10 10:25:07 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "WARNING [05/10 10:25:07 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[05/10 10:25:07 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 10:25:34 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0058 s/iter. Inference: 0.2458 s/iter. Eval: 3.0899 s/iter. Total: 3.3414 s/iter. ETA=0:07:44\n",
            "[05/10 10:25:46 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0055 s/iter. Inference: 0.2135 s/iter. Eval: 3.0403 s/iter. Total: 3.2597 s/iter. ETA=0:07:20\n",
            "[05/10 10:25:52 d2.evaluation.evaluator]: Inference done 18/150. Dataloading: 0.0073 s/iter. Inference: 0.2270 s/iter. Eval: 2.6651 s/iter. Total: 2.9000 s/iter. ETA=0:06:22\n",
            "[05/10 10:25:59 d2.evaluation.evaluator]: Inference done 20/150. Dataloading: 0.0071 s/iter. Inference: 0.2320 s/iter. Eval: 2.7407 s/iter. Total: 2.9805 s/iter. ETA=0:06:27\n",
            "[05/10 10:26:07 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0069 s/iter. Inference: 0.2285 s/iter. Eval: 2.8779 s/iter. Total: 3.1139 s/iter. ETA=0:06:38\n",
            "[05/10 10:26:16 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0065 s/iter. Inference: 0.2276 s/iter. Eval: 3.0350 s/iter. Total: 3.2698 s/iter. ETA=0:06:51\n",
            "[05/10 10:26:22 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0064 s/iter. Inference: 0.2285 s/iter. Eval: 3.0127 s/iter. Total: 3.2484 s/iter. ETA=0:06:42\n",
            "[05/10 10:26:27 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0067 s/iter. Inference: 0.2302 s/iter. Eval: 3.0974 s/iter. Total: 3.3353 s/iter. ETA=0:06:50\n",
            "[05/10 10:26:36 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0067 s/iter. Inference: 0.2299 s/iter. Eval: 3.3391 s/iter. Total: 3.5767 s/iter. ETA=0:07:16\n",
            "[05/10 10:26:42 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0070 s/iter. Inference: 0.2314 s/iter. Eval: 3.4344 s/iter. Total: 3.6739 s/iter. ETA=0:07:24\n",
            "[05/10 10:26:50 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0069 s/iter. Inference: 0.2369 s/iter. Eval: 3.4374 s/iter. Total: 3.6825 s/iter. ETA=0:07:18\n",
            "[05/10 10:26:56 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0069 s/iter. Inference: 0.2357 s/iter. Eval: 3.3970 s/iter. Total: 3.6410 s/iter. ETA=0:07:05\n",
            "[05/10 10:27:03 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0069 s/iter. Inference: 0.2345 s/iter. Eval: 3.5210 s/iter. Total: 3.7638 s/iter. ETA=0:07:16\n",
            "[05/10 10:27:09 d2.evaluation.evaluator]: Inference done 36/150. Dataloading: 0.0067 s/iter. Inference: 0.2335 s/iter. Eval: 3.4849 s/iter. Total: 3.7266 s/iter. ETA=0:07:04\n",
            "[05/10 10:27:20 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0068 s/iter. Inference: 0.2350 s/iter. Eval: 3.5852 s/iter. Total: 3.8286 s/iter. ETA=0:07:08\n",
            "[05/10 10:27:26 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0067 s/iter. Inference: 0.2344 s/iter. Eval: 3.6344 s/iter. Total: 3.8772 s/iter. ETA=0:07:10\n",
            "[05/10 10:27:34 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0066 s/iter. Inference: 0.2338 s/iter. Eval: 3.7558 s/iter. Total: 3.9979 s/iter. ETA=0:07:19\n",
            "[05/10 10:27:40 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0065 s/iter. Inference: 0.2332 s/iter. Eval: 3.8253 s/iter. Total: 4.0667 s/iter. ETA=0:07:23\n",
            "[05/10 10:27:47 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0064 s/iter. Inference: 0.2340 s/iter. Eval: 3.8954 s/iter. Total: 4.1377 s/iter. ETA=0:07:26\n",
            "[05/10 10:27:55 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0062 s/iter. Inference: 0.2333 s/iter. Eval: 3.8875 s/iter. Total: 4.1289 s/iter. ETA=0:07:17\n",
            "[05/10 10:28:01 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0062 s/iter. Inference: 0.2329 s/iter. Eval: 3.9483 s/iter. Total: 4.1893 s/iter. ETA=0:07:19\n",
            "[05/10 10:28:07 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0061 s/iter. Inference: 0.2343 s/iter. Eval: 3.9709 s/iter. Total: 4.2132 s/iter. ETA=0:07:18\n",
            "[05/10 10:28:19 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0062 s/iter. Inference: 0.2354 s/iter. Eval: 4.0515 s/iter. Total: 4.2951 s/iter. ETA=0:07:18\n",
            "[05/10 10:28:26 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0060 s/iter. Inference: 0.2338 s/iter. Eval: 4.0375 s/iter. Total: 4.2794 s/iter. ETA=0:07:07\n",
            "[05/10 10:28:36 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0060 s/iter. Inference: 0.2350 s/iter. Eval: 3.9788 s/iter. Total: 4.2216 s/iter. ETA=0:06:49\n",
            "[05/10 10:28:42 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0063 s/iter. Inference: 0.2332 s/iter. Eval: 3.8436 s/iter. Total: 4.0849 s/iter. ETA=0:06:23\n",
            "[05/10 10:28:48 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0063 s/iter. Inference: 0.2327 s/iter. Eval: 3.8742 s/iter. Total: 4.1150 s/iter. ETA=0:06:22\n",
            "[05/10 10:28:53 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0061 s/iter. Inference: 0.2331 s/iter. Eval: 3.8250 s/iter. Total: 4.0661 s/iter. ETA=0:06:10\n",
            "[05/10 10:28:59 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0059 s/iter. Inference: 0.2298 s/iter. Eval: 3.6479 s/iter. Total: 3.8854 s/iter. ETA=0:05:38\n",
            "[05/10 10:29:05 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0056 s/iter. Inference: 0.2234 s/iter. Eval: 3.4444 s/iter. Total: 3.6752 s/iter. ETA=0:05:01\n",
            "[05/10 10:29:11 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0056 s/iter. Inference: 0.2219 s/iter. Eval: 3.4171 s/iter. Total: 3.6464 s/iter. ETA=0:04:51\n",
            "[05/10 10:29:18 d2.evaluation.evaluator]: Inference done 73/150. Dataloading: 0.0055 s/iter. Inference: 0.2213 s/iter. Eval: 3.3619 s/iter. Total: 3.5905 s/iter. ETA=0:04:36\n",
            "[05/10 10:29:23 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0055 s/iter. Inference: 0.2221 s/iter. Eval: 3.3848 s/iter. Total: 3.6143 s/iter. ETA=0:04:34\n",
            "[05/10 10:29:28 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0054 s/iter. Inference: 0.2192 s/iter. Eval: 3.2588 s/iter. Total: 3.4852 s/iter. ETA=0:04:10\n",
            "[05/10 10:29:34 d2.evaluation.evaluator]: Inference done 81/150. Dataloading: 0.0053 s/iter. Inference: 0.2174 s/iter. Eval: 3.1948 s/iter. Total: 3.4192 s/iter. ETA=0:03:55\n",
            "[05/10 10:29:40 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0053 s/iter. Inference: 0.2184 s/iter. Eval: 3.1397 s/iter. Total: 3.3651 s/iter. ETA=0:03:42\n",
            "[05/10 10:29:45 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0054 s/iter. Inference: 0.2184 s/iter. Eval: 3.1258 s/iter. Total: 3.3514 s/iter. ETA=0:03:34\n",
            "[05/10 10:29:51 d2.evaluation.evaluator]: Inference done 89/150. Dataloading: 0.0053 s/iter. Inference: 0.2185 s/iter. Eval: 3.0695 s/iter. Total: 3.2950 s/iter. ETA=0:03:20\n",
            "[05/10 10:29:58 d2.evaluation.evaluator]: Inference done 93/150. Dataloading: 0.0052 s/iter. Inference: 0.2167 s/iter. Eval: 2.9997 s/iter. Total: 3.2232 s/iter. ETA=0:03:03\n",
            "[05/10 10:30:03 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0051 s/iter. Inference: 0.2155 s/iter. Eval: 2.9520 s/iter. Total: 3.1744 s/iter. ETA=0:02:51\n",
            "[05/10 10:30:16 d2.evaluation.evaluator]: Inference done 99/150. Dataloading: 0.0052 s/iter. Inference: 0.2162 s/iter. Eval: 2.9861 s/iter. Total: 3.2093 s/iter. ETA=0:02:43\n",
            "[05/10 10:30:22 d2.evaluation.evaluator]: Inference done 103/150. Dataloading: 0.0051 s/iter. Inference: 0.2144 s/iter. Eval: 2.9254 s/iter. Total: 3.1466 s/iter. ETA=0:02:27\n",
            "[05/10 10:30:29 d2.evaluation.evaluator]: Inference done 108/150. Dataloading: 0.0050 s/iter. Inference: 0.2127 s/iter. Eval: 2.8371 s/iter. Total: 3.0564 s/iter. ETA=0:02:08\n",
            "[05/10 10:30:35 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0051 s/iter. Inference: 0.2136 s/iter. Eval: 2.8128 s/iter. Total: 3.0331 s/iter. ETA=0:01:58\n",
            "[05/10 10:30:41 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0051 s/iter. Inference: 0.2136 s/iter. Eval: 2.7851 s/iter. Total: 3.0054 s/iter. ETA=0:01:48\n",
            "[05/10 10:30:49 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0051 s/iter. Inference: 0.2143 s/iter. Eval: 2.8214 s/iter. Total: 3.0425 s/iter. ETA=0:01:46\n",
            "[05/10 10:30:54 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0051 s/iter. Inference: 0.2139 s/iter. Eval: 2.8182 s/iter. Total: 3.0389 s/iter. ETA=0:01:40\n",
            "[05/10 10:31:03 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0050 s/iter. Inference: 0.2139 s/iter. Eval: 2.8126 s/iter. Total: 3.0331 s/iter. ETA=0:01:30\n",
            "[05/10 10:31:09 d2.evaluation.evaluator]: Inference done 124/150. Dataloading: 0.0051 s/iter. Inference: 0.2151 s/iter. Eval: 2.7629 s/iter. Total: 2.9847 s/iter. ETA=0:01:17\n",
            "[05/10 10:31:15 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0050 s/iter. Inference: 0.2144 s/iter. Eval: 2.7361 s/iter. Total: 2.9572 s/iter. ETA=0:01:08\n",
            "[05/10 10:31:20 d2.evaluation.evaluator]: Inference done 132/150. Dataloading: 0.0050 s/iter. Inference: 0.2121 s/iter. Eval: 2.6678 s/iter. Total: 2.8865 s/iter. ETA=0:00:51\n",
            "[05/10 10:31:26 d2.evaluation.evaluator]: Inference done 137/150. Dataloading: 0.0051 s/iter. Inference: 0.2135 s/iter. Eval: 2.5966 s/iter. Total: 2.8168 s/iter. ETA=0:00:36\n",
            "[05/10 10:31:31 d2.evaluation.evaluator]: Inference done 140/150. Dataloading: 0.0051 s/iter. Inference: 0.2139 s/iter. Eval: 2.5757 s/iter. Total: 2.7963 s/iter. ETA=0:00:27\n",
            "[05/10 10:31:39 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0050 s/iter. Inference: 0.2120 s/iter. Eval: 2.5323 s/iter. Total: 2.7509 s/iter. ETA=0:00:13\n",
            "[05/10 10:31:45 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0049 s/iter. Inference: 0.2101 s/iter. Eval: 2.4963 s/iter. Total: 2.7128 s/iter. ETA=0:00:02\n",
            "[05/10 10:31:47 d2.evaluation.evaluator]: Total inference time: 0:06:33.294939 (2.712379 s / iter per device, on 1 devices)\n",
            "[05/10 10:31:47 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:30 (0.209623 s / iter per device, on 1 devices)\n",
            "[05/10 10:31:47 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/10 10:31:47 d2.evaluation.coco_evaluation]: Saving results to /content/output/chkpt/inference/coco_instances_results.json\n",
            "[05/10 10:31:48 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 10:31:48 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/10 10:31:48 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.14 seconds.\n",
            "[05/10 10:31:48 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 10:31:48 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.06 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[05/10 10:31:48 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[05/10 10:31:48 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP    | category              | AP    | category   | AP    |\n",
            "|:-----------|:------|:----------------------|:------|:-----------|:------|\n",
            "| Bottle     | 0.000 | Bottle cap            | 0.000 | Can        | 0.000 |\n",
            "| Cigarette  | 0.000 | Cup                   | 0.000 | Lid        | 0.000 |\n",
            "| Other      | 0.000 | Plastic bag & wrapper | 0.000 | Pop tab    | 0.000 |\n",
            "| Straw      | 0.000 |                       |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.46s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 10:31:49 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[05/10 10:31:49 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.53 seconds.\n",
            "[05/10 10:31:49 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 10:31:49 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.07 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.125\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.223\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.102\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.046\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.222\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.283\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.291\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.098\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            "[05/10 10:31:49 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 12.548 | 22.287 | 10.199 | 0.000 | 4.612 | 17.209 |\n",
            "[05/10 10:31:49 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category   | AP     | category              | AP     | category   | AP     |\n",
            "|:-----------|:-------|:----------------------|:-------|:-----------|:-------|\n",
            "| Bottle     | 26.873 | Bottle cap            | 22.284 | Can        | 8.653  |\n",
            "| Cigarette  | 2.503  | Cup                   | 14.947 | Lid        | 13.489 |\n",
            "| Other      | 9.061  | Plastic bag & wrapper | 22.471 | Pop tab    | 2.178  |\n",
            "| Straw      | 3.015  |                       |        |            |        |\n",
            "[05/10 10:31:49 d2.engine.defaults]: Evaluation results for TACO_val in csv format:\n",
            "[05/10 10:31:49 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[05/10 10:31:49 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 10:31:49 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[05/10 10:31:49 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[05/10 10:31:49 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 10:31:49 d2.evaluation.testing]: copypaste: 12.5476,22.2872,10.1987,0.0000,4.6121,17.2088\n",
            "[05/10 10:31:49 d2.utils.events]:  eta: 1:04:19  iter: 4499  total_loss: 0.7832  loss_ins: 0.6413  loss_cate: 0.1658  time: 3.2873  data_time: 1.5891  lr: 0.00020084  max_mem: 9678M\n",
            "[05/10 10:33:03 d2.utils.events]:  eta: 1:03:33  iter: 4519  total_loss: 0.7441  loss_ins: 0.5528  loss_cate: 0.1805  time: 3.2904  data_time: 2.0764  lr: 0.00019776  max_mem: 9678M\n",
            "[05/10 10:34:09 d2.utils.events]:  eta: 1:02:35  iter: 4539  total_loss: 0.7862  loss_ins: 0.6061  loss_cate: 0.1694  time: 3.2903  data_time: 1.6257  lr: 0.00019472  max_mem: 9678M\n",
            "[05/10 10:35:12 d2.utils.events]:  eta: 1:01:52  iter: 4559  total_loss: 0.6468  loss_ins: 0.4639  loss_cate: 0.1684  time: 3.2894  data_time: 1.5917  lr: 0.00019172  max_mem: 9678M\n",
            "[05/10 10:36:19 d2.utils.events]:  eta: 1:00:47  iter: 4579  total_loss: 0.7541  loss_ins: 0.6004  loss_cate: 0.1647  time: 3.2897  data_time: 1.7107  lr: 0.00018877  max_mem: 9678M\n",
            "[05/10 10:37:30 d2.utils.events]:  eta: 0:59:49  iter: 4599  total_loss: 0.7578  loss_ins: 0.5994  loss_cate: 0.1748  time: 3.2916  data_time: 1.8593  lr: 0.00018586  max_mem: 9678M\n",
            "[05/10 10:38:35 d2.utils.events]:  eta: 0:58:50  iter: 4619  total_loss: 0.6795  loss_ins: 0.5631  loss_cate: 0.1461  time: 3.2914  data_time: 1.6458  lr: 0.00018299  max_mem: 9678M\n",
            "[05/10 10:39:42 d2.utils.events]:  eta: 0:57:52  iter: 4639  total_loss: 0.6321  loss_ins: 0.4897  loss_cate: 0.1564  time: 3.2917  data_time: 1.7060  lr: 0.00018016  max_mem: 9678M\n",
            "[05/10 10:40:56 d2.utils.events]:  eta: 0:56:54  iter: 4659  total_loss: 0.7307  loss_ins: 0.5489  loss_cate: 0.1663  time: 3.2928  data_time: 1.8307  lr: 0.00017738  max_mem: 9678M\n",
            "[05/10 10:41:59 d2.utils.events]:  eta: 0:55:52  iter: 4679  total_loss: 0.7112  loss_ins: 0.5399  loss_cate: 0.1847  time: 3.2919  data_time: 1.6376  lr: 0.00017465  max_mem: 9678M\n",
            "[05/10 10:43:06 d2.utils.events]:  eta: 0:54:46  iter: 4699  total_loss: 0.7559  loss_ins: 0.6025  loss_cate: 0.1664  time: 3.2921  data_time: 1.7955  lr: 0.00017196  max_mem: 9678M\n",
            "[05/10 10:44:11 d2.utils.events]:  eta: 0:53:42  iter: 4719  total_loss: 0.7114  loss_ins: 0.5338  loss_cate: 0.1562  time: 3.2917  data_time: 1.6454  lr: 0.00016931  max_mem: 9678M\n",
            "[05/10 10:45:18 d2.utils.events]:  eta: 0:52:39  iter: 4739  total_loss: 0.787  loss_ins: 0.5866  loss_cate: 0.1796  time: 3.2924  data_time: 1.6788  lr: 0.00016671  max_mem: 9678M\n",
            "[05/10 10:46:23 d2.utils.events]:  eta: 0:51:04  iter: 4759  total_loss: 0.6008  loss_ins: 0.4506  loss_cate: 0.1453  time: 3.2919  data_time: 1.6824  lr: 0.00016416  max_mem: 9678M\n",
            "[05/10 10:47:30 d2.utils.events]:  eta: 0:49:28  iter: 4779  total_loss: 0.6428  loss_ins: 0.4888  loss_cate: 0.1607  time: 3.2925  data_time: 1.7954  lr: 0.00016165  max_mem: 9678M\n",
            "[05/10 10:48:42 d2.utils.events]:  eta: 0:48:35  iter: 4799  total_loss: 0.6272  loss_ins: 0.4589  loss_cate: 0.1662  time: 3.2925  data_time: 1.8429  lr: 0.00015919  max_mem: 9678M\n",
            "[05/10 10:49:51 d2.utils.events]:  eta: 0:48:04  iter: 4819  total_loss: 0.8257  loss_ins: 0.6661  loss_cate: 0.1719  time: 3.2937  data_time: 1.8096  lr: 0.00015677  max_mem: 9678M\n",
            "[05/10 10:50:54 d2.utils.events]:  eta: 0:47:02  iter: 4839  total_loss: 0.4922  loss_ins: 0.3387  loss_cate: 0.1436  time: 3.2927  data_time: 1.5644  lr: 0.0001544  max_mem: 9678M\n",
            "[05/10 10:51:58 d2.utils.events]:  eta: 0:45:42  iter: 4859  total_loss: 0.6659  loss_ins: 0.5188  loss_cate: 0.1532  time: 3.2921  data_time: 1.6327  lr: 0.00015208  max_mem: 9678M\n",
            "[05/10 10:53:00 d2.utils.events]:  eta: 0:44:25  iter: 4879  total_loss: 0.6625  loss_ins: 0.4854  loss_cate: 0.1525  time: 3.2905  data_time: 1.4950  lr: 0.00014981  max_mem: 9678M\n",
            "[05/10 10:54:11 d2.utils.events]:  eta: 0:43:24  iter: 4899  total_loss: 0.6975  loss_ins: 0.506  loss_cate: 0.1701  time: 3.2923  data_time: 1.9539  lr: 0.00014758  max_mem: 9678M\n",
            "[05/10 10:55:16 d2.utils.events]:  eta: 0:41:50  iter: 4919  total_loss: 0.6546  loss_ins: 0.4901  loss_cate: 0.1632  time: 3.2921  data_time: 1.7080  lr: 0.00014541  max_mem: 9678M\n",
            "[05/10 10:56:17 d2.utils.events]:  eta: 0:40:49  iter: 4939  total_loss: 0.721  loss_ins: 0.5575  loss_cate: 0.1669  time: 3.2906  data_time: 1.4309  lr: 0.00014328  max_mem: 9678M\n",
            "[05/10 10:57:31 d2.utils.events]:  eta: 0:39:48  iter: 4959  total_loss: 0.6915  loss_ins: 0.5385  loss_cate: 0.1511  time: 3.2926  data_time: 1.9091  lr: 0.0001412  max_mem: 9678M\n",
            "[05/10 10:58:34 d2.utils.events]:  eta: 0:38:35  iter: 4979  total_loss: 0.7407  loss_ins: 0.5535  loss_cate: 0.1696  time: 3.2917  data_time: 1.5498  lr: 0.00013917  max_mem: 9678M\n",
            "[05/10 10:59:38 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/TACO-expl/data/annotations_off_0_val.json\n",
            "[05/10 10:59:38 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/10 10:59:38 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[05/10 10:59:38 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "WARNING [05/10 10:59:38 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[05/10 10:59:38 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 11:00:05 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0026 s/iter. Inference: 0.2203 s/iter. Eval: 3.3308 s/iter. Total: 3.5538 s/iter. ETA=0:08:13\n",
            "[05/10 11:00:11 d2.evaluation.evaluator]: Inference done 14/150. Dataloading: 0.0037 s/iter. Inference: 0.2196 s/iter. Eval: 2.7652 s/iter. Total: 2.9890 s/iter. ETA=0:06:46\n",
            "[05/10 11:00:18 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0036 s/iter. Inference: 0.2317 s/iter. Eval: 3.2287 s/iter. Total: 3.4645 s/iter. ETA=0:07:47\n",
            "[05/10 11:00:24 d2.evaluation.evaluator]: Inference done 18/150. Dataloading: 0.0042 s/iter. Inference: 0.2350 s/iter. Eval: 2.8718 s/iter. Total: 3.1119 s/iter. ETA=0:06:50\n",
            "[05/10 11:00:30 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0041 s/iter. Inference: 0.2338 s/iter. Eval: 3.0646 s/iter. Total: 3.3038 s/iter. ETA=0:07:12\n",
            "[05/10 11:00:38 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0040 s/iter. Inference: 0.2265 s/iter. Eval: 3.1397 s/iter. Total: 3.3713 s/iter. ETA=0:07:14\n",
            "[05/10 11:00:44 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0039 s/iter. Inference: 0.2289 s/iter. Eval: 3.3005 s/iter. Total: 3.5347 s/iter. ETA=0:07:32\n",
            "[05/10 11:00:49 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0041 s/iter. Inference: 0.2277 s/iter. Eval: 3.3920 s/iter. Total: 3.6256 s/iter. ETA=0:07:40\n",
            "[05/10 11:00:54 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0042 s/iter. Inference: 0.2295 s/iter. Eval: 3.4837 s/iter. Total: 3.7193 s/iter. ETA=0:07:48\n",
            "[05/10 11:01:03 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0041 s/iter. Inference: 0.2259 s/iter. Eval: 3.3742 s/iter. Total: 3.6060 s/iter. ETA=0:07:23\n",
            "[05/10 11:01:14 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0041 s/iter. Inference: 0.2260 s/iter. Eval: 3.6924 s/iter. Total: 3.9247 s/iter. ETA=0:07:58\n",
            "[05/10 11:01:25 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0043 s/iter. Inference: 0.2284 s/iter. Eval: 3.9670 s/iter. Total: 4.2021 s/iter. ETA=0:08:28\n",
            "[05/10 11:01:31 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0045 s/iter. Inference: 0.2341 s/iter. Eval: 3.8981 s/iter. Total: 4.1390 s/iter. ETA=0:08:12\n",
            "[05/10 11:01:38 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0044 s/iter. Inference: 0.2325 s/iter. Eval: 3.8294 s/iter. Total: 4.0685 s/iter. ETA=0:07:56\n",
            "[05/10 11:01:48 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0048 s/iter. Inference: 0.2340 s/iter. Eval: 4.0297 s/iter. Total: 4.2707 s/iter. ETA=0:08:15\n",
            "[05/10 11:01:53 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0047 s/iter. Inference: 0.2337 s/iter. Eval: 4.0767 s/iter. Total: 4.3174 s/iter. ETA=0:08:16\n",
            "[05/10 11:02:03 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0049 s/iter. Inference: 0.2337 s/iter. Eval: 4.1072 s/iter. Total: 4.3483 s/iter. ETA=0:08:11\n",
            "[05/10 11:02:09 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0049 s/iter. Inference: 0.2334 s/iter. Eval: 4.1549 s/iter. Total: 4.3958 s/iter. ETA=0:08:12\n",
            "[05/10 11:02:22 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0047 s/iter. Inference: 0.2318 s/iter. Eval: 4.2907 s/iter. Total: 4.5297 s/iter. ETA=0:08:18\n",
            "[05/10 11:02:29 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0048 s/iter. Inference: 0.2320 s/iter. Eval: 4.3605 s/iter. Total: 4.5998 s/iter. ETA=0:08:21\n",
            "[05/10 11:02:35 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0047 s/iter. Inference: 0.2314 s/iter. Eval: 4.4021 s/iter. Total: 4.6408 s/iter. ETA=0:08:21\n",
            "[05/10 11:02:44 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0047 s/iter. Inference: 0.2299 s/iter. Eval: 4.3874 s/iter. Total: 4.6246 s/iter. ETA=0:08:10\n",
            "[05/10 11:02:52 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0047 s/iter. Inference: 0.2307 s/iter. Eval: 4.4577 s/iter. Total: 4.6956 s/iter. ETA=0:08:13\n",
            "[05/10 11:03:02 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0048 s/iter. Inference: 0.2310 s/iter. Eval: 4.4887 s/iter. Total: 4.7270 s/iter. ETA=0:08:06\n",
            "[05/10 11:03:08 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0048 s/iter. Inference: 0.2305 s/iter. Eval: 4.5106 s/iter. Total: 4.7484 s/iter. ETA=0:08:04\n",
            "[05/10 11:03:14 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0047 s/iter. Inference: 0.2288 s/iter. Eval: 4.4409 s/iter. Total: 4.6768 s/iter. ETA=0:07:47\n",
            "[05/10 11:03:24 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0048 s/iter. Inference: 0.2287 s/iter. Eval: 4.3611 s/iter. Total: 4.5969 s/iter. ETA=0:07:25\n",
            "[05/10 11:03:30 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0048 s/iter. Inference: 0.2276 s/iter. Eval: 4.2027 s/iter. Total: 4.4373 s/iter. ETA=0:06:57\n",
            "[05/10 11:03:37 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0048 s/iter. Inference: 0.2265 s/iter. Eval: 4.2442 s/iter. Total: 4.4776 s/iter. ETA=0:06:56\n",
            "[05/10 11:03:42 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0047 s/iter. Inference: 0.2262 s/iter. Eval: 4.1769 s/iter. Total: 4.4100 s/iter. ETA=0:06:41\n",
            "[05/10 11:03:47 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0047 s/iter. Inference: 0.2269 s/iter. Eval: 4.0327 s/iter. Total: 4.2665 s/iter. ETA=0:06:15\n",
            "[05/10 11:03:53 d2.evaluation.evaluator]: Inference done 66/150. Dataloading: 0.0048 s/iter. Inference: 0.2298 s/iter. Eval: 3.8560 s/iter. Total: 4.0927 s/iter. ETA=0:05:43\n",
            "[05/10 11:03:59 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0048 s/iter. Inference: 0.2294 s/iter. Eval: 3.8105 s/iter. Total: 4.0467 s/iter. ETA=0:05:31\n",
            "[05/10 11:04:05 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0048 s/iter. Inference: 0.2283 s/iter. Eval: 3.7871 s/iter. Total: 4.0222 s/iter. ETA=0:05:21\n",
            "[05/10 11:04:13 d2.evaluation.evaluator]: Inference done 73/150. Dataloading: 0.0048 s/iter. Inference: 0.2286 s/iter. Eval: 3.7255 s/iter. Total: 3.9609 s/iter. ETA=0:05:04\n",
            "[05/10 11:04:19 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0047 s/iter. Inference: 0.2286 s/iter. Eval: 3.7522 s/iter. Total: 3.9876 s/iter. ETA=0:05:03\n",
            "[05/10 11:04:25 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0047 s/iter. Inference: 0.2275 s/iter. Eval: 3.6120 s/iter. Total: 3.8462 s/iter. ETA=0:04:36\n",
            "[05/10 11:04:30 d2.evaluation.evaluator]: Inference done 81/150. Dataloading: 0.0046 s/iter. Inference: 0.2245 s/iter. Eval: 3.5297 s/iter. Total: 3.7607 s/iter. ETA=0:04:19\n",
            "[05/10 11:04:35 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0045 s/iter. Inference: 0.2220 s/iter. Eval: 3.4557 s/iter. Total: 3.6841 s/iter. ETA=0:04:03\n",
            "[05/10 11:04:41 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0046 s/iter. Inference: 0.2224 s/iter. Eval: 3.4377 s/iter. Total: 3.6666 s/iter. ETA=0:03:54\n",
            "[05/10 11:04:47 d2.evaluation.evaluator]: Inference done 89/150. Dataloading: 0.0045 s/iter. Inference: 0.2222 s/iter. Eval: 3.3801 s/iter. Total: 3.6088 s/iter. ETA=0:03:40\n",
            "[05/10 11:04:52 d2.evaluation.evaluator]: Inference done 92/150. Dataloading: 0.0045 s/iter. Inference: 0.2219 s/iter. Eval: 3.3188 s/iter. Total: 3.5472 s/iter. ETA=0:03:25\n",
            "[05/10 11:04:58 d2.evaluation.evaluator]: Inference done 95/150. Dataloading: 0.0045 s/iter. Inference: 0.2197 s/iter. Eval: 3.2699 s/iter. Total: 3.4960 s/iter. ETA=0:03:12\n",
            "[05/10 11:05:04 d2.evaluation.evaluator]: Inference done 97/150. Dataloading: 0.0044 s/iter. Inference: 0.2197 s/iter. Eval: 3.2601 s/iter. Total: 3.4862 s/iter. ETA=0:03:04\n",
            "[05/10 11:05:14 d2.evaluation.evaluator]: Inference done 99/150. Dataloading: 0.0045 s/iter. Inference: 0.2199 s/iter. Eval: 3.2884 s/iter. Total: 3.5147 s/iter. ETA=0:02:59\n",
            "[05/10 11:05:20 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0044 s/iter. Inference: 0.2196 s/iter. Eval: 3.2446 s/iter. Total: 3.4706 s/iter. ETA=0:02:46\n",
            "[05/10 11:05:26 d2.evaluation.evaluator]: Inference done 105/150. Dataloading: 0.0044 s/iter. Inference: 0.2182 s/iter. Eval: 3.1941 s/iter. Total: 3.4186 s/iter. ETA=0:02:33\n",
            "[05/10 11:05:34 d2.evaluation.evaluator]: Inference done 109/150. Dataloading: 0.0043 s/iter. Inference: 0.2173 s/iter. Eval: 3.1451 s/iter. Total: 3.3686 s/iter. ETA=0:02:18\n",
            "[05/10 11:05:39 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0044 s/iter. Inference: 0.2176 s/iter. Eval: 3.1300 s/iter. Total: 3.3538 s/iter. ETA=0:02:10\n",
            "[05/10 11:05:45 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0045 s/iter. Inference: 0.2173 s/iter. Eval: 3.0926 s/iter. Total: 3.3163 s/iter. ETA=0:01:59\n",
            "[05/10 11:05:52 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0045 s/iter. Inference: 0.2176 s/iter. Eval: 3.1281 s/iter. Total: 3.3521 s/iter. ETA=0:01:57\n",
            "[05/10 11:06:01 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0046 s/iter. Inference: 0.2182 s/iter. Eval: 3.1446 s/iter. Total: 3.3692 s/iter. ETA=0:01:51\n",
            "[05/10 11:06:13 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0046 s/iter. Inference: 0.2188 s/iter. Eval: 3.1555 s/iter. Total: 3.3807 s/iter. ETA=0:01:41\n",
            "[05/10 11:06:18 d2.evaluation.evaluator]: Inference done 122/150. Dataloading: 0.0046 s/iter. Inference: 0.2193 s/iter. Eval: 3.1409 s/iter. Total: 3.3667 s/iter. ETA=0:01:34\n",
            "[05/10 11:06:25 d2.evaluation.evaluator]: Inference done 126/150. Dataloading: 0.0045 s/iter. Inference: 0.2174 s/iter. Eval: 3.0908 s/iter. Total: 3.3146 s/iter. ETA=0:01:19\n",
            "[05/10 11:06:31 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0045 s/iter. Inference: 0.2162 s/iter. Eval: 3.0652 s/iter. Total: 3.2877 s/iter. ETA=0:01:09\n",
            "[05/10 11:06:37 d2.evaluation.evaluator]: Inference done 132/150. Dataloading: 0.0045 s/iter. Inference: 0.2162 s/iter. Eval: 3.0280 s/iter. Total: 3.2506 s/iter. ETA=0:00:58\n",
            "[05/10 11:06:42 d2.evaluation.evaluator]: Inference done 136/150. Dataloading: 0.0046 s/iter. Inference: 0.2176 s/iter. Eval: 2.9721 s/iter. Total: 3.1961 s/iter. ETA=0:00:44\n",
            "[05/10 11:06:49 d2.evaluation.evaluator]: Inference done 140/150. Dataloading: 0.0046 s/iter. Inference: 0.2170 s/iter. Eval: 2.9230 s/iter. Total: 3.1464 s/iter. ETA=0:00:31\n",
            "[05/10 11:06:56 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0045 s/iter. Inference: 0.2140 s/iter. Eval: 2.8687 s/iter. Total: 3.0889 s/iter. ETA=0:00:15\n",
            "[05/10 11:07:03 d2.evaluation.evaluator]: Inference done 148/150. Dataloading: 0.0045 s/iter. Inference: 0.2131 s/iter. Eval: 2.8533 s/iter. Total: 3.0726 s/iter. ETA=0:00:06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 11:07:07 d2.evaluation.evaluator]: Total inference time: 0:07:23.646504 (3.059631 s / iter per device, on 1 devices)\n",
            "[05/10 11:07:07 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:30 (0.212418 s / iter per device, on 1 devices)\n",
            "[05/10 11:07:08 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/10 11:07:08 d2.evaluation.coco_evaluation]: Saving results to /content/output/chkpt/inference/coco_instances_results.json\n",
            "[05/10 11:07:08 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 11:07:08 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/10 11:07:08 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.07 seconds.\n",
            "[05/10 11:07:08 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 11:07:08 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.04 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[05/10 11:07:08 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[05/10 11:07:08 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP    | category              | AP    | category   | AP    |\n",
            "|:-----------|:------|:----------------------|:------|:-----------|:------|\n",
            "| Bottle     | 0.000 | Bottle cap            | 0.000 | Can        | 0.000 |\n",
            "| Cigarette  | 0.000 | Cup                   | 0.000 | Lid        | 0.000 |\n",
            "| Other      | 0.000 | Plastic bag & wrapper | 0.000 | Pop tab    | 0.000 |\n",
            "| Straw      | 0.000 |                       |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.30s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 11:07:08 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[05/10 11:07:09 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.34 seconds.\n",
            "[05/10 11:07:09 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 11:07:09 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.04 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.128\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.241\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.111\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.045\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.224\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.283\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.289\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.118\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.384\n",
            "[05/10 11:07:09 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 12.807 | 24.145 | 11.109 | 0.116 | 4.460 | 17.284 |\n",
            "[05/10 11:07:09 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category   | AP     | category              | AP     | category   | AP     |\n",
            "|:-----------|:-------|:----------------------|:-------|:-----------|:-------|\n",
            "| Bottle     | 27.926 | Bottle cap            | 23.782 | Can        | 8.773  |\n",
            "| Cigarette  | 1.497  | Cup                   | 13.824 | Lid        | 12.823 |\n",
            "| Other      | 8.500  | Plastic bag & wrapper | 25.534 | Pop tab    | 2.723  |\n",
            "| Straw      | 2.692  |                       |        |            |        |\n",
            "[05/10 11:07:09 d2.engine.defaults]: Evaluation results for TACO_val in csv format:\n",
            "[05/10 11:07:09 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[05/10 11:07:09 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 11:07:09 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[05/10 11:07:09 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[05/10 11:07:09 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 11:07:09 d2.evaluation.testing]: copypaste: 12.8073,24.1452,11.1085,0.1155,4.4601,17.2839\n",
            "[05/10 11:07:09 d2.utils.events]:  eta: 0:37:47  iter: 4999  total_loss: 0.7267  loss_ins: 0.5643  loss_cate: 0.1671  time: 3.2911  data_time: 1.6349  lr: 0.00013718  max_mem: 9678M\n",
            "[05/10 11:08:12 d2.utils.events]:  eta: 0:36:34  iter: 5019  total_loss: 0.6614  loss_ins: 0.5028  loss_cate: 0.1575  time: 3.2902  data_time: 1.5945  lr: 0.00013525  max_mem: 9678M\n",
            "[05/10 11:09:17 d2.utils.events]:  eta: 0:35:32  iter: 5039  total_loss: 0.6433  loss_ins: 0.4952  loss_cate: 0.1421  time: 3.2898  data_time: 1.6404  lr: 0.00013337  max_mem: 9678M\n",
            "[05/10 11:10:27 d2.utils.events]:  eta: 0:34:31  iter: 5059  total_loss: 0.6304  loss_ins: 0.4798  loss_cate: 0.1539  time: 3.2912  data_time: 1.9049  lr: 0.00013153  max_mem: 9678M\n",
            "[05/10 11:11:34 d2.utils.events]:  eta: 0:33:31  iter: 5079  total_loss: 0.7557  loss_ins: 0.5693  loss_cate: 0.1667  time: 3.2917  data_time: 1.7352  lr: 0.00012975  max_mem: 9678M\n",
            "[05/10 11:12:43 d2.utils.events]:  eta: 0:32:31  iter: 5099  total_loss: 0.646  loss_ins: 0.5011  loss_cate: 0.152  time: 3.2910  data_time: 1.7582  lr: 0.00012801  max_mem: 9678M\n",
            "[05/10 11:13:46 d2.utils.events]:  eta: 0:31:33  iter: 5119  total_loss: 0.6638  loss_ins: 0.4909  loss_cate: 0.1403  time: 3.2901  data_time: 1.5615  lr: 0.00012633  max_mem: 9678M\n",
            "[05/10 11:14:54 d2.utils.events]:  eta: 0:30:32  iter: 5139  total_loss: 0.6886  loss_ins: 0.5324  loss_cate: 0.1747  time: 3.2906  data_time: 1.7002  lr: 0.0001247  max_mem: 9678M\n",
            "[05/10 11:15:59 d2.utils.events]:  eta: 0:29:37  iter: 5159  total_loss: 0.7101  loss_ins: 0.5667  loss_cate: 0.1459  time: 3.2905  data_time: 1.7254  lr: 0.00012312  max_mem: 9678M\n",
            "[05/10 11:17:06 d2.utils.events]:  eta: 0:28:36  iter: 5179  total_loss: 0.5622  loss_ins: 0.4128  loss_cate: 0.1461  time: 3.2906  data_time: 1.7513  lr: 0.00012159  max_mem: 9678M\n",
            "[05/10 11:18:10 d2.utils.events]:  eta: 0:27:33  iter: 5199  total_loss: 0.7635  loss_ins: 0.5958  loss_cate: 0.1433  time: 3.2901  data_time: 1.6996  lr: 0.00012011  max_mem: 9678M\n",
            "[05/10 11:19:17 d2.utils.events]:  eta: 0:26:35  iter: 5219  total_loss: 0.6402  loss_ins: 0.4604  loss_cate: 0.1572  time: 3.2904  data_time: 1.7374  lr: 0.00011868  max_mem: 9678M\n",
            "[05/10 11:20:21 d2.utils.events]:  eta: 0:25:38  iter: 5239  total_loss: 0.6641  loss_ins: 0.5161  loss_cate: 0.1636  time: 3.2899  data_time: 1.6105  lr: 0.0001173  max_mem: 9678M\n",
            "[05/10 11:21:27 d2.utils.events]:  eta: 0:24:34  iter: 5259  total_loss: 0.7187  loss_ins: 0.543  loss_cate: 0.1626  time: 3.2896  data_time: 1.5272  lr: 0.00011598  max_mem: 9678M\n",
            "[05/10 11:22:35 d2.utils.events]:  eta: 0:23:37  iter: 5279  total_loss: 0.7333  loss_ins: 0.5507  loss_cate: 0.1553  time: 3.2901  data_time: 1.7223  lr: 0.0001147  max_mem: 9678M\n",
            "[05/10 11:23:39 d2.utils.events]:  eta: 0:22:31  iter: 5299  total_loss: 0.6823  loss_ins: 0.5095  loss_cate: 0.1703  time: 3.2896  data_time: 1.6093  lr: 0.00011348  max_mem: 9678M\n",
            "[05/10 11:24:42 d2.utils.events]:  eta: 0:21:36  iter: 5319  total_loss: 0.6963  loss_ins: 0.532  loss_cate: 0.1682  time: 3.2887  data_time: 1.4994  lr: 0.00011231  max_mem: 9678M\n",
            "[05/10 11:25:45 d2.utils.events]:  eta: 0:20:31  iter: 5339  total_loss: 0.6017  loss_ins: 0.4796  loss_cate: 0.138  time: 3.2879  data_time: 1.6461  lr: 0.0001112  max_mem: 9678M\n",
            "[05/10 11:26:55 d2.utils.events]:  eta: 0:19:35  iter: 5359  total_loss: 0.6313  loss_ins: 0.505  loss_cate: 0.1484  time: 3.2893  data_time: 1.8300  lr: 0.00011013  max_mem: 9678M\n",
            "[05/10 11:27:59 d2.utils.events]:  eta: 0:18:30  iter: 5379  total_loss: 0.6673  loss_ins: 0.5101  loss_cate: 0.159  time: 3.2887  data_time: 1.6207  lr: 0.00010912  max_mem: 9678M\n",
            "[05/10 11:29:08 d2.utils.events]:  eta: 0:17:35  iter: 5399  total_loss: 0.707  loss_ins: 0.5606  loss_cate: 0.1552  time: 3.2884  data_time: 1.6203  lr: 0.00010816  max_mem: 9678M\n",
            "[05/10 11:30:13 d2.utils.events]:  eta: 0:16:33  iter: 5419  total_loss: 0.7098  loss_ins: 0.5462  loss_cate: 0.1673  time: 3.2883  data_time: 1.7478  lr: 0.00010726  max_mem: 9678M\n",
            "[05/10 11:31:20 d2.utils.events]:  eta: 0:15:30  iter: 5439  total_loss: 0.6285  loss_ins: 0.4593  loss_cate: 0.1521  time: 3.2886  data_time: 1.7639  lr: 0.0001064  max_mem: 9678M\n",
            "[05/10 11:32:25 d2.utils.events]:  eta: 0:14:29  iter: 5459  total_loss: 0.5824  loss_ins: 0.413  loss_cate: 0.1347  time: 3.2883  data_time: 1.6119  lr: 0.0001056  max_mem: 9678M\n",
            "[05/10 11:33:31 d2.utils.events]:  eta: 0:13:29  iter: 5479  total_loss: 0.6436  loss_ins: 0.4671  loss_cate: 0.1501  time: 3.2883  data_time: 1.7411  lr: 0.00010485  max_mem: 9678M\n",
            "[05/10 11:34:33 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/TACO-expl/data/annotations_off_0_val.json\n",
            "[05/10 11:34:33 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/10 11:34:33 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[05/10 11:34:33 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "WARNING [05/10 11:34:33 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[05/10 11:34:33 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 11:35:05 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0043 s/iter. Inference: 0.2676 s/iter. Eval: 3.9940 s/iter. Total: 4.2659 s/iter. ETA=0:09:52\n",
            "[05/10 11:35:11 d2.evaluation.evaluator]: Inference done 14/150. Dataloading: 0.0043 s/iter. Inference: 0.2349 s/iter. Eval: 3.1709 s/iter. Total: 3.4105 s/iter. ETA=0:07:43\n",
            "[05/10 11:35:16 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0050 s/iter. Inference: 0.2322 s/iter. Eval: 3.3601 s/iter. Total: 3.5978 s/iter. ETA=0:08:05\n",
            "[05/10 11:35:21 d2.evaluation.evaluator]: Inference done 18/150. Dataloading: 0.0062 s/iter. Inference: 0.2368 s/iter. Eval: 2.9312 s/iter. Total: 3.1749 s/iter. ETA=0:06:59\n",
            "[05/10 11:35:26 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0061 s/iter. Inference: 0.2349 s/iter. Eval: 3.0712 s/iter. Total: 3.3132 s/iter. ETA=0:07:14\n",
            "[05/10 11:35:34 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0057 s/iter. Inference: 0.2353 s/iter. Eval: 3.1467 s/iter. Total: 3.3887 s/iter. ETA=0:07:17\n",
            "[05/10 11:35:43 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0058 s/iter. Inference: 0.2329 s/iter. Eval: 3.2511 s/iter. Total: 3.4908 s/iter. ETA=0:07:23\n",
            "[05/10 11:35:48 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0062 s/iter. Inference: 0.2348 s/iter. Eval: 3.3495 s/iter. Total: 3.5919 s/iter. ETA=0:07:32\n",
            "[05/10 11:35:54 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0067 s/iter. Inference: 0.2348 s/iter. Eval: 3.2851 s/iter. Total: 3.5282 s/iter. ETA=0:07:17\n",
            "[05/10 11:36:06 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0067 s/iter. Inference: 0.2380 s/iter. Eval: 3.5180 s/iter. Total: 3.7644 s/iter. ETA=0:07:39\n",
            "[05/10 11:36:15 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0065 s/iter. Inference: 0.2385 s/iter. Eval: 3.7237 s/iter. Total: 3.9706 s/iter. ETA=0:08:00\n",
            "[05/10 11:36:24 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0071 s/iter. Inference: 0.2451 s/iter. Eval: 3.7382 s/iter. Total: 3.9923 s/iter. ETA=0:07:55\n",
            "[05/10 11:36:36 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0069 s/iter. Inference: 0.2396 s/iter. Eval: 3.7390 s/iter. Total: 3.9872 s/iter. ETA=0:07:42\n",
            "[05/10 11:36:41 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0070 s/iter. Inference: 0.2384 s/iter. Eval: 3.7749 s/iter. Total: 4.0221 s/iter. ETA=0:07:42\n",
            "[05/10 11:36:48 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0072 s/iter. Inference: 0.2379 s/iter. Eval: 3.7712 s/iter. Total: 4.0181 s/iter. ETA=0:07:34\n",
            "[05/10 11:36:54 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0072 s/iter. Inference: 0.2379 s/iter. Eval: 3.8030 s/iter. Total: 4.0501 s/iter. ETA=0:07:33\n",
            "[05/10 11:37:08 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0070 s/iter. Inference: 0.2358 s/iter. Eval: 3.9885 s/iter. Total: 4.2332 s/iter. ETA=0:07:45\n",
            "[05/10 11:37:15 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0072 s/iter. Inference: 0.2356 s/iter. Eval: 4.0743 s/iter. Total: 4.3191 s/iter. ETA=0:07:50\n",
            "[05/10 11:37:21 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0072 s/iter. Inference: 0.2348 s/iter. Eval: 4.1237 s/iter. Total: 4.3677 s/iter. ETA=0:07:51\n",
            "[05/10 11:37:30 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0071 s/iter. Inference: 0.2322 s/iter. Eval: 4.1137 s/iter. Total: 4.3550 s/iter. ETA=0:07:41\n",
            "[05/10 11:37:37 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0071 s/iter. Inference: 0.2335 s/iter. Eval: 4.1845 s/iter. Total: 4.4271 s/iter. ETA=0:07:44\n",
            "[05/10 11:37:46 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0071 s/iter. Inference: 0.2334 s/iter. Eval: 4.1896 s/iter. Total: 4.4320 s/iter. ETA=0:07:36\n",
            "[05/10 11:37:52 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0071 s/iter. Inference: 0.2330 s/iter. Eval: 4.2365 s/iter. Total: 4.4786 s/iter. ETA=0:07:36\n",
            "[05/10 11:37:59 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0068 s/iter. Inference: 0.2300 s/iter. Eval: 4.0988 s/iter. Total: 4.3375 s/iter. ETA=0:07:09\n",
            "[05/10 11:38:07 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0069 s/iter. Inference: 0.2297 s/iter. Eval: 4.0870 s/iter. Total: 4.3255 s/iter. ETA=0:06:59\n",
            "[05/10 11:38:14 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0066 s/iter. Inference: 0.2283 s/iter. Eval: 3.9564 s/iter. Total: 4.1932 s/iter. ETA=0:06:34\n",
            "[05/10 11:38:19 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0065 s/iter. Inference: 0.2296 s/iter. Eval: 3.9802 s/iter. Total: 4.2183 s/iter. ETA=0:06:32\n",
            "[05/10 11:38:25 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0064 s/iter. Inference: 0.2268 s/iter. Eval: 3.8660 s/iter. Total: 4.1010 s/iter. ETA=0:06:09\n",
            "[05/10 11:38:31 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0064 s/iter. Inference: 0.2288 s/iter. Eval: 3.7412 s/iter. Total: 3.9783 s/iter. ETA=0:05:46\n",
            "[05/10 11:38:36 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0063 s/iter. Inference: 0.2321 s/iter. Eval: 3.5755 s/iter. Total: 3.8157 s/iter. ETA=0:05:16\n",
            "[05/10 11:38:44 d2.evaluation.evaluator]: Inference done 69/150. Dataloading: 0.0062 s/iter. Inference: 0.2324 s/iter. Eval: 3.5731 s/iter. Total: 3.8135 s/iter. ETA=0:05:08\n",
            "[05/10 11:38:50 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0061 s/iter. Inference: 0.2309 s/iter. Eval: 3.4954 s/iter. Total: 3.7342 s/iter. ETA=0:04:51\n",
            "[05/10 11:38:56 d2.evaluation.evaluator]: Inference done 73/150. Dataloading: 0.0061 s/iter. Inference: 0.2319 s/iter. Eval: 3.5264 s/iter. Total: 3.7662 s/iter. ETA=0:04:49\n",
            "[05/10 11:39:02 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0060 s/iter. Inference: 0.2322 s/iter. Eval: 3.5630 s/iter. Total: 3.8030 s/iter. ETA=0:04:49\n",
            "[05/10 11:39:08 d2.evaluation.evaluator]: Inference done 77/150. Dataloading: 0.0060 s/iter. Inference: 0.2335 s/iter. Eval: 3.4841 s/iter. Total: 3.7254 s/iter. ETA=0:04:31\n",
            "[05/10 11:39:15 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0059 s/iter. Inference: 0.2336 s/iter. Eval: 3.4212 s/iter. Total: 3.6626 s/iter. ETA=0:04:16\n",
            "[05/10 11:39:21 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0057 s/iter. Inference: 0.2303 s/iter. Eval: 3.3247 s/iter. Total: 3.5625 s/iter. ETA=0:03:55\n",
            "[05/10 11:39:27 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0057 s/iter. Inference: 0.2281 s/iter. Eval: 3.3087 s/iter. Total: 3.5443 s/iter. ETA=0:03:46\n",
            "[05/10 11:39:33 d2.evaluation.evaluator]: Inference done 89/150. Dataloading: 0.0056 s/iter. Inference: 0.2274 s/iter. Eval: 3.2544 s/iter. Total: 3.4892 s/iter. ETA=0:03:32\n",
            "[05/10 11:39:39 d2.evaluation.evaluator]: Inference done 91/150. Dataloading: 0.0056 s/iter. Inference: 0.2282 s/iter. Eval: 3.2416 s/iter. Total: 3.4773 s/iter. ETA=0:03:25\n",
            "[05/10 11:39:45 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.0057 s/iter. Inference: 0.2305 s/iter. Eval: 3.1890 s/iter. Total: 3.4270 s/iter. ETA=0:03:11\n",
            "[05/10 11:39:52 d2.evaluation.evaluator]: Inference done 97/150. Dataloading: 0.0056 s/iter. Inference: 0.2291 s/iter. Eval: 3.1579 s/iter. Total: 3.3944 s/iter. ETA=0:02:59\n",
            "[05/10 11:40:01 d2.evaluation.evaluator]: Inference done 99/150. Dataloading: 0.0055 s/iter. Inference: 0.2280 s/iter. Eval: 3.1791 s/iter. Total: 3.4143 s/iter. ETA=0:02:54\n",
            "[05/10 11:40:08 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0055 s/iter. Inference: 0.2282 s/iter. Eval: 3.1454 s/iter. Total: 3.3809 s/iter. ETA=0:02:42\n",
            "[05/10 11:40:14 d2.evaluation.evaluator]: Inference done 105/150. Dataloading: 0.0054 s/iter. Inference: 0.2269 s/iter. Eval: 3.1044 s/iter. Total: 3.3385 s/iter. ETA=0:02:30\n",
            "[05/10 11:40:21 d2.evaluation.evaluator]: Inference done 109/150. Dataloading: 0.0053 s/iter. Inference: 0.2238 s/iter. Eval: 3.0463 s/iter. Total: 3.2772 s/iter. ETA=0:02:14\n",
            "[05/10 11:40:26 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0053 s/iter. Inference: 0.2240 s/iter. Eval: 3.0359 s/iter. Total: 3.2669 s/iter. ETA=0:02:07\n",
            "[05/10 11:40:31 d2.evaluation.evaluator]: Inference done 113/150. Dataloading: 0.0052 s/iter. Inference: 0.2242 s/iter. Eval: 3.0221 s/iter. Total: 3.2533 s/iter. ETA=0:02:00\n",
            "[05/10 11:40:41 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0053 s/iter. Inference: 0.2241 s/iter. Eval: 3.0518 s/iter. Total: 3.2829 s/iter. ETA=0:01:54\n",
            "[05/10 11:40:48 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0053 s/iter. Inference: 0.2235 s/iter. Eval: 3.0606 s/iter. Total: 3.2911 s/iter. ETA=0:01:48\n",
            "[05/10 11:40:58 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0052 s/iter. Inference: 0.2236 s/iter. Eval: 3.0589 s/iter. Total: 3.2895 s/iter. ETA=0:01:38\n",
            "[05/10 11:41:03 d2.evaluation.evaluator]: Inference done 122/150. Dataloading: 0.0052 s/iter. Inference: 0.2234 s/iter. Eval: 3.0457 s/iter. Total: 3.2760 s/iter. ETA=0:01:31\n",
            "[05/10 11:41:08 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0051 s/iter. Inference: 0.2221 s/iter. Eval: 3.0072 s/iter. Total: 3.2361 s/iter. ETA=0:01:20\n",
            "[05/10 11:41:13 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0051 s/iter. Inference: 0.2217 s/iter. Eval: 2.9969 s/iter. Total: 3.2254 s/iter. ETA=0:01:14\n",
            "[05/10 11:41:19 d2.evaluation.evaluator]: Inference done 130/150. Dataloading: 0.0051 s/iter. Inference: 0.2216 s/iter. Eval: 2.9660 s/iter. Total: 3.1945 s/iter. ETA=0:01:03\n",
            "[05/10 11:41:25 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0052 s/iter. Inference: 0.2229 s/iter. Eval: 2.8846 s/iter. Total: 3.1145 s/iter. ETA=0:00:46\n",
            "[05/10 11:41:31 d2.evaluation.evaluator]: Inference done 138/150. Dataloading: 0.0052 s/iter. Inference: 0.2218 s/iter. Eval: 2.8607 s/iter. Total: 3.0893 s/iter. ETA=0:00:37\n",
            "[05/10 11:41:37 d2.evaluation.evaluator]: Inference done 143/150. Dataloading: 0.0051 s/iter. Inference: 0.2192 s/iter. Eval: 2.7941 s/iter. Total: 3.0200 s/iter. ETA=0:00:21\n",
            "[05/10 11:41:44 d2.evaluation.evaluator]: Inference done 146/150. Dataloading: 0.0050 s/iter. Inference: 0.2178 s/iter. Eval: 2.7806 s/iter. Total: 3.0050 s/iter. ETA=0:00:12\n",
            "[05/10 11:41:49 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0050 s/iter. Inference: 0.2167 s/iter. Eval: 2.7585 s/iter. Total: 2.9817 s/iter. ETA=0:00:02\n",
            "[05/10 11:41:51 d2.evaluation.evaluator]: Total inference time: 0:07:11.572890 (2.976365 s / iter per device, on 1 devices)\n",
            "[05/10 11:41:51 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:31 (0.216226 s / iter per device, on 1 devices)\n",
            "[05/10 11:41:52 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/10 11:41:52 d2.evaluation.coco_evaluation]: Saving results to /content/output/chkpt/inference/coco_instances_results.json\n",
            "[05/10 11:41:52 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 11:41:52 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/10 11:41:52 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.54 seconds.\n",
            "[05/10 11:41:52 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 11:41:52 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.04 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[05/10 11:41:52 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[05/10 11:41:52 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP    | category              | AP    | category   | AP    |\n",
            "|:-----------|:------|:----------------------|:------|:-----------|:------|\n",
            "| Bottle     | 0.000 | Bottle cap            | 0.000 | Can        | 0.000 |\n",
            "| Cigarette  | 0.000 | Cup                   | 0.000 | Lid        | 0.000 |\n",
            "| Other      | 0.000 | Plastic bag & wrapper | 0.000 | Pop tab    | 0.000 |\n",
            "| Straw      | 0.000 |                       |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.29s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 11:41:53 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[05/10 11:41:53 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.34 seconds.\n",
            "[05/10 11:41:53 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 11:41:53 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.04 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.240\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.109\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.047\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.222\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.281\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.286\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.104\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389\n",
            "[05/10 11:41:53 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 12.657 | 23.982 | 10.941 | 0.000 | 4.684 | 17.423 |\n",
            "[05/10 11:41:53 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category   | AP     | category              | AP     | category   | AP     |\n",
            "|:-----------|:-------|:----------------------|:-------|:-----------|:-------|\n",
            "| Bottle     | 28.066 | Bottle cap            | 23.774 | Can        | 7.682  |\n",
            "| Cigarette  | 2.350  | Cup                   | 15.379 | Lid        | 11.803 |\n",
            "| Other      | 8.967  | Plastic bag & wrapper | 24.194 | Pop tab    | 2.178  |\n",
            "| Straw      | 2.173  |                       |        |            |        |\n",
            "[05/10 11:41:53 d2.engine.defaults]: Evaluation results for TACO_val in csv format:\n",
            "[05/10 11:41:53 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[05/10 11:41:53 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 11:41:53 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[05/10 11:41:53 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[05/10 11:41:53 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 11:41:53 d2.evaluation.testing]: copypaste: 12.6565,23.9824,10.9406,0.0000,4.6841,17.4226\n",
            "[05/10 11:41:53 d2.utils.events]:  eta: 0:12:28  iter: 5499  total_loss: 0.7569  loss_ins: 0.5653  loss_cate: 0.1659  time: 3.2873  data_time: 1.6917  lr: 0.00010416  max_mem: 9678M\n",
            "[05/10 11:43:05 d2.utils.events]:  eta: 0:11:25  iter: 5519  total_loss: 0.693  loss_ins: 0.5124  loss_cate: 0.1734  time: 3.2889  data_time: 1.9719  lr: 0.00010352  max_mem: 9678M\n",
            "[05/10 11:44:06 d2.utils.events]:  eta: 0:10:24  iter: 5539  total_loss: 0.7266  loss_ins: 0.5678  loss_cate: 0.1612  time: 3.2877  data_time: 1.5402  lr: 0.00010293  max_mem: 9678M\n",
            "[05/10 11:45:09 d2.utils.events]:  eta: 0:09:24  iter: 5559  total_loss: 0.6941  loss_ins: 0.5274  loss_cate: 0.1595  time: 3.2867  data_time: 1.5459  lr: 0.0001024  max_mem: 9678M\n",
            "[05/10 11:46:14 d2.utils.events]:  eta: 0:08:25  iter: 5579  total_loss: 0.6115  loss_ins: 0.438  loss_cate: 0.1613  time: 3.2864  data_time: 1.7178  lr: 0.00010192  max_mem: 9678M\n",
            "[05/10 11:47:18 d2.utils.events]:  eta: 0:07:25  iter: 5599  total_loss: 0.5788  loss_ins: 0.4025  loss_cate: 0.1581  time: 3.2861  data_time: 1.5859  lr: 0.00010149  max_mem: 9678M\n",
            "[05/10 11:48:24 d2.utils.events]:  eta: 0:06:24  iter: 5619  total_loss: 0.579  loss_ins: 0.43  loss_cate: 0.1379  time: 3.2859  data_time: 1.6993  lr: 0.00010112  max_mem: 9678M\n",
            "[05/10 11:49:27 d2.utils.events]:  eta: 0:05:24  iter: 5639  total_loss: 0.769  loss_ins: 0.6237  loss_cate: 0.1463  time: 3.2852  data_time: 1.4014  lr: 0.0001008  max_mem: 9678M\n",
            "[05/10 11:50:38 d2.utils.events]:  eta: 0:04:24  iter: 5659  total_loss: 0.6087  loss_ins: 0.4525  loss_cate: 0.1299  time: 3.2866  data_time: 1.9403  lr: 0.00010053  max_mem: 9678M\n",
            "[05/10 11:51:41 d2.utils.events]:  eta: 0:03:24  iter: 5679  total_loss: 0.6055  loss_ins: 0.481  loss_cate: 0.1326  time: 3.2858  data_time: 1.4943  lr: 0.00010032  max_mem: 9678M\n",
            "[05/10 11:52:46 d2.utils.events]:  eta: 0:02:23  iter: 5699  total_loss: 0.6196  loss_ins: 0.4984  loss_cate: 0.1352  time: 3.2855  data_time: 1.6228  lr: 0.00010016  max_mem: 9678M\n",
            "[05/10 11:53:53 d2.utils.events]:  eta: 0:01:23  iter: 5719  total_loss: 0.6635  loss_ins: 0.4992  loss_cate: 0.1658  time: 3.2857  data_time: 1.8613  lr: 0.00010006  max_mem: 9678M\n",
            "[05/10 11:54:56 d2.utils.events]:  eta: 0:00:23  iter: 5739  total_loss: 0.7503  loss_ins: 0.59  loss_cate: 0.1703  time: 3.2850  data_time: 1.6600  lr: 0.00010001  max_mem: 9678M\n",
            "[05/10 11:55:30 d2.utils.events]:  eta: 0:00:00  iter: 5747  total_loss: 0.6734  loss_ins: 0.5405  loss_cate: 0.1502  time: 3.2856  data_time: 1.8581  lr: 0.0001  max_mem: 9678M\n",
            "[05/10 11:55:30 d2.engine.hooks]: Overall training speed: 3795 iterations in 3:27:52 (3.2865 s / it)\n",
            "[05/10 11:55:30 d2.engine.hooks]: Total training time: 4:35:51 (1:07:59 on hooks)\n",
            "[05/10 11:55:30 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/TACO-expl/data/annotations_off_0_val.json\n",
            "[05/10 11:55:30 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/10 11:55:30 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[05/10 11:55:30 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "WARNING [05/10 11:55:30 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[05/10 11:55:30 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 11:55:56 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0076 s/iter. Inference: 0.2225 s/iter. Eval: 3.1479 s/iter. Total: 3.3780 s/iter. ETA=0:07:49\n",
            "[05/10 11:56:06 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0074 s/iter. Inference: 0.2088 s/iter. Eval: 2.7651 s/iter. Total: 2.9817 s/iter. ETA=0:06:42\n",
            "[05/10 11:56:14 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0077 s/iter. Inference: 0.2241 s/iter. Eval: 2.5038 s/iter. Total: 2.7363 s/iter. ETA=0:05:58\n",
            "[05/10 11:56:20 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0079 s/iter. Inference: 0.2178 s/iter. Eval: 2.5584 s/iter. Total: 2.7850 s/iter. ETA=0:05:59\n",
            "[05/10 11:56:27 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0077 s/iter. Inference: 0.2163 s/iter. Eval: 2.6286 s/iter. Total: 2.8535 s/iter. ETA=0:06:02\n",
            "[05/10 11:56:37 d2.evaluation.evaluator]: Inference done 25/150. Dataloading: 0.0074 s/iter. Inference: 0.2198 s/iter. Eval: 2.8165 s/iter. Total: 3.0447 s/iter. ETA=0:06:20\n",
            "[05/10 11:56:43 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0070 s/iter. Inference: 0.2221 s/iter. Eval: 2.8117 s/iter. Total: 3.0420 s/iter. ETA=0:06:14\n",
            "[05/10 11:56:49 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0069 s/iter. Inference: 0.2226 s/iter. Eval: 2.9520 s/iter. Total: 3.1829 s/iter. ETA=0:06:28\n",
            "[05/10 11:56:55 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0068 s/iter. Inference: 0.2222 s/iter. Eval: 3.0707 s/iter. Total: 3.3011 s/iter. ETA=0:06:39\n",
            "[05/10 11:57:02 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0071 s/iter. Inference: 0.2271 s/iter. Eval: 3.0919 s/iter. Total: 3.3277 s/iter. ETA=0:06:35\n",
            "[05/10 11:57:08 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0069 s/iter. Inference: 0.2259 s/iter. Eval: 3.0479 s/iter. Total: 3.2823 s/iter. ETA=0:06:24\n",
            "[05/10 11:57:15 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0067 s/iter. Inference: 0.2252 s/iter. Eval: 3.1669 s/iter. Total: 3.4006 s/iter. ETA=0:06:34\n",
            "[05/10 11:57:20 d2.evaluation.evaluator]: Inference done 36/150. Dataloading: 0.0064 s/iter. Inference: 0.2228 s/iter. Eval: 3.1179 s/iter. Total: 3.3490 s/iter. ETA=0:06:21\n",
            "[05/10 11:57:28 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0069 s/iter. Inference: 0.2233 s/iter. Eval: 3.1768 s/iter. Total: 3.4087 s/iter. ETA=0:06:21\n",
            "[05/10 11:57:34 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0069 s/iter. Inference: 0.2239 s/iter. Eval: 3.2532 s/iter. Total: 3.4858 s/iter. ETA=0:06:26\n",
            "[05/10 11:57:43 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0068 s/iter. Inference: 0.2252 s/iter. Eval: 3.3834 s/iter. Total: 3.6174 s/iter. ETA=0:06:37\n",
            "[05/10 11:57:51 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0067 s/iter. Inference: 0.2237 s/iter. Eval: 3.4228 s/iter. Total: 3.6552 s/iter. ETA=0:06:34\n",
            "[05/10 11:58:00 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0065 s/iter. Inference: 0.2238 s/iter. Eval: 3.4554 s/iter. Total: 3.6875 s/iter. ETA=0:06:30\n",
            "[05/10 11:58:06 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0067 s/iter. Inference: 0.2248 s/iter. Eval: 3.5231 s/iter. Total: 3.7566 s/iter. ETA=0:06:34\n",
            "[05/10 11:58:14 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0066 s/iter. Inference: 0.2251 s/iter. Eval: 3.5247 s/iter. Total: 3.7584 s/iter. ETA=0:06:27\n",
            "[05/10 11:58:21 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0064 s/iter. Inference: 0.2237 s/iter. Eval: 3.5187 s/iter. Total: 3.7507 s/iter. ETA=0:06:18\n",
            "[05/10 11:58:26 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0067 s/iter. Inference: 0.2245 s/iter. Eval: 3.3930 s/iter. Total: 3.6262 s/iter. ETA=0:05:55\n",
            "[05/10 11:58:33 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0066 s/iter. Inference: 0.2253 s/iter. Eval: 3.4499 s/iter. Total: 3.6838 s/iter. ETA=0:05:57\n",
            "[05/10 11:58:38 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0063 s/iter. Inference: 0.2249 s/iter. Eval: 3.3427 s/iter. Total: 3.5760 s/iter. ETA=0:05:36\n",
            "[05/10 11:58:45 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0062 s/iter. Inference: 0.2229 s/iter. Eval: 3.3409 s/iter. Total: 3.5720 s/iter. ETA=0:05:28\n",
            "[05/10 11:58:53 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0062 s/iter. Inference: 0.2252 s/iter. Eval: 3.1596 s/iter. Total: 3.3929 s/iter. ETA=0:04:55\n",
            "[05/10 11:58:59 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0061 s/iter. Inference: 0.2259 s/iter. Eval: 3.0359 s/iter. Total: 3.2697 s/iter. ETA=0:04:31\n",
            "[05/10 11:59:05 d2.evaluation.evaluator]: Inference done 69/150. Dataloading: 0.0060 s/iter. Inference: 0.2255 s/iter. Eval: 3.0320 s/iter. Total: 3.2655 s/iter. ETA=0:04:24\n",
            "[05/10 11:59:13 d2.evaluation.evaluator]: Inference done 73/150. Dataloading: 0.0058 s/iter. Inference: 0.2218 s/iter. Eval: 2.9646 s/iter. Total: 3.1940 s/iter. ETA=0:04:05\n",
            "[05/10 11:59:19 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0059 s/iter. Inference: 0.2222 s/iter. Eval: 2.9963 s/iter. Total: 3.2261 s/iter. ETA=0:04:05\n",
            "[05/10 11:59:24 d2.evaluation.evaluator]: Inference done 77/150. Dataloading: 0.0059 s/iter. Inference: 0.2223 s/iter. Eval: 2.9351 s/iter. Total: 3.1651 s/iter. ETA=0:03:51\n",
            "[05/10 11:59:30 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0057 s/iter. Inference: 0.2236 s/iter. Eval: 2.8862 s/iter. Total: 3.1173 s/iter. ETA=0:03:38\n",
            "[05/10 11:59:35 d2.evaluation.evaluator]: Inference done 83/150. Dataloading: 0.0056 s/iter. Inference: 0.2231 s/iter. Eval: 2.8378 s/iter. Total: 3.0683 s/iter. ETA=0:03:25\n",
            "[05/10 11:59:43 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0056 s/iter. Inference: 0.2240 s/iter. Eval: 2.8201 s/iter. Total: 3.0514 s/iter. ETA=0:03:15\n",
            "[05/10 11:59:48 d2.evaluation.evaluator]: Inference done 89/150. Dataloading: 0.0055 s/iter. Inference: 0.2231 s/iter. Eval: 2.7723 s/iter. Total: 3.0026 s/iter. ETA=0:03:03\n",
            "[05/10 11:59:54 d2.evaluation.evaluator]: Inference done 91/150. Dataloading: 0.0054 s/iter. Inference: 0.2239 s/iter. Eval: 2.7677 s/iter. Total: 2.9989 s/iter. ETA=0:02:56\n",
            "[05/10 11:59:59 d2.evaluation.evaluator]: Inference done 95/150. Dataloading: 0.0053 s/iter. Inference: 0.2211 s/iter. Eval: 2.6938 s/iter. Total: 2.9220 s/iter. ETA=0:02:40\n",
            "[05/10 12:00:04 d2.evaluation.evaluator]: Inference done 97/150. Dataloading: 0.0053 s/iter. Inference: 0.2199 s/iter. Eval: 2.6891 s/iter. Total: 2.9161 s/iter. ETA=0:02:34\n",
            "[05/10 12:00:13 d2.evaluation.evaluator]: Inference done 99/150. Dataloading: 0.0052 s/iter. Inference: 0.2211 s/iter. Eval: 2.7177 s/iter. Total: 2.9458 s/iter. ETA=0:02:30\n",
            "[05/10 12:00:19 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0052 s/iter. Inference: 0.2223 s/iter. Eval: 2.6860 s/iter. Total: 2.9153 s/iter. ETA=0:02:19\n",
            "[05/10 12:00:24 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0051 s/iter. Inference: 0.2200 s/iter. Eval: 2.6269 s/iter. Total: 2.8537 s/iter. ETA=0:02:05\n",
            "[05/10 12:00:29 d2.evaluation.evaluator]: Inference done 109/150. Dataloading: 0.0051 s/iter. Inference: 0.2183 s/iter. Eval: 2.5947 s/iter. Total: 2.8197 s/iter. ETA=0:01:55\n",
            "[05/10 12:00:34 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0051 s/iter. Inference: 0.2184 s/iter. Eval: 2.5886 s/iter. Total: 2.8137 s/iter. ETA=0:01:49\n",
            "[05/10 12:00:40 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0051 s/iter. Inference: 0.2190 s/iter. Eval: 2.5660 s/iter. Total: 2.7918 s/iter. ETA=0:01:40\n",
            "[05/10 12:00:49 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0051 s/iter. Inference: 0.2192 s/iter. Eval: 2.6164 s/iter. Total: 2.8425 s/iter. ETA=0:01:39\n",
            "[05/10 12:00:55 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0051 s/iter. Inference: 0.2186 s/iter. Eval: 2.6193 s/iter. Total: 2.8448 s/iter. ETA=0:01:33\n",
            "[05/10 12:01:04 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0050 s/iter. Inference: 0.2182 s/iter. Eval: 2.6266 s/iter. Total: 2.8515 s/iter. ETA=0:01:25\n",
            "[05/10 12:01:09 d2.evaluation.evaluator]: Inference done 123/150. Dataloading: 0.0050 s/iter. Inference: 0.2182 s/iter. Eval: 2.5973 s/iter. Total: 2.8223 s/iter. ETA=0:01:16\n",
            "[05/10 12:01:15 d2.evaluation.evaluator]: Inference done 126/150. Dataloading: 0.0050 s/iter. Inference: 0.2188 s/iter. Eval: 2.5787 s/iter. Total: 2.8042 s/iter. ETA=0:01:07\n",
            "[05/10 12:01:20 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0050 s/iter. Inference: 0.2175 s/iter. Eval: 2.5546 s/iter. Total: 2.7787 s/iter. ETA=0:00:58\n",
            "[05/10 12:01:26 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0049 s/iter. Inference: 0.2184 s/iter. Eval: 2.4686 s/iter. Total: 2.6935 s/iter. ETA=0:00:40\n",
            "[05/10 12:01:32 d2.evaluation.evaluator]: Inference done 138/150. Dataloading: 0.0050 s/iter. Inference: 0.2188 s/iter. Eval: 2.4543 s/iter. Total: 2.6798 s/iter. ETA=0:00:32\n",
            "[05/10 12:01:38 d2.evaluation.evaluator]: Inference done 142/150. Dataloading: 0.0050 s/iter. Inference: 0.2196 s/iter. Eval: 2.4137 s/iter. Total: 2.6399 s/iter. ETA=0:00:21\n",
            "[05/10 12:01:43 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0050 s/iter. Inference: 0.2178 s/iter. Eval: 2.3978 s/iter. Total: 2.6222 s/iter. ETA=0:00:13\n",
            "[05/10 12:01:48 d2.evaluation.evaluator]: Inference done 148/150. Dataloading: 0.0049 s/iter. Inference: 0.2167 s/iter. Eval: 2.3791 s/iter. Total: 2.6023 s/iter. ETA=0:00:05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/10 12:01:51 d2.evaluation.evaluator]: Total inference time: 0:06:14.999883 (2.586206 s / iter per device, on 1 devices)\n",
            "[05/10 12:01:51 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:31 (0.215733 s / iter per device, on 1 devices)\n",
            "[05/10 12:01:51 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/10 12:01:51 d2.evaluation.coco_evaluation]: Saving results to /content/output/chkpt/inference/coco_instances_results.json\n",
            "[05/10 12:01:51 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 12:01:51 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/10 12:01:51 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.10 seconds.\n",
            "[05/10 12:01:51 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 12:01:52 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.06 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[05/10 12:01:52 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[05/10 12:01:52 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP    | category              | AP    | category   | AP    |\n",
            "|:-----------|:------|:----------------------|:------|:-----------|:------|\n",
            "| Bottle     | 0.000 | Bottle cap            | 0.000 | Can        | 0.000 |\n",
            "| Cigarette  | 0.000 | Cup                   | 0.000 | Lid        | 0.000 |\n",
            "| Other      | 0.000 | Plastic bag & wrapper | 0.000 | Pop tab    | 0.000 |\n",
            "| Straw      | 0.000 |                       |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.41s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/10 12:01:52 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[05/10 12:01:53 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.47 seconds.\n",
            "[05/10 12:01:53 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/10 12:01:53 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.06 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.124\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.220\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.105\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.043\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.216\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.277\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.284\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.118\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380\n",
            "[05/10 12:01:53 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 12.388 | 21.968 | 10.453 | 0.008 | 4.278 | 16.715 |\n",
            "[05/10 12:01:53 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category   | AP     | category              | AP     | category   | AP     |\n",
            "|:-----------|:-------|:----------------------|:-------|:-----------|:-------|\n",
            "| Bottle     | 27.680 | Bottle cap            | 24.593 | Can        | 8.018  |\n",
            "| Cigarette  | 1.595  | Cup                   | 12.454 | Lid        | 14.178 |\n",
            "| Other      | 8.912  | Plastic bag & wrapper | 23.802 | Pop tab    | 0.045  |\n",
            "| Straw      | 2.599  |                       |        |            |        |\n",
            "[05/10 12:01:53 d2.engine.defaults]: Evaluation results for TACO_val in csv format:\n",
            "[05/10 12:01:53 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[05/10 12:01:53 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 12:01:53 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[05/10 12:01:53 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[05/10 12:01:53 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/10 12:01:53 d2.evaluation.testing]: copypaste: 12.3876,21.9676,10.4528,0.0083,4.2785,16.7154\n",
            "[05/10 12:01:53 d2.utils.events]:  eta: 0:00:00  iter: 5747  total_loss: 0.6734  loss_ins: 0.5405  loss_cate: 0.1502  time: 3.2856  data_time: 1.8581  lr: 0.0001  max_mem: 9678M\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "OrderedDict([('bbox',\n",
              "              {'AP': 0.0,\n",
              "               'AP50': 0.0,\n",
              "               'AP75': 0.0,\n",
              "               'APs': 0.0,\n",
              "               'APm': 0.0,\n",
              "               'APl': 0.0,\n",
              "               'AP-Bottle': 0.0,\n",
              "               'AP-Bottle cap': 0.0,\n",
              "               'AP-Can': 0.0,\n",
              "               'AP-Cigarette': 0.0,\n",
              "               'AP-Cup': 0.0,\n",
              "               'AP-Lid': 0.0,\n",
              "               'AP-Other': 0.0,\n",
              "               'AP-Plastic bag & wrapper': 0.0,\n",
              "               'AP-Pop tab': 0.0,\n",
              "               'AP-Straw': 0.0}),\n",
              "             ('segm',\n",
              "              {'AP': 12.38762151415335,\n",
              "               'AP50': 21.96755226726416,\n",
              "               'AP75': 10.452759541690543,\n",
              "               'APs': 0.00825082508250825,\n",
              "               'APm': 4.278477916381564,\n",
              "               'APl': 16.71542642024696,\n",
              "               'AP-Bottle': 27.679730827190497,\n",
              "               'AP-Bottle cap': 24.59293810272514,\n",
              "               'AP-Can': 8.017971522106816,\n",
              "               'AP-Cigarette': 1.595264536529435,\n",
              "               'AP-Cup': 12.454409879637003,\n",
              "               'AP-Lid': 14.177918135062932,\n",
              "               'AP-Other': 8.91154771488288,\n",
              "               'AP-Plastic bag & wrapper': 23.80179038756663,\n",
              "               'AP-Pop tab': 0.04537953795379538,\n",
              "               'AP-Straw': 2.5992644978783592})])"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.resume_or_load(resume = True)  # load last checkpoint or MODEL.WEIGHTS\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

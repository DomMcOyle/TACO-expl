{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomMcOyle/TACO-expl/blob/add_detr/Training%20Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW0_gODeKIeQ",
        "outputId": "50cfae04-0ab4-464c-afa9-c2fb353de484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TACO-expl' already exists and is not an empty directory.\n",
            "/content/TACO-expl\n",
            "M\tHDDETR/models/segmentation.py\n",
            "Already on 'add_detr'\n",
            "Your branch is up to date with 'origin/add_detr'.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/DomMcOyle/TACO-expl\n",
        "%cd /content/TACO-expl/\n",
        "!git checkout add_detr\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUv0ZCdoQwPv"
      },
      "source": [
        "After running the following cell, reset the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwcWEhm-wBCf"
      },
      "outputs": [],
      "source": [
        "%cd /content/TACO-expl/HDDETR\n",
        "!pip install -r requirements.txt\n",
        "!pip install mmcv==2.1.0 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.1/index.html\n",
        "!pip install mmdet\n",
        "%cd /content/TACO-expl/HDDETR/models/ops\n",
        "!python setup.py build install\n",
        "# unit test (should see all checking is True)\n",
        "#!python test.py\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODG94BFmYgl_",
        "outputId": "3ada1ead-5280-4ff0-aa47-260be8131cd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ],
      "source": [
        "!git pull origin add_detr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XATP-Y3ZglE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4ab0b5-2f99-4154-f73a-e1154647a830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TACO-expl\n",
            "Drive already mounted at /content/MyDrive/; to attempt to forcibly remount, call drive.mount(\"/content/MyDrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "%cd TACO-expl\n",
        "import os.path\n",
        "import json\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import datetime as dt\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import torch.utils.data\n",
        "from torch.utils.data import DataLoader\n",
        "from pycocotools import mask as coco_mask\n",
        "\n",
        "from HDDETR.datasets.torchvision_datasets.coco import CocoDetection as TvCocoDetection\n",
        "import HDDETR.datasets.transforms as T\n",
        "from HDDETR.datasets.data_prefetcher import data_prefetcher\n",
        "from HDDETR.datasets.coco_eval import CocoEvaluator\n",
        "from HDDETR.models.deformable_transformer import DeformableTransformerEncoderLayer\n",
        "from torchvision.ops import RoIAlign\n",
        "import HDDETR.util.misc as mutils\n",
        "from HDDETR.util import box_ops\n",
        "\n",
        "import HDDETR.models\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/MyDrive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2dI-mnxFBNC"
      },
      "source": [
        "#split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KutH3M6PmlFU"
      },
      "outputs": [],
      "source": [
        "keep_categories = [\"Bottle\", \"Bottle cap\", \"Can\", \"Cigarette\", \"Cup\",\n",
        "                   \"Lid\", \"Plastic bag & wrapper\", \"Pop tab\", \"Straw\"]\n",
        "\n",
        "def create_map(original, keep_supercategories):\n",
        "  class_map = {}\n",
        "  for cat in original:\n",
        "    if cat[\"supercategory\"] in keep_supercategories:\n",
        "      class_map[cat[\"name\"]] = cat[\"supercategory\"]\n",
        "    else:\n",
        "      class_map[cat[\"name\"]] = \"Other\"\n",
        "  return class_map\n",
        "\n",
        "def replace_dataset_classes(dataset, class_map):\n",
        "      \"\"\" Replaces classes of dataset based on a dictionary\"\"\"\n",
        "      class_new_names = list(set(class_map.values()))\n",
        "      class_new_names.sort()\n",
        "      class_originals = copy.deepcopy(dataset['categories'])\n",
        "      dataset['categories'] = []\n",
        "      class_ids_map = {}  # map from old id to new id\n",
        "\n",
        "      # Assign background id 0\n",
        "      has_background = False\n",
        "      if 'Background' in class_new_names:\n",
        "          if class_new_names.index('Background') != 0:\n",
        "              class_new_names.remove('Background')\n",
        "              class_new_names.insert(0, 'Background')\n",
        "          has_background = True\n",
        "\n",
        "      # Replace categories\n",
        "      for id_new, class_new_name in enumerate(class_new_names):\n",
        "          # Make sure id:0 is reserved for background\n",
        "          id_rectified = id_new\n",
        "          if not has_background:\n",
        "              id_rectified += 1\n",
        "\n",
        "          category = {\n",
        "              'supercategory': '',\n",
        "              'id': id_rectified,  # Background has id=0\n",
        "              'name': class_new_name,\n",
        "          }\n",
        "          dataset['categories'].append(category)\n",
        "          # Map class names\n",
        "          for class_original in class_originals:\n",
        "              if class_map[class_original['name']] == class_new_name:\n",
        "                  class_ids_map[class_original['id']] = id_rectified\n",
        "\n",
        "      # Update annotations category id tag\n",
        "      for ann in dataset['annotations']:\n",
        "          ann['category_id'] = class_ids_map[ann['category_id']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RclP141dy5RU"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/TACO-expl/data/annotations.json\", \"r\") as f:\n",
        "    dataset = json.loads(f.read())\n",
        "\n",
        "class_map = create_map(dataset[\"categories\"], keep_categories)\n",
        "replace_dataset_classes(dataset, class_map)\n",
        "dataset[\"categories\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ann_input_path = '/content/TACO-expl/data/annotations_unofficial.json'\n",
        "\n",
        "# Load annotations\n",
        "with open(ann_input_path, 'r') as f:\n",
        "    dataset = json.loads(f.read())"
      ],
      "metadata": {
        "id": "V2eTUIDihrRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDh8FFzFhqut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1df67ab-1fef-4bc8-b830-66d9350bbb7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "removed image:\n",
            "{'width': 3024, 'flickr_640_url': 'https://farm66.staticflickr.com/65535/51619246111_ee6756e766_z.jpg', 'license': 'CC', 'file_name': 'unofficial/000695.jpg', 'date_captured': None, 'coco_url': None, 'flickr_url': 'https://farm66.staticflickr.com/65535/51619246111_f71c90527a_o.png', 'id': 695, 'height': 4032}\n",
            "\n",
            "removed image:\n",
            "{'width': 3000, 'flickr_640_url': 'https://farm66.staticflickr.com/65535/51276178289_2c1a52e70c_z.jpg', 'license': 'CC', 'file_name': 'unofficial/001983.jpg', 'date_captured': None, 'coco_url': None, 'flickr_url': 'https://farm66.staticflickr.com/65535/51276178289_8d7672f4a3_o.png', 'id': 1983, 'height': 4000}\n",
            "\n",
            "removed image:\n",
            "{'width': 3000, 'flickr_640_url': 'https://farm66.staticflickr.com/65535/49042140463_03d68b0bbc_z.jpg', 'license': 'CC', 'file_name': 'unofficial/002213.jpg', 'date_captured': None, 'coco_url': None, 'flickr_url': 'https://farm66.staticflickr.com/65535/49042140463_20aa4445e2_o.png', 'id': 2213, 'height': 4000}\n",
            "\n",
            "removed image:\n",
            "{'width': 804, 'flickr_640_url': 'https://farm66.staticflickr.com/65535/50834208797_d287244b8d_z.jpg', 'license': 'CC', 'file_name': 'unofficial/002801.jpg', 'date_captured': None, 'coco_url': None, 'flickr_url': 'https://farm66.staticflickr.com/65535/50834208797_57fdf1190c_o.png', 'id': 2801, 'height': 1320}\n",
            "\n",
            "removed image:\n",
            "{'width': 3000, 'flickr_640_url': 'https://farm66.staticflickr.com/65535/51316887964_de3d27e282_z.jpg', 'license': 'CC', 'file_name': 'unofficial/003113.jpg', 'date_captured': None, 'coco_url': None, 'flickr_url': 'https://farm66.staticflickr.com/65535/51316887964_d0b5aacdfc_o.png', 'id': 3113, 'height': 4000}\n",
            "\n",
            "removed image:\n",
            "{'width': 3024, 'flickr_640_url': 'https://farm66.staticflickr.com/65535/51619247376_3f7a75024f_z.jpg', 'license': 'CC', 'file_name': 'unofficial/003507.jpg', 'date_captured': None, 'coco_url': None, 'flickr_url': 'https://farm66.staticflickr.com/65535/51619247376_b71b8c56e2_o.png', 'id': 3507, 'height': 4032}\n",
            "\n",
            "removed image:\n",
            "{'width': 3000, 'flickr_640_url': 'https://farm66.staticflickr.com/65535/49578175608_dd8a774234_z.jpg', 'license': 'CC', 'file_name': 'unofficial/003683.jpg', 'date_captured': None, 'coco_url': None, 'flickr_url': 'https://farm66.staticflickr.com/65535/49578175608_be905c1963_o.png', 'id': 3683, 'height': 4000}\n",
            "\n",
            "[{'category_id': 7, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 107, 'image_id': 53}, {'category_id': 7, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 974, 'image_id': 473}, {'category_id': 7, 'bbox': [8176.14, 12861.421, 595.3499999999995, 635.039999999999], 'segmentation': [[8652, 13496, 8771, 13100, 8414, 12861, 8176, 13179]], 'area': 198487.18952999962, 'iscrowd': 0, 'id': 1422, 'image_id': 695}, {'category_id': 1, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 1486, 'image_id': 738}, {'category_id': 7, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 2599, 'image_id': 1268}, {'category_id': 10, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 2854, 'image_id': 1394}, {'category_id': 7, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 3246, 'image_id': 1577}, {'category_id': 1, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 4166, 'image_id': 1983}, {'category_id': 7, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 4660, 'image_id': 2213}, {'category_id': 8, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 6044, 'image_id': 2801}, {'category_id': 10, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 6717, 'image_id': 3113}, {'category_id': 7, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 7597, 'image_id': 3507}, {'category_id': 7, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 7937, 'image_id': 3670}, {'category_id': 1, 'bbox': [11438.89, 15641.385, 2750.0, 7061.110999999999], 'segmentation': [[12808, 15641, 13245, 15654, 13676, 15723, 13714, 15873, 13776, 16304, 13839, 16610, 14170, 17104, 14189, 17460, 14089, 18143, 14120, 18812, 14095, 19325, 13989, 19940, 13964, 20546, 13989, 20946, 13883, 21271, 13776, 21640, 13458, 22471, 13089, 22640, 12620, 22702, 12089, 22521, 11801, 22352, 11589, 22034, 11439, 21465, 11451, 20927, 11526, 19890, 11676, 19370, 11670, 18826, 11751, 18395, 11945, 17324, 12151, 16949, 12326, 16630, 12551, 16330, 12651, 16112, 12645, 15861]], 'area': 14690336.965624997, 'iscrowd': 0, 'id': 7964, 'image_id': 3683}, {'category_id': 1, 'bbox': [8232.64, 13913.087, 3875.0, 5054.167000000001], 'segmentation': [[8233, 14951, 8626, 14513, 8908, 14269, 9408, 14069, 9801, 13913, 9958, 13982, 10089, 14226, 10220, 14457, 10383, 14576, 10933, 14488, 11251, 14988, 11545, 15413, 11695, 15932, 11945, 16294, 12001, 16596, 12108, 16940, 11864, 17778, 11770, 18059, 11651, 18440, 11514, 18561, 11233, 18849, 10839, 18967, 10576, 18867, 10276, 18442, 9983, 17948, 9745, 18023, 9783, 18242, 9901, 18429, 9720, 18567, 9458, 18248, 9258, 17635, 9064, 17187, 8695, 16555, 8408, 15812, 8320, 15666, 8251, 15266]], 'area': 12438213.134375002, 'iscrowd': 0, 'id': 7965, 'image_id': 3683}, {'category_id': 7, 'bbox': [6133.3335, 2075.586, 3393.749500000001, 2882.6390000000006], 'segmentation': [[6377, 2076, 6777, 2138, 7396, 2382, 7852, 2551, 8383, 2832, 9033, 3026, 9377, 3088, 9496, 3444, 9527, 3932, 9408, 4519, 9240, 4738, 9115, 4958, 8215, 4727, 7021, 4508, 6271, 4352, 6133, 4133, 6202, 3175, 6333, 2650, 6333, 2194]], 'area': 6685012.383680503, 'iscrowd': 0, 'id': 7966, 'image_id': 3683}, {'category_id': 7, 'bbox': [7440.9727, 6766.5586, 5975.0003, 7843.750399999999], 'segmentation': [[7478, 7529, 7853, 7417, 8353, 7335, 8828, 7379, 9210, 7529, 9291, 7292, 9285, 6942, 9391, 6767, 9722, 6867, 10322, 7260, 10910, 7704, 11191, 7960, 11547, 8542, 11828, 9054, 11922, 9467, 12041, 10112, 12303, 10487, 12535, 11006, 12791, 11475, 12978, 11800, 13028, 12058, 13160, 12658, 13241, 13196, 13416, 13533, 13328, 13814, 13066, 13939, 12741, 13960, 12097, 14110, 11491, 14329, 11053, 14435, 10566, 14592, 10222, 14610, 10078, 14304, 9885, 13879, 9653, 13192, 9366, 12783, 9066, 12433, 8785, 11933, 8578, 11371, 8297, 11052, 7847, 10643, 7753, 10280, 7647, 9887, 7491, 9124, 7447, 8568, 7441, 8347, 7522, 7991, 7547, 7710]], 'area': 27865044.14958336, 'iscrowd': 0, 'id': 7967, 'image_id': 3683}, {'category_id': 5, 'bbox': [1012.5, 15851.976, 5225.0, 4958.3330000000005], 'segmentation': [[1012, 17552, 1238, 17314, 1394, 17039, 1638, 16864, 1906, 16589, 2188, 16314, 2488, 16002, 2688, 15908, 2812, 15852, 3288, 16002, 3750, 16239, 4175, 16483, 4888, 16977, 6006, 17583, 6238, 17852, 6019, 18135, 5756, 18442, 5612, 18773, 5388, 19142, 4700, 19773, 4069, 20373, 3438, 20810, 3069, 20418, 2669, 19937, 2344, 19568, 1975, 19018, 1688, 18760, 1350, 18235]], 'area': 14432976.11562501, 'iscrowd': 0, 'id': 7968, 'image_id': 3683}, {'category_id': 7, 'bbox': [3868.75, 5172.8086, 2668.75, 2981.25], 'segmentation': [[3981, 7748, 4319, 7379, 4550, 7023, 4719, 6442, 4719, 5985, 4738, 5679, 4869, 5404, 5306, 5235, 5831, 5173, 6056, 5298, 6419, 5460, 6538, 5648, 6538, 5767, 6312, 6142, 6125, 6467, 5925, 6779, 5744, 6923, 5631, 7210, 5425, 7404, 5019, 7604, 4862, 8004, 4381, 8054, 3869, 8154]], 'area': 3743300.78125, 'iscrowd': 0, 'id': 7969, 'image_id': 3683}, {'category_id': 7, 'bbox': [237.5, 18567.254, 1643.75, 2995.138999999999], 'segmentation': [[606, 18761, 1056, 18711, 1325, 18599, 1550, 18567, 1612, 18699, 1675, 18842, 1800, 18949, 1881, 19086, 1794, 19380, 1706, 20324, 1694, 20930, 1619, 21449, 512, 21562, 238, 21487, 294, 21100, 344, 20737, 444, 20344, 531, 19962, 481, 19774, 550, 19336, 506, 19017]], 'area': 3643999.6281249993, 'iscrowd': 0, 'id': 7970, 'image_id': 3683}, {'category_id': 8, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 8061, 'image_id': 3713}, {'category_id': 4, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 8211, 'image_id': 3774}, {'category_id': 4, 'bbox': [inf, inf, -inf, -inf], 'segmentation': [], 'area': 0, 'iscrowd': 0, 'id': 8258, 'image_id': 3800}]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "parser = argparse.ArgumentParser(description='User args')\n",
        "parser.add_argument('--dataset_dir', required=True, help='Path to dataset annotations')\n",
        "parser.add_argument('--test_percentage', type=int, default=10, required=False, help='Percentage of images used for the testing set')\n",
        "parser.add_argument('--val_percentage', type=int, default=10, required=False, help='Percentage of images used for the validation set')\n",
        "parser.add_argument('--nr_trials', type=int, default=10, required=False, help='Number of splits')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\"\"\"\n",
        "args = {\n",
        "    \"nr_trials\":1,\n",
        "    \"test_percentage\":0.1,\n",
        "    \"val_percentage\":0.1,\n",
        "    \"dataset_dir\":'/content/TACO-expl/data'\n",
        "}\n",
        "\n",
        "ann_input_path = args[\"dataset_dir\"] + '/annotations_unofficial.json'\n",
        "\n",
        "# Load annotations\n",
        "with open(ann_input_path, 'r') as f:\n",
        "    dataset = json.loads(f.read())\n",
        "\n",
        "keep_categories = [\"Bottle\", \"Bottle cap\", \"Can\", \"Cigarette\", \"Cup\",\n",
        "                   \"Lid\", \"Plastic bag & wrapper\", \"Pop tab\", \"Straw\"]\n",
        "if keep_categories is not None:\n",
        "  class_map = create_map(dataset[\"categories\"], keep_categories)\n",
        "  replace_dataset_classes(dataset, class_map)\n",
        "\n",
        "anns = dataset['annotations']\n",
        "scene_anns = dataset['scene_annotations']\n",
        "imgs = dataset['images']\n",
        "nr_images = len(imgs)\n",
        "\n",
        "dimensions = {im['id']: [im['height'], im['width']] for im in dataset['images']}\n",
        "bad_ann = []\n",
        "image_with_at_least_one_ann = set()\n",
        "# check sanity of annotation\n",
        "for a in anns:\n",
        "    h, w = dimensions[a[\"image_id\"]]\n",
        "    boxes = a[\"bbox\"]\n",
        "    # guard against no boxes via resizing\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "    boxes[:, 2:] += boxes[:, :2]\n",
        "    boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "    boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "    segmentations = True if a[\"segmentation\"] else False\n",
        "\n",
        "    keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "    if (keep and segmentations):\n",
        "      image_with_at_least_one_ann.add(a[\"image_id\"])\n",
        "    else:\n",
        "      bad_ann.append(a)\n",
        "\n",
        "\n",
        "imgs = []\n",
        "# if image is without boxes is removed\n",
        "for i in dataset['images']:\n",
        "  if i[\"id\"] in image_with_at_least_one_ann:\n",
        "    imgs.append(i)\n",
        "  else:\n",
        "    print(\"removed image:\")\n",
        "    print(i)\n",
        "    print()\n",
        "print(bad_ann)\n",
        "\n",
        "for i in range(args[\"nr_trials\"]):\n",
        "    #random.shuffle(imgs)\n",
        "\n",
        "    # Add new datasets\n",
        "    train_set = {\n",
        "        'info': None,\n",
        "        'images': [],\n",
        "        'annotations': [],\n",
        "        'scene_annotations': [],\n",
        "        'licenses': [],\n",
        "        'categories': [],\n",
        "        'scene_categories': [],\n",
        "    }\n",
        "    train_set['info'] =  dataset['info']\n",
        "    train_set['categories'] = dataset['categories']\n",
        "    train_set['scene_categories'] = dataset['scene_categories']\n",
        "\n",
        "    val_set = copy.deepcopy(train_set)\n",
        "    test_set = copy.deepcopy(train_set)\n",
        "\n",
        "    train_set['images'], partial = train_test_split(imgs,\n",
        "                                                    random_state=42,\n",
        "                                                               test_size=args[\"test_percentage\"]+args[\"val_percentage\"])\n",
        "    val_set['images'], test_set[\"images\"] = train_test_split(partial,\n",
        "                                                             random_state=42,\n",
        "                                                             test_size=args[\"test_percentage\"]/(args[\"test_percentage\"]+args[\"val_percentage\"]))\n",
        "\n",
        "    # Aux Image Ids to split annotations\n",
        "    test_img_ids, val_img_ids, train_img_ids = [],[],[]\n",
        "    for img in test_set['images']:\n",
        "        test_img_ids.append(img['id'])\n",
        "\n",
        "    for img in val_set['images']:\n",
        "        val_img_ids.append(img['id'])\n",
        "\n",
        "    for img in train_set['images']:\n",
        "        train_img_ids.append(img['id'])\n",
        "\n",
        "    # Split instance annotations\n",
        "    for ann in anns:\n",
        "        if ann['image_id'] in test_img_ids:\n",
        "            test_set['annotations'].append(ann)\n",
        "        elif ann['image_id'] in val_img_ids:\n",
        "            val_set['annotations'].append(ann)\n",
        "        elif ann['image_id'] in train_img_ids:\n",
        "            train_set['annotations'].append(ann)\n",
        "\n",
        "    # Split scene tags\n",
        "    for ann in scene_anns:\n",
        "        if ann['image_id'] in test_img_ids:\n",
        "            test_set['scene_annotations'].append(ann)\n",
        "        elif ann['image_id'] in val_img_ids:\n",
        "            val_set['scene_annotations'].append(ann)\n",
        "        elif ann['image_id'] in train_img_ids:\n",
        "            train_set['scene_annotations'].append(ann)\n",
        "\n",
        "    # Write dataset splits\n",
        "    ann_train_out_path = args[\"dataset_dir\"] + '/' + 'annotations_c_' + str(i) +'_train.json'\n",
        "    ann_val_out_path   = args[\"dataset_dir\"] + '/' + 'annotations_c_' + str(i) + '_val.json'\n",
        "    ann_test_out_path  = args[\"dataset_dir\"] + '/' + 'annotations_c_' + str(i) + '_test.json'\n",
        "\n",
        "    with open(ann_train_out_path, 'w+') as f:\n",
        "        f.write(json.dumps(train_set))\n",
        "\n",
        "    with open(ann_val_out_path, 'w+') as f:\n",
        "        f.write(json.dumps(val_set))\n",
        "\n",
        "    with open(ann_test_out_path, 'w+') as f:\n",
        "        f.write(json.dumps(test_set))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s9VTnF5FG-q"
      },
      "source": [
        "#detr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SMqe-f0uqtjo"
      },
      "outputs": [],
      "source": [
        "class TACODataset(TvCocoDetection):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_folder,\n",
        "        ann_file,\n",
        "        transforms,\n",
        "        cache_mode=False,\n",
        "        local_rank=0,\n",
        "        local_size=1,\n",
        "        use_crowd=False,\n",
        "    ):\n",
        "        super(TACODataset, self).__init__(\n",
        "            img_folder,\n",
        "            ann_file,\n",
        "            cache_mode=cache_mode,\n",
        "            local_rank=local_rank,\n",
        "            local_size=local_size,\n",
        "        )\n",
        "        self._transforms = transforms\n",
        "        self.prepare = ConvertCocoPolysToMask(use_crowd)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super(TACODataset, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = {\"image_id\": image_id, \"annotations\": target}\n",
        "        img, target = self.prepare(img, target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class ConvertCocoPolysToMask(object):\n",
        "    def __init__(self, use_crowd):\n",
        "       self.use_crowd = use_crowd\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "        image_id = torch.tensor([image_id])\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        if not self.use_crowd:\n",
        "          anno = [obj for obj in anno if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0]\n",
        "\n",
        "        # condition as sanity check for empty segmentations\n",
        "        boxes = [obj[\"bbox\"] for obj in anno if obj[\"segmentation\"]]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno if obj[\"segmentation\"]]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64) - 1 # for labels between 0 and 9\n",
        "\n",
        "        segmentations = [obj[\"segmentation\"] for obj in anno if obj[\"segmentation\"]]\n",
        "        masks = self.convert_coco_poly_to_mask(segmentations=segmentations, height=h, width=w)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        masks = masks[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "\n",
        "        # for conversion to coco api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno if obj[\"segmentation\"]])\n",
        "        iscrowd = torch.tensor(\n",
        "            [obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno if obj[\"segmentation\"]]\n",
        "        )\n",
        "        target[\"area\"] = area[keep]\n",
        "        target[\"iscrowd\"] = iscrowd[keep]\n",
        "\n",
        "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
        "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def convert_coco_poly_to_mask(self, segmentations, height, width):\n",
        "      masks = []\n",
        "      for polygons in segmentations:\n",
        "          rles = coco_mask.frPyObjects(polygons, height, width)\n",
        "\n",
        "          mask = coco_mask.decode(rles)\n",
        "          if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "          mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "          mask = mask.any(dim=2)\n",
        "          masks.append(mask)\n",
        "      if masks:\n",
        "          masks = torch.stack(masks, dim=0)\n",
        "      else:\n",
        "          masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "      return masks\n",
        "\n",
        "\n",
        "def make_coco_transforms(image_set):\n",
        "\n",
        "    normalize = T.Compose(\n",
        "        [T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
        "    )\n",
        "    #max_size = 1333\n",
        "    max_size = 1024\n",
        "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
        "              # 832, 864, 896, 928, 960]\n",
        "\n",
        "    if image_set == \"train\":\n",
        "        return T.Compose(\n",
        "            [\n",
        "                T.RandomHorizontalFlip(),\n",
        "                T.RandomSelect(\n",
        "                    T.RandomResize(scales, max_size=max_size),\n",
        "                    T.Compose(\n",
        "                        [\n",
        "                            # T.RandomResize([400, 500, 600])\n",
        "                            T.RandomResize([2000, 2500, 3000]),\n",
        "                            #T.RandomSizeCrop(384, 600),\n",
        "                            T.RandomSizeCrop(1920, 3000),\n",
        "                            T.RandomResize(scales, max_size=max_size),\n",
        "                        ]\n",
        "                    ),\n",
        "                ),\n",
        "                normalize,\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    if image_set == \"val\":\n",
        "        return T.Compose([T.RandomResize([800], max_size=max_size), normalize,])\n",
        "    if image_set == \"test\":\n",
        "        return None\n",
        "\n",
        "    raise ValueError(f\"unknown {image_set}\")\n",
        "\n",
        "def create_dataset(split):\n",
        "  ann_file = Path(\"/content/TACO-expl/data/annotations_c_0_\" + split +\".json\")\n",
        "  img_folder = Path(\"/content/MyDrive/MyDrive/\")\n",
        "\n",
        "  dataset = TACODataset(\n",
        "      img_folder,\n",
        "      ann_file,\n",
        "      transforms=make_coco_transforms(split),\n",
        "      local_rank=mutils.get_local_rank(),\n",
        "      local_size=mutils.get_local_size(),\n",
        "  )\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ERweJ8HTfm9",
        "outputId": "b409e851-c9fd-44c5-ea74-71e124c26d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-13 19:31:49--  https://github.com/HDETR/H-Deformable-DETR/releases/download/v0.1/r50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/517883062/ce62ee7a-43ec-4230-8bf1-348a1530d246?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240213%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240213T193150Z&X-Amz-Expires=300&X-Amz-Signature=1da9fa91357ecfd533e468913b0aa0885ee393afc7bde745642904523dbf503d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=517883062&response-content-disposition=attachment%3B%20filename%3Dr50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-02-13 19:31:50--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/517883062/ce62ee7a-43ec-4230-8bf1-348a1530d246?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240213%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240213T193150Z&X-Amz-Expires=300&X-Amz-Signature=1da9fa91357ecfd533e468913b0aa0885ee393afc7bde745642904523dbf503d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=517883062&response-content-disposition=attachment%3B%20filename%3Dr50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 192422339 (184M) [application/octet-stream]\n",
            "Saving to: ‘r50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth’\n",
            "\n",
            "r50_hybrid_branch_l 100%[===================>] 183.51M   248MB/s    in 0.7s    \n",
            "\n",
            "2024-02-13 19:31:50 (248 MB/s) - ‘r50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth’ saved [192422339/192422339]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/HDETR/H-Deformable-DETR/releases/download/v0.1/r50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth\n",
        "!mv r50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth r50.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hcD6Qhp9AE7K"
      },
      "outputs": [],
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "args = AttrDict()\n",
        "args.output_dir = './out/'\n",
        "args.with_box_refine = True\n",
        "args.two_stage = True\n",
        "args.dim_feedforward = 2048\n",
        "args.num_queries_one2one = 300\n",
        "args.num_queries_one2many = 1500\n",
        "args.k_one2many = 6\n",
        "args.lambda_one2many = 1.0\n",
        "args.mixed_selection = True\n",
        "args.look_forward_twice = True\n",
        "args.dataset_file = \"coco\"\n",
        "args.device = 'cuda'\n",
        "args.hidden_dim = 256\n",
        "args.position_embedding = 'sine'\n",
        "args.position_embedding_scale = np.pi *2\n",
        "#args.lr_backbone =2e-5\n",
        "args.lr_backbone = 0\n",
        "args.masks = False\n",
        "args.num_feature_levels = 4\n",
        "args.backbone = \"resnet50\"\n",
        "args.dilation = False\n",
        "args.nheads = 8\n",
        "args.enc_layers = 6\n",
        "args.dec_layers = 6\n",
        "args.dim_feedforwards = 2048\n",
        "args.dropout = 0\n",
        "args.dec_n_points = 4\n",
        "args.enc_n_points = 4\n",
        "args.use_checkpoint = True\n",
        "args.aux_loss = False\n",
        "args.cls_loss_coef=2\n",
        "args.giou_loss_coef=0 #2\n",
        "args.focal_alpha=0.25\n",
        "args.topk=100\n",
        "args.bbox_loss_coef=0 #5\n",
        "args.set_cost_class=2\n",
        "args.set_cost_bbox=5\n",
        "args.set_cost_giou=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jAq0eFHI2YUO"
      },
      "outputs": [],
      "source": [
        "def load_detr(args):\n",
        "  model,crit, postproc = HDDETR.models.build(args)\n",
        "  postproc.update({\"segm\": HDDETR.models.segmentation.PostProcessSegmMFD()})\n",
        "  crit.losses.append(\"masksMFD\")\n",
        "  crit.losses.remove(\"boxes\")\n",
        "  crit.num_classes = 10\n",
        "  crit.weight_dict[\"loss_mask\"] = 1\n",
        "  crit.weight_dict[\"loss_dice\"] = 1\n",
        "  model.num_queries = 300\n",
        "  model.transformer.two_stage_num_proposals = 300\n",
        "  model.load_state_dict(torch.load(\"r50.pth\")[\"model\"])\n",
        "  return model, crit, postproc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Aa-WuoDL55-M",
        "outputId": "60e85507-514b-4acc-a28f-f8415f3ef1fe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom HDDETR.util.misc import NestedTensor\\nfrom PIL import Image\\nimport torch\\nfrom torchvision import transforms\\n\\ndev = torch.device(\"cuda\")\\nmodel.to(dev)\\nto_t = transforms.ToTensor()\\nimg = Image.open(\"treno.jpg\")\\na = to_t(img).reshape((1,3,640,480))\\na = a.to(dev)\\nmask = torch.zeros((1,640,480), dtype=torch.bool, device=dev)\\nmask = mask.to(dev)\\nnt = NestedTensor(a, mask)\\nnt = nt.to(dev)\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "from HDDETR.util.misc import NestedTensor\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "dev = torch.device(\"cuda\")\n",
        "model.to(dev)\n",
        "to_t = transforms.ToTensor()\n",
        "img = Image.open(\"treno.jpg\")\n",
        "a = to_t(img).reshape((1,3,640,480))\n",
        "a = a.to(dev)\n",
        "mask = torch.zeros((1,640,480), dtype=torch.bool, device=dev)\n",
        "mask = mask.to(dev)\n",
        "nt = NestedTensor(a, mask)\n",
        "nt = nt.to(dev)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vohh7Js-FMQa"
      },
      "outputs": [],
      "source": [
        "class MaskFrozenDETR(nn.Module):\n",
        "  def __init__(self, detr, device, num_classes):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.detr = detr\n",
        "    self.detr.num_queries = self.detr.num_queries_one2one\n",
        "    self.detr.transformer.two_stage_num_proposals = self.detr.num_queries_one2one\n",
        "    self.num_classes = num_classes\n",
        "    for param in detr.parameters():\n",
        "      param.requires_grad = False\n",
        "    # vedi deformable encoder block\n",
        "    self.feature_enc_1 = DeformableTransformerEncoderLayer(d_model=256,dropout=0, activation='gelu')\n",
        "    self.feature_enc_2 = DeformableTransformerEncoderLayer(d_model=256,dropout=0, activation='gelu')\n",
        "    self.box_enc_1 = DeformableTransformerEncoderLayer(d_model=128,dropout=0, activation='gelu')\n",
        "    self.box_enc_2 = DeformableTransformerEncoderLayer(d_model=128,dropout=0, activation='gelu')\n",
        "    self.channel_mapper = nn.Linear(256, 128)\n",
        "    self.query_channel_mapper = nn.Linear(256, 128)\n",
        "    self.roialign = RoIAlign(output_size=(32,32),spatial_scale=0.25, sampling_ratio=-1)\n",
        "    self.class_adapter = nn.Linear(256, num_classes)\n",
        "    self.topk = 96\n",
        "\n",
        "\n",
        "  def forward(self, input, sizes):\n",
        "    bs, _, h, w = input.tensors.shape\n",
        "\n",
        "    # get output from the H-DETR\n",
        "    detr_out = self.detr(input)\n",
        "\n",
        "    # compute reference points for the following encoder layers\n",
        "    ref_points = self.detr.transformer.encoder.get_reference_points(detr_out[\"intermediate_enc_out\"][\"spatial_shapes\"],\n",
        "                                                                    detr_out[\"intermediate_enc_out\"][\"valid_ratios\"],\n",
        "                                                                    self.device)\n",
        "    # remove key not required by DeformableTransformerEncoderLayer.forward()\n",
        "    detr_out[\"intermediate_enc_out\"].pop(\"valid_ratios\")\n",
        "\n",
        "    # pass the multi-scale encoder maps to the two deformable layers\n",
        "    enc_maps = self.feature_enc_1(**detr_out[\"intermediate_enc_out\"], reference_points=ref_points)\n",
        "\n",
        "    #detr_out[\"intermediate_enc_out\"][\"src\"] = enc_maps\n",
        "    enc_maps = self.feature_enc_2(**detr_out[\"intermediate_enc_out\"], reference_points=ref_points).permute(0, 2, 1)\n",
        "\n",
        "    # interpolate he maps to the backbone dimension\n",
        "    # detr_out[\"backbone_out\"].shape\n",
        "    backbone_h, backbone_w = detr_out[\"backbone_out\"].tensors.shape[-1], detr_out[\"backbone_out\"].tensors.shape[-2]\n",
        "    fe = mutils.interpolate(enc_maps, backbone_h*backbone_w, mode=\"linear\")\n",
        "\n",
        "    # sum the maps and reduce dimensionality\n",
        "    f = detr_out[\"backbone_out\"].tensors.view((bs, 256, -1 )) + fe\n",
        "\n",
        "    # map channel reduction\n",
        "    mapped_f = self.channel_mapper(f.permute(0, 2, 1))\n",
        "\n",
        "    # computing class for each proposal\n",
        "    logits = self.class_adapter(detr_out['decoder_out'][-1])\n",
        "\n",
        "    # computing top 100 proposals, boxes and queries\n",
        "    ci = logits.sigmoid()\n",
        "    topk_values, topk_indexes = torch.topk(\n",
        "            ci.view(logits.shape[0], -1), self.topk, dim=1\n",
        "    ) # takes top logits. may take a box more than once\n",
        "    nboxes = detr_out[\"pred_boxes\"]\n",
        "    topk_boxes = topk_indexes // logits.shape[2]\n",
        "    nboxes = torch.gather(nboxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n",
        "    nboxes = nboxes[:,:self.topk, :]\n",
        "    boxes = box_ops.box_cxcywh_to_xyxy(nboxes)\n",
        "    img_h, img_w = sizes.unbind(1)\n",
        "    scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
        "    boxes = boxes * scale_fct[:, None, :]\n",
        "    boxes = torch.clamp(boxes, min=0)\n",
        "    object_queries = detr_out[\"decoder_out\"][-1, :, :self.detr.num_queries_one2one, :]\n",
        "    object_queries = torch.gather(object_queries, 1, topk_boxes.unsqueeze(-1).repeat(1,1,object_queries.shape[-1]))\n",
        "    logits = torch.gather(logits, 1, topk_boxes.unsqueeze(-1).repeat(1,1, num_classes))\n",
        "\n",
        "\n",
        "    batchindexes = torch.arange(bs).reshape(bs,1,1).repeat(1,self.topk,1).to(self.device)\n",
        "    roiboxes = torch.cat([batchindexes, boxes], -1).reshape(bs*self.topk, 5)\n",
        "\n",
        "    Ri = self.roialign(mapped_f.reshape((bs, 128, backbone_h, backbone_w )), roiboxes)\n",
        "    maskRi = self.roialign(detr_out[\"backbone_out\"].mask.to(torch.float32).unsqueeze(1), roiboxes)\\\n",
        "                 .to(torch.bool)\\\n",
        "                 .permute(0,2,3,1).squeeze()\n",
        "\n",
        "    Ri = Ri.permute(0, 2, 3, 1).reshape(bs*self.topk,32*32, 128)\n",
        "     # ?\n",
        "\n",
        "    valid_ratios_box = torch.stack([self.detr.transformer.get_valid_ratio(m.unsqueeze(0)) for m in maskRi], 0)\n",
        "\n",
        "    sp_shapes = torch.tensor([[32,32]]).to(self.device)\n",
        "    level_start_index = torch.tensor([0]).to(self.device)\n",
        "\n",
        "    ref_point_box = self.detr.transformer.encoder.get_reference_points(sp_shapes,\n",
        "                                                                    valid_ratios_box,\n",
        "                                                                    self.device)\n",
        "\n",
        "    maskRi = maskRi.reshape(bs*self.topk,32*32)\n",
        "    Ri = self.box_enc_1(Ri, padding_mask=maskRi,\n",
        "                        level_start_index=level_start_index,\n",
        "                        pos=None,\n",
        "                        reference_points=ref_point_box,\n",
        "                        spatial_shapes=sp_shapes)\n",
        "\n",
        "    Ri = self.box_enc_2(Ri, padding_mask=maskRi,\n",
        "                        level_start_index=level_start_index,\n",
        "                        pos=None,\n",
        "                        reference_points=ref_point_box,\n",
        "                        spatial_shapes=sp_shapes)\n",
        "\n",
        "\n",
        "    object_queries = self.query_channel_mapper(object_queries).reshape(bs*self.topk, 128, 1)\n",
        "\n",
        "\n",
        "    segmasks = torch.bmm(Ri, object_queries).sigmoid().reshape(bs, self.topk, 32, 32)\n",
        "\n",
        "\n",
        "    return {\"pred_masks\": segmasks,\n",
        "            \"pred_logits\": logits,\n",
        "            \"pred_boxes\": nboxes,\n",
        "            \"unnormal_boxes\": boxes}\n",
        "\n",
        "def _paste( roi, empty_mask, flatboxes, index):\n",
        "      ox = int(round(flatboxes[index][0].item()))\n",
        "      oy = int(round(flatboxes[index][1].item()))\n",
        "      x1 = min(roi.shape[1], empty_mask[index].shape[1]-ox)\n",
        "      y1 = min(roi.shape[0], empty_mask[index].shape[0]-oy)\n",
        "\n",
        "      empty_mask[index][oy:oy+roi.shape[0],\n",
        "                        ox:ox+roi.shape[1]] = roi[:y1,:x1]\n",
        "      return empty_mask\n",
        "\n",
        "\n",
        "def create_masks(input, boxes, segmasks):\n",
        "    bs, _, h, w = input.tensors.shape\n",
        "    emptym = torch.zeros(segmasks.shape[0], h, w)\n",
        "    flatboxes = boxes.view(segmasks.shape[0], 4)\n",
        "    for m in range(segmasks.shape[0]):\n",
        "      box_w = max(int(round((flatboxes[m][2] - flatboxes[m][0]).item())), 1)\n",
        "      box_h = max(int(round((flatboxes[m][3] - flatboxes[m][1]).item())), 1)\n",
        "      inter = mutils.interpolate(segmasks[m].expand(1,1,-1,-1),\n",
        "                                 (box_h, box_w),\n",
        "                                 mode=\"bilinear\")[0,0,:,:]\n",
        "      emptym = _paste(inter, emptym, flatboxes, m)\n",
        "\n",
        "    segmasks = emptym.reshape(boxes.shape[0], boxes.shape[1], h, w)\n",
        "    return segmasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w0JOZXORcoX",
        "outputId": "4c41b659-588a-42fc-8f18-03447174c1b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.05s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topk for eval: 100\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(42)\n",
        "batch_size = 2\n",
        "lr = 1.5e-4\n",
        "betas = (0.9, 0.999)\n",
        "weight_decay = 5e-5\n",
        "epochs = 6\n",
        "num_classes = 10\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "dataset_train = create_dataset(\"train\")\n",
        "dataset_val = create_dataset(\"val\")\n",
        "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
        "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
        "\n",
        "batch_sampler_train = torch.utils.data.BatchSampler(\n",
        "        sampler_train, batch_size, drop_last=True\n",
        ")\n",
        "\n",
        "data_loader_train = DataLoader(\n",
        "        dataset_train,\n",
        "        batch_sampler=batch_sampler_train,\n",
        "        collate_fn=mutils.collate_fn,\n",
        "        pin_memory=True,)\n",
        "\n",
        "data_loader_val = DataLoader(\n",
        "        dataset_val,\n",
        "        batch_size,\n",
        "        sampler=sampler_val,\n",
        "        drop_last=False,\n",
        "        collate_fn=mutils.collate_fn,\n",
        "        pin_memory=True,)\n",
        "\n",
        "detr, criterion, postprocessor = load_detr(args)\n",
        "\n",
        "mfdetr = MaskFrozenDETR(detr, device, num_classes)\n",
        "mfdetr.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW([p for p in mfdetr.parameters() if p.requires_grad], lr=lr,\n",
        "                              betas=betas,\n",
        "                              weight_decay=weight_decay, fused=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-R_MuOnmBxrP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def train_MFDETR(model, criterion ,postprocessors, dl_train, dl_val, optimizer, epochs, device, save_path):\n",
        "\n",
        "  for e in range(epochs):\n",
        "    model.train()\n",
        "    prefetcher = data_prefetcher(dl_train, device, prefetch=True)\n",
        "    samples, targets = prefetcher.next()\n",
        "    criterion.train()\n",
        "    metric_logger = mutils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter(\"lr\", mutils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
        "    metric_logger.add_meter(\n",
        "        \"class_error\", mutils.SmoothedValue(window_size=1, fmt=\"{value:.2f}\")\n",
        "    )\n",
        "    header = \"Epoch: [{}]\".format(e)\n",
        "    print_freq = 10\n",
        "    batch_counter = 0\n",
        "    for b in metric_logger.log_every(range(len(dl_train)), print_freq, header):\n",
        "\n",
        "      sizes = torch.stack([t[\"size\"] for t in targets])\n",
        "      t = time.time()\n",
        "      outputs = model(samples, sizes)\n",
        "      t = time.time()\n",
        "      loss_dict = criterion(outputs, targets)\n",
        "      t = time.time()\n",
        "      weight_dict = criterion.weight_dict\n",
        "      losses = sum(loss_dict[k]*weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "      loss_dict_reduced = mutils.reduce_dict(loss_dict)\n",
        "      loss_dict_reduced_unscaled = {\n",
        "            f\"{k}_unscaled\": v for k, v in loss_dict_reduced.items()}\n",
        "      loss_dict_reduced_scaled = {\n",
        "            k: v * weight_dict[k]\n",
        "            for k, v in loss_dict_reduced.items()\n",
        "            if k in weight_dict}\n",
        "      losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
        "\n",
        "      loss_value = losses_reduced_scaled.item()\n",
        "      if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            return model\n",
        "      #print(f\"loss reduction: {losses}\")\n",
        "      #print(f\"loss: {loss_dict_reduced_scaled}\")\n",
        "      t = time.time()\n",
        "      optimizer.zero_grad()\n",
        "      losses.backward()\n",
        "      optimizer.step()\n",
        "      print(f\"optimization: {time.time() - t}\")\n",
        "      metric_logger.update(\n",
        "            loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled\n",
        "      )\n",
        "      metric_logger.update(class_error=loss_dict_reduced[\"class_error\"])\n",
        "      metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "      samples, targets = prefetcher.next()\n",
        "      batch_counter += 1\n",
        "      if batch_counter % 10:\n",
        "        torch.save({\n",
        "            \"epochs\":e,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': losses,\n",
        "            'batch':batch_counter\n",
        "        }, save_path + \"checkpoint.pth\")\n",
        "\n",
        "    val_evaluation(model, criterion, postprocessors, dl_val)\n",
        "    torch.save({\n",
        "            \"epochs\":e,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': losses,\n",
        "            'batch':batch_counter\n",
        "        }, save_path + \"finalcheckpoint.pth\")\n",
        "  return model\n",
        "\n",
        "def val_evaluation(model, crterion, postprocessors, dl_val):\n",
        "  model.eval()\n",
        "  criterion.eval()\n",
        "  iou_types = tuple(k for k in (\"segm\", \"bbox\") if k in postprocessors.keys())\n",
        "  coco_evaluator = CocoEvaluator(dl_val.dataset.coco, iou_types)\n",
        "  metric_logger = mutils.MetricLogger(delimiter=\"  \")\n",
        "  metric_logger.add_meter(\n",
        "        \"class_error\", mutils.SmoothedValue(window_size=1, fmt=\"{value:.2f}\"))\n",
        "  header = \"Validation:\"\n",
        "  for samples, targets in metric_logger.log_every(dl_val, 10, header):\n",
        "      samples = samples.to(device)\n",
        "      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "      sizes = torch.stack([t[\"size\"] for t in targets])\n",
        "      outputs = model(samples, sizes)\n",
        "      loss_dict = criterion(outputs, targets)\n",
        "      weight_dict = criterion.weight_dict\n",
        "\n",
        "      # reduce losses over all GPUs for logging purposes\n",
        "      loss_dict_reduced = mutils.reduce_dict(loss_dict)\n",
        "      loss_dict_reduced_scaled = {\n",
        "          k: v * weight_dict[k]\n",
        "          for k, v in loss_dict_reduced.items()\n",
        "          if k in weight_dict}\n",
        "      loss_dict_reduced_unscaled = {\n",
        "          f\"{k}_unscaled\": v for k, v in loss_dict_reduced.items()}\n",
        "      metric_logger.update(\n",
        "            loss=sum(loss_dict_reduced_scaled.values()),\n",
        "            **loss_dict_reduced_scaled,\n",
        "            **loss_dict_reduced_unscaled,)\n",
        "      metric_logger.update(class_error=loss_dict_reduced[\"class_error\"])\n",
        "\n",
        "      orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
        "      results = postprocessors[\"bbox\"](outputs, orig_target_sizes)\n",
        "      target_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n",
        "      print(orig_target_sizes)\n",
        "      results = postprocessors[\"segm\"](\n",
        "                results, outputs, orig_target_sizes, target_sizes)\n",
        "      print(\"A\")\n",
        "      res = {target[\"image_id\"].item(): output\n",
        "            for target, output in zip(targets, results)}\n",
        "      if coco_evaluator is not None:\n",
        "          coco_evaluator.update(res)\n",
        "\n",
        "  # gather the stats from all processes\n",
        "  metric_logger.synchronize_between_processes()\n",
        "  print(\"Averaged stats:\", metric_logger)\n",
        "  if coco_evaluator is not None:\n",
        "      coco_evaluator.synchronize_between_processes()\n",
        "      # accumulate predictions from all images\n",
        "  if coco_evaluator is not None:\n",
        "      coco_evaluator.accumulate()\n",
        "      coco_evaluator.summarize()\n",
        "  stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "  if coco_evaluator is not None:\n",
        "      if \"bbox\" in postprocessors.keys():\n",
        "          stats[\"coco_eval_bbox\"] = coco_evaluator.coco_eval[\"bbox\"].stats.tolist()\n",
        "      if \"segm\" in postprocessors.keys():\n",
        "          stats[\"coco_eval_masks\"] = coco_evaluator.coco_eval[\"segm\"].stats.tolist()\n",
        "\n",
        "  return stats, coco_evaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRkLkswfMkYT"
      },
      "outputs": [],
      "source": [
        "path = '/content/MyDrive/MyDrive/ML4CV'\n",
        "train_MFDETR(mfdetr, criterion, postprocessor, data_loader_train, data_loader_val, optimizer, 1, device, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9NEtvq-e0E4",
        "outputId": "0e165a12-3b36-437d-9b2f-5faf79f4427e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topk for eval: 100\n",
            "[{'boxes': tensor([[0.6126, 0.2461, 0.3870, 0.1128],\n",
            "        [0.3392, 0.1906, 0.1598, 0.2236]], device='cuda:0'), 'labels': tensor([7, 6], device='cuda:0'), 'masks': tensor([[[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]],\n",
            "\n",
            "        [[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]]], device='cuda:0'), 'image_id': tensor([2030], device='cuda:0'), 'area': tensor([22935.6992, 18777.1680], device='cuda:0'), 'iscrowd': tensor([0, 0], device='cuda:0'), 'orig_size': tensor([4000, 3000], device='cuda:0'), 'size': tensor([821, 640], device='cuda:0')}, {'boxes': tensor([[0.9139, 0.1033, 0.0940, 0.0457],\n",
            "        [0.5526, 0.5670, 0.1320, 0.1162]], device='cuda:0'), 'labels': tensor([7, 7], device='cuda:0'), 'masks': tensor([[[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]],\n",
            "\n",
            "        [[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]]], device='cuda:0'), 'image_id': tensor([253], device='cuda:0'), 'area': tensor([2589.0325, 7790.9854], device='cuda:0'), 'iscrowd': tensor([0, 0], device='cuda:0'), 'orig_size': tensor([4000, 3000], device='cuda:0'), 'size': tensor([1024,  768], device='cuda:0')}]\n",
            "torch.Size([2, 3, 1024, 768])\n"
          ]
        }
      ],
      "source": [
        "detr, criterion, postprocessor = load_detr(args)\n",
        "prefetcher = data_prefetcher(data_loader_train, device, prefetch=True)\n",
        "samples, targets = prefetcher.next()\n",
        "print(targets)\n",
        "print(samples.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwUPtYgZe-Gi",
        "outputId": "c43e688f-79e4-4297-a32f-16a24d9252dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'boxes': tensor([[0.4073, 0.5974, 0.1229, 0.0812]], device='cuda:0'),\n",
              " 'labels': tensor([7], device='cuda:0'),\n",
              " 'masks': tensor([[[False, False, False,  ..., False, False, False],\n",
              "          [False, False, False,  ..., False, False, False],\n",
              "          [False, False, False,  ..., False, False, False],\n",
              "          ...,\n",
              "          [False, False, False,  ..., False, False, False],\n",
              "          [False, False, False,  ..., False, False, False],\n",
              "          [False, False, False,  ..., False, False, False]]], device='cuda:0'),\n",
              " 'image_id': tensor([2597], device='cuda:0'),\n",
              " 'area': tensor([2727.7170], device='cuda:0'),\n",
              " 'iscrowd': tensor([0], device='cuda:0'),\n",
              " 'orig_size': tensor([4000, 3000], device='cuda:0'),\n",
              " 'size': tensor([810, 608], device='cuda:0')}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "targets[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmdiV03DXBER",
        "outputId": "aaf88f05-b037-46cc-b9c0-83b53047f789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "topk for eval: 100\n",
            "torch.Size([3, 3, 810, 682])\n",
            "4\n",
            "torch.Size([3, 102, 86])\n",
            "vrr\n",
            "torch.Size([3, 4, 2])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoder_out\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([6, 3, 300, 256])\n",
            "pred_logits\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 300, 91])\n",
            "pred_boxes\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 300, 4])\n",
            "pred_logits_one2many\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 0, 91])\n",
            "pred_boxes_one2many\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 0, 4])\n",
            "backbone_out\n",
            "<class 'HDDETR.util.misc.NestedTensor'>\n",
            "tensor([[[[0.0049, 0.0369, 0.0051,  ..., 0.0220, 0.0219, 0.0206],\n",
            "          [0.0048, 0.0052, 0.0342,  ..., 0.0053, 0.0047, 0.0188],\n",
            "          [0.0046, 0.0048, 0.0791,  ..., 0.0056, 0.0050, 0.0191],\n",
            "          ...,\n",
            "          [0.1141, 0.0891, 0.1674,  ..., 0.0054, 0.0049, 0.0188],\n",
            "          [0.1164, 0.1170, 0.1109,  ..., 0.0048, 0.0044, 0.0193],\n",
            "          [0.1293, 0.0303, 0.0901,  ..., 0.0183, 0.0197, 0.0287]],\n",
            "\n",
            "         [[0.0417, 0.1120, 0.1045,  ..., 0.1492, 0.1489, 0.1255],\n",
            "          [0.0504, 0.0879, 0.1362,  ..., 0.1457, 0.1457, 0.1119],\n",
            "          [0.1288, 0.1151, 0.1309,  ..., 0.1459, 0.1459, 0.1122],\n",
            "          ...,\n",
            "          [0.2525, 0.2837, 0.3458,  ..., 0.1457, 0.1457, 0.1127],\n",
            "          [0.2883, 0.2623, 0.2998,  ..., 0.1453, 0.1454, 0.1105],\n",
            "          [0.2336, 0.2009, 0.1915,  ..., 0.1028, 0.0985, 0.0777]],\n",
            "\n",
            "         [[0.1614, 0.0082, 0.1106,  ..., 0.0565, 0.1869, 0.2387],\n",
            "          [0.3053, 0.1502, 0.2526,  ..., 0.2896, 0.3948, 0.4863],\n",
            "          [0.2933, 0.0000, 0.1384,  ..., 0.2823, 0.2999, 0.3774],\n",
            "          ...,\n",
            "          [0.2061, 0.1298, 0.2783,  ..., 0.2858, 0.2755, 0.3995],\n",
            "          [0.2749, 0.0000, 0.7251,  ..., 0.3103, 0.4026, 0.3598],\n",
            "          [0.5070, 0.5971, 0.6663,  ..., 0.4549, 0.3153, 0.2133]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0351, 0.1113, 0.1135,  ..., 0.0257, 0.0258, 0.0216],\n",
            "          [0.0187, 0.1049, 0.1545,  ..., 0.0257, 0.0257, 0.0204],\n",
            "          [0.0131, 0.0916, 0.1382,  ..., 0.0257, 0.0257, 0.0204],\n",
            "          ...,\n",
            "          [0.1251, 0.1132, 0.1475,  ..., 0.0257, 0.0257, 0.0203],\n",
            "          [0.1251, 0.1245, 0.1556,  ..., 0.0258, 0.0258, 0.0205],\n",
            "          [0.1050, 0.0739, 0.1345,  ..., 0.0262, 0.0266, 0.0222]],\n",
            "\n",
            "         [[0.5574, 0.7346, 0.5055,  ..., 0.4857, 0.5100, 0.5326],\n",
            "          [0.1273, 0.5021, 0.2049,  ..., 0.5225, 0.5491, 0.5957],\n",
            "          [0.0143, 0.3889, 0.2171,  ..., 0.5163, 0.5447, 0.5970],\n",
            "          ...,\n",
            "          [0.6167, 0.5973, 0.0448,  ..., 0.5157, 0.5451, 0.5962],\n",
            "          [0.6289, 0.8185, 0.0180,  ..., 0.5702, 0.5922, 0.6257],\n",
            "          [0.4864, 0.2445, 0.0576,  ..., 0.5180, 0.5259, 0.4898]],\n",
            "\n",
            "         [[1.0912, 0.0000, 0.0000,  ..., 0.1567, 0.0000, 0.0000],\n",
            "          [0.5752, 0.1070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.7092, 0.5110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0804,  ..., 0.3671, 0.2651, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0068, 0.0041, 0.1117,  ..., 0.0042, 0.0052, 0.0052],\n",
            "          [0.0069, 0.0056, 0.0835,  ..., 0.0048, 0.0049, 0.0076],\n",
            "          [0.0056, 0.0056, 0.0626,  ..., 0.0050, 0.0058, 0.0091],\n",
            "          ...,\n",
            "          [0.0273, 0.0049, 0.0054,  ..., 0.0054, 0.0049, 0.0188],\n",
            "          [0.0277, 0.0046, 0.0048,  ..., 0.0048, 0.0044, 0.0193],\n",
            "          [0.0361, 0.0182, 0.0183,  ..., 0.0183, 0.0197, 0.0287]],\n",
            "\n",
            "         [[0.0808, 0.0439, 0.1327,  ..., 0.0122, 0.0232, 0.0722],\n",
            "          [0.0815, 0.0508, 0.0867,  ..., 0.0097, 0.0111, 0.0050],\n",
            "          [0.0710, 0.0881, 0.1040,  ..., 0.0104, 0.0095, 0.0319],\n",
            "          ...,\n",
            "          [0.1456, 0.1458, 0.1461,  ..., 0.1457, 0.1457, 0.1127],\n",
            "          [0.1432, 0.1455, 0.1459,  ..., 0.1453, 0.1454, 0.1105],\n",
            "          [0.1051, 0.1031, 0.1030,  ..., 0.1028, 0.0985, 0.0777]],\n",
            "\n",
            "         [[0.2107, 0.4565, 0.0110,  ..., 0.2444, 0.3677, 0.7809],\n",
            "          [0.8181, 0.5103, 0.3370,  ..., 0.0000, 0.4614, 0.7798],\n",
            "          [1.0747, 1.0217, 0.2350,  ..., 0.1447, 0.0000, 0.5733],\n",
            "          ...,\n",
            "          [0.1296, 0.3725, 0.2979,  ..., 0.2858, 0.2755, 0.3995],\n",
            "          [0.2718, 0.3149, 0.2991,  ..., 0.3103, 0.4026, 0.3598],\n",
            "          [0.0985, 0.4384, 0.3479,  ..., 0.4549, 0.3153, 0.2133]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.1472, 0.1851, 0.2606,  ..., 0.1519, 0.2352, 0.1774],\n",
            "          [0.1903, 0.2344, 0.2848,  ..., 0.2030, 0.2347, 0.1772],\n",
            "          [0.2471, 0.2309, 0.3005,  ..., 0.1811, 0.2067, 0.1314],\n",
            "          ...,\n",
            "          [0.0256, 0.0257, 0.0257,  ..., 0.0257, 0.0257, 0.0203],\n",
            "          [0.0258, 0.0258, 0.0258,  ..., 0.0258, 0.0258, 0.0205],\n",
            "          [0.0281, 0.0261, 0.0261,  ..., 0.0262, 0.0266, 0.0222]],\n",
            "\n",
            "         [[0.7375, 0.4006, 0.6861,  ..., 0.3637, 0.8192, 0.2677],\n",
            "          [0.5336, 0.4179, 1.0099,  ..., 0.6509, 0.6345, 0.5287],\n",
            "          [0.7894, 0.5741, 0.9564,  ..., 0.3602, 0.4665, 0.1855],\n",
            "          ...,\n",
            "          [0.5323, 0.5267, 0.5168,  ..., 0.5157, 0.5451, 0.5962],\n",
            "          [0.5656, 0.5707, 0.5698,  ..., 0.5702, 0.5922, 0.6257],\n",
            "          [0.4855, 0.5143, 0.5180,  ..., 0.5180, 0.5259, 0.4898]],\n",
            "\n",
            "         [[0.2127, 0.1598, 0.0000,  ..., 0.2160, 0.0458, 0.0000],\n",
            "          [0.0327, 0.0850, 0.0000,  ..., 0.3735, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0342, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.1890, 0.2809, 0.3667,  ..., 0.3671, 0.2651, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.1244, 0.1024, 0.1056,  ..., 0.0220, 0.0219, 0.0206],\n",
            "          [0.1001, 0.0816, 0.0766,  ..., 0.0053, 0.0047, 0.0188],\n",
            "          [0.1052, 0.0835, 0.0851,  ..., 0.0056, 0.0050, 0.0191],\n",
            "          ...,\n",
            "          [0.0273, 0.0049, 0.0054,  ..., 0.0054, 0.0049, 0.0188],\n",
            "          [0.0277, 0.0046, 0.0048,  ..., 0.0048, 0.0044, 0.0193],\n",
            "          [0.0361, 0.0182, 0.0183,  ..., 0.0183, 0.0197, 0.0287]],\n",
            "\n",
            "         [[0.2689, 0.2739, 0.2802,  ..., 0.1492, 0.1489, 0.1255],\n",
            "          [0.2584, 0.2601, 0.2606,  ..., 0.1457, 0.1457, 0.1119],\n",
            "          [0.2583, 0.2594, 0.2585,  ..., 0.1459, 0.1459, 0.1122],\n",
            "          ...,\n",
            "          [0.1456, 0.1458, 0.1461,  ..., 0.1457, 0.1457, 0.1127],\n",
            "          [0.1432, 0.1455, 0.1459,  ..., 0.1453, 0.1454, 0.1105],\n",
            "          [0.1051, 0.1031, 0.1030,  ..., 0.1028, 0.0985, 0.0777]],\n",
            "\n",
            "         [[0.1591, 0.1979, 0.2199,  ..., 0.0565, 0.1869, 0.2387],\n",
            "          [0.0326, 0.0890, 0.0485,  ..., 0.2896, 0.3948, 0.4863],\n",
            "          [0.0363, 0.1816, 0.0774,  ..., 0.2823, 0.2999, 0.3774],\n",
            "          ...,\n",
            "          [0.1296, 0.3725, 0.2979,  ..., 0.2858, 0.2755, 0.3995],\n",
            "          [0.2718, 0.3149, 0.2991,  ..., 0.3103, 0.4026, 0.3598],\n",
            "          [0.0985, 0.4384, 0.3479,  ..., 0.4549, 0.3153, 0.2133]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0409, 0.0178, 0.0142,  ..., 0.0257, 0.0258, 0.0216],\n",
            "          [0.0117, 0.0117, 0.0117,  ..., 0.0257, 0.0257, 0.0204],\n",
            "          [0.0116, 0.0118, 0.0118,  ..., 0.0257, 0.0257, 0.0204],\n",
            "          ...,\n",
            "          [0.0256, 0.0257, 0.0257,  ..., 0.0257, 0.0257, 0.0203],\n",
            "          [0.0258, 0.0258, 0.0258,  ..., 0.0258, 0.0258, 0.0205],\n",
            "          [0.0281, 0.0261, 0.0261,  ..., 0.0262, 0.0266, 0.0222]],\n",
            "\n",
            "         [[0.3213, 0.3684, 0.4147,  ..., 0.4857, 0.5100, 0.5326],\n",
            "          [0.2976, 0.3877, 0.4208,  ..., 0.5225, 0.5491, 0.5957],\n",
            "          [0.2984, 0.4409, 0.4665,  ..., 0.5163, 0.5447, 0.5970],\n",
            "          ...,\n",
            "          [0.5323, 0.5267, 0.5168,  ..., 0.5157, 0.5451, 0.5962],\n",
            "          [0.5656, 0.5707, 0.5698,  ..., 0.5702, 0.5922, 0.6257],\n",
            "          [0.4855, 0.5143, 0.5180,  ..., 0.5180, 0.5259, 0.4898]],\n",
            "\n",
            "         [[0.1492, 0.0000, 0.0000,  ..., 0.1567, 0.0000, 0.0000],\n",
            "          [0.0080, 0.0000, 0.0180,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.2595, 0.0579, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0342, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.1890, 0.2809, 0.3667,  ..., 0.3671, 0.2651, 0.0000]]]],\n",
            "       device='cuda:0')\n",
            "intermediate_enc_out\n",
            "<class 'dict'>\n",
            "src\n",
            "torch.Size([3, 11680, 256])\n",
            "valid_ratios\n",
            "torch.Size([3, 4, 2])\n",
            "spatial_shapes\n",
            "torch.Size([4, 2])\n",
            "level_start_index\n",
            "torch.Size([4])\n",
            "pos\n",
            "torch.Size([3, 11680, 256])\n",
            "padding_mask\n",
            "torch.Size([3, 11680])\n",
            "enc_outputs\n",
            "<class 'dict'>\n",
            "pred_logits\n",
            "torch.Size([3, 11680, 91])\n",
            "pred_boxes\n",
            "torch.Size([3, 11680, 4])\n"
          ]
        }
      ],
      "source": [
        "detr, criterion, postprocessor = load_detr(args)\n",
        "prefetcher = data_prefetcher(data_loader_train, device, prefetch=True)\n",
        "samples, targets = prefetcher.next()\n",
        "print(samples.tensors.shape)\n",
        "detr = detr.to(torch.device(\"cuda\"))\n",
        "out = detr(samples)\n",
        "for k in out.keys():\n",
        "  print(k)\n",
        "  print(type(out[k]))\n",
        "\n",
        "  if type(out[k]) == torch.Tensor:\n",
        "    print(out[k].shape)\n",
        "  elif isinstance(out[k], dict):\n",
        "    for kk in out[k].keys():\n",
        "      print(kk)\n",
        "      if type(out[k][kk]) == torch.Tensor:\n",
        "        print(out[k][kk].shape)\n",
        "      else:\n",
        "        print(out[k][kk])\n",
        "  else:\n",
        "    print(out[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4kYxKUVsTQq",
        "outputId": "6d87765a-b723-4eb8-91fc-8398544a8291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 0.])\n",
            "tensor([1., 0.])\n"
          ]
        }
      ],
      "source": [
        "a = torch.Tensor([True, False])\n",
        "print(a)\n",
        "print(a.to(torch.float32))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.ids[39]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwbFsFbOohto",
        "outputId": "61d25068-8e7b-41ef-80aa-033dd16584e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "-2dI-mnxFBNC"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
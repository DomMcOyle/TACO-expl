{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-2dI-mnxFBNC"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomMcOyle/TACO-expl/blob/add_detr/Training%20notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uW0_gODeKIeQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2344424-02f5-482e-e9e7-758e809096f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TACO-expl'...\n",
            "remote: Enumerating objects: 983, done.\u001b[K\n",
            "remote: Counting objects: 100% (409/409), done.\u001b[K\n",
            "remote: Compressing objects: 100% (210/210), done.\u001b[K\n",
            "remote: Total 983 (delta 266), reused 332 (delta 197), pack-reused 574\u001b[K\n",
            "Receiving objects: 100% (983/983), 99.85 MiB | 31.88 MiB/s, done.\n",
            "Resolving deltas: 100% (639/639), done.\n",
            "/content/TACO-expl\n",
            "Branch 'add_detr' set up to track remote branch 'add_detr' from 'origin'.\n",
            "Switched to a new branch 'add_detr'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/DomMcOyle/TACO-expl\n",
        "%cd /content/TACO-expl/\n",
        "!git checkout add_detr\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the following cell, reset the environment"
      ],
      "metadata": {
        "id": "pUv0ZCdoQwPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/TACO-expl/HDDETR\n",
        "!pip install -r requirements.txt\n",
        "!pip install mmcv==2.1.0 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.1/index.html\n",
        "!pip install mmdet\n",
        "%cd /content/TACO-expl/HDDETR/models/ops\n",
        "!python setup.py build install\n",
        "# unit test (should see all checking is True)\n",
        "#!python test.py\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "UwcWEhm-wBCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin add_detr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODG94BFmYgl_",
        "outputId": "efcc6f9b-55f6-438d-8fd4-a1fbf87dbb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects:  20% (1/5)\rUnpacking objects:  40% (2/5)\rUnpacking objects:  60% (3/5)\rUnpacking objects:  80% (4/5)\rUnpacking objects: 100% (5/5)\rUnpacking objects: 100% (5/5), 444 bytes | 222.00 KiB/s, done.\n",
            "From https://github.com/DomMcOyle/TACO-expl\n",
            " * branch            add_detr   -> FETCH_HEAD\n",
            "   db83cde..9307f53  add_detr   -> origin/add_detr\n",
            "Updating db83cde..9307f53\n",
            "Fast-forward\n",
            " HDDETR/models/backbone.py | 6 \u001b[32m+++++\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 5 insertions(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd TACO-expl\n",
        "import os.path\n",
        "import json\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import datetime as dt\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import torch.utils.data\n",
        "from torch.utils.data import DataLoader\n",
        "from pycocotools import mask as coco_mask\n",
        "\n",
        "from HDDETR.datasets.torchvision_datasets.coco import CocoDetection as TvCocoDetection\n",
        "import HDDETR.datasets.transforms as T\n",
        "from HDDETR.datasets.data_prefetcher import data_prefetcher\n",
        "from HDDETR.datasets.coco_eval import CocoEvaluator\n",
        "from HDDETR.models.deformable_transformer import DeformableTransformerEncoderLayer\n",
        "from torchvision.ops import RoIAlign\n",
        "import HDDETR.util.misc as mutils\n",
        "from HDDETR.util import box_ops\n",
        "\n",
        "import HDDETR.models\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/MyDrive/\")"
      ],
      "metadata": {
        "id": "XATP-Y3ZglE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535bc855-189a-451a-e41b-d0a0b8d6908b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TACO-expl\n",
            "Drive already mounted at /content/MyDrive/; to attempt to forcibly remount, call drive.mount(\"/content/MyDrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#split"
      ],
      "metadata": {
        "id": "-2dI-mnxFBNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keep_categories = [\"Bottle\", \"Bottle cap\", \"Can\", \"Cigarette\", \"Cup\",\n",
        "                   \"Lid\", \"Plastic bag & wrapper\", \"Pop tab\", \"Straw\"]\n",
        "\n",
        "def create_map(original, keep_supercategories):\n",
        "  class_map = {}\n",
        "  for cat in original:\n",
        "    if cat[\"supercategory\"] in keep_supercategories:\n",
        "      class_map[cat[\"name\"]] = cat[\"supercategory\"]\n",
        "    else:\n",
        "      class_map[cat[\"name\"]] = \"Other\"\n",
        "  return class_map\n",
        "\n",
        "def replace_dataset_classes(dataset, class_map):\n",
        "      \"\"\" Replaces classes of dataset based on a dictionary\"\"\"\n",
        "      class_new_names = list(set(class_map.values()))\n",
        "      class_new_names.sort()\n",
        "      class_originals = copy.deepcopy(dataset['categories'])\n",
        "      dataset['categories'] = []\n",
        "      class_ids_map = {}  # map from old id to new id\n",
        "\n",
        "      # Assign background id 0\n",
        "      has_background = False\n",
        "      if 'Background' in class_new_names:\n",
        "          if class_new_names.index('Background') != 0:\n",
        "              class_new_names.remove('Background')\n",
        "              class_new_names.insert(0, 'Background')\n",
        "          has_background = True\n",
        "\n",
        "      # Replace categories\n",
        "      for id_new, class_new_name in enumerate(class_new_names):\n",
        "          # Make sure id:0 is reserved for background\n",
        "          id_rectified = id_new\n",
        "          if not has_background:\n",
        "              id_rectified += 1\n",
        "\n",
        "          category = {\n",
        "              'supercategory': '',\n",
        "              'id': id_rectified,  # Background has id=0\n",
        "              'name': class_new_name,\n",
        "          }\n",
        "          dataset['categories'].append(category)\n",
        "          # Map class names\n",
        "          for class_original in class_originals:\n",
        "              if class_map[class_original['name']] == class_new_name:\n",
        "                  class_ids_map[class_original['id']] = id_rectified\n",
        "\n",
        "      # Update annotations category id tag\n",
        "      for ann in dataset['annotations']:\n",
        "          ann['category_id'] = class_ids_map[ann['category_id']]\n"
      ],
      "metadata": {
        "id": "KutH3M6PmlFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/TACO/data/annotations.json\", \"r\") as f:\n",
        "    dataset = json.loads(f.read())\n",
        "\n",
        "class_map = create_map(dataset[\"categories\"], keep_categories)\n",
        "replace_dataset_classes(dataset, class_map)\n",
        "dataset[\"categories\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RclP141dy5RU",
        "outputId": "66022aca-d855-436d-c343-77c8fa5b3e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Aluminium foil': 'Other', 'Battery': 'Other', 'Aluminium blister pack': 'Other', 'Carded blister pack': 'Other', 'Other plastic bottle': 'Bottle', 'Clear plastic bottle': 'Bottle', 'Glass bottle': 'Bottle', 'Plastic bottle cap': 'Bottle cap', 'Metal bottle cap': 'Bottle cap', 'Broken glass': 'Other', 'Food Can': 'Can', 'Aerosol': 'Can', 'Drink can': 'Can', 'Toilet tube': 'Other', 'Other carton': 'Other', 'Egg carton': 'Other', 'Drink carton': 'Other', 'Corrugated carton': 'Other', 'Meal carton': 'Other', 'Pizza box': 'Other', 'Paper cup': 'Cup', 'Disposable plastic cup': 'Cup', 'Foam cup': 'Cup', 'Glass cup': 'Cup', 'Other plastic cup': 'Cup', 'Food waste': 'Other', 'Glass jar': 'Other', 'Plastic lid': 'Lid', 'Metal lid': 'Lid', 'Other plastic': 'Other', 'Magazine paper': 'Other', 'Tissues': 'Other', 'Wrapping paper': 'Other', 'Normal paper': 'Other', 'Paper bag': 'Other', 'Plastified paper bag': 'Other', 'Plastic film': 'Plastic bag & wrapper', 'Six pack rings': 'Plastic bag & wrapper', 'Garbage bag': 'Plastic bag & wrapper', 'Other plastic wrapper': 'Plastic bag & wrapper', 'Single-use carrier bag': 'Plastic bag & wrapper', 'Polypropylene bag': 'Plastic bag & wrapper', 'Crisp packet': 'Plastic bag & wrapper', 'Spread tub': 'Other', 'Tupperware': 'Other', 'Disposable food container': 'Other', 'Foam food container': 'Other', 'Other plastic container': 'Other', 'Plastic glooves': 'Other', 'Plastic utensils': 'Other', 'Pop tab': 'Pop tab', 'Rope & strings': 'Other', 'Scrap metal': 'Other', 'Shoe': 'Other', 'Squeezable tube': 'Other', 'Plastic straw': 'Straw', 'Paper straw': 'Straw', 'Styrofoam piece': 'Other', 'Unlabeled litter': 'Other', 'Cigarette': 'Cigarette'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'supercategory': '', 'id': 1, 'name': 'Bottle'},\n",
              " {'supercategory': '', 'id': 2, 'name': 'Bottle cap'},\n",
              " {'supercategory': '', 'id': 3, 'name': 'Can'},\n",
              " {'supercategory': '', 'id': 4, 'name': 'Cigarette'},\n",
              " {'supercategory': '', 'id': 5, 'name': 'Cup'},\n",
              " {'supercategory': '', 'id': 6, 'name': 'Lid'},\n",
              " {'supercategory': '', 'id': 7, 'name': 'Other'},\n",
              " {'supercategory': '', 'id': 8, 'name': 'Plastic bag & wrapper'},\n",
              " {'supercategory': '', 'id': 9, 'name': 'Pop tab'},\n",
              " {'supercategory': '', 'id': 10, 'name': 'Straw'}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "parser = argparse.ArgumentParser(description='User args')\n",
        "parser.add_argument('--dataset_dir', required=True, help='Path to dataset annotations')\n",
        "parser.add_argument('--test_percentage', type=int, default=10, required=False, help='Percentage of images used for the testing set')\n",
        "parser.add_argument('--val_percentage', type=int, default=10, required=False, help='Percentage of images used for the validation set')\n",
        "parser.add_argument('--nr_trials', type=int, default=10, required=False, help='Number of splits')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\"\"\"\n",
        "args = {\n",
        "    \"nr_trials\":1,\n",
        "    \"test_percentage\":0.1,\n",
        "    \"val_percentage\":0.1,\n",
        "    \"dataset_dir\":'/content/TACO/data'\n",
        "}\n",
        "\n",
        "ann_input_path = args[\"dataset_dir\"] + '/annotations_unofficial.json'\n",
        "\n",
        "# Load annotations\n",
        "with open(ann_input_path, 'r') as f:\n",
        "    dataset = json.loads(f.read())\n",
        "\n",
        "keep_categories = [\"Bottle\", \"Bottle cap\", \"Can\", \"Cigarette\", \"Cup\",\n",
        "                   \"Lid\", \"Plastic bag & wrapper\", \"Pop tab\", \"Straw\"]\n",
        "if keep_categories is not None:\n",
        "  class_map = create_map(dataset[\"categories\"], keep_categories)\n",
        "  replace_dataset_classes(dataset, class_map)\n",
        "\n",
        "anns = dataset['annotations']\n",
        "scene_anns = dataset['scene_annotations']\n",
        "imgs = dataset['images']\n",
        "nr_images = len(imgs)\n",
        "\n",
        "for i in range(args[\"nr_trials\"]):\n",
        "    random.shuffle(imgs)\n",
        "\n",
        "    # Add new datasets\n",
        "    train_set = {\n",
        "        'info': None,\n",
        "        'images': [],\n",
        "        'annotations': [],\n",
        "        'scene_annotations': [],\n",
        "        'licenses': [],\n",
        "        'categories': [],\n",
        "        'scene_categories': [],\n",
        "    }\n",
        "    train_set['info'] =  dataset['info']\n",
        "    train_set['categories'] = dataset['categories']\n",
        "    train_set['scene_categories'] = dataset['scene_categories']\n",
        "\n",
        "    val_set = copy.deepcopy(train_set)\n",
        "    test_set = copy.deepcopy(train_set)\n",
        "\n",
        "    train_set['images'], partial = train_test_split(dataset['images'],\n",
        "                                                    random_state=42,\n",
        "                                                               test_size=args[\"test_percentage\"]+args[\"val_percentage\"])\n",
        "    val_set['images'], test_set[\"images\"] = train_test_split(partial,\n",
        "                                                             random_state=42,\n",
        "                                                             test_size=args[\"test_percentage\"]/(args[\"test_percentage\"]+args[\"val_percentage\"]))\n",
        "\n",
        "    # Aux Image Ids to split annotations\n",
        "    test_img_ids, val_img_ids, train_img_ids = [],[],[]\n",
        "    for img in test_set['images']:\n",
        "        test_img_ids.append(img['id'])\n",
        "\n",
        "    for img in val_set['images']:\n",
        "        val_img_ids.append(img['id'])\n",
        "\n",
        "    for img in train_set['images']:\n",
        "        train_img_ids.append(img['id'])\n",
        "\n",
        "    # Split instance annotations\n",
        "    for ann in anns:\n",
        "        if ann['image_id'] in test_img_ids:\n",
        "            test_set['annotations'].append(ann)\n",
        "        elif ann['image_id'] in val_img_ids:\n",
        "            val_set['annotations'].append(ann)\n",
        "        elif ann['image_id'] in train_img_ids:\n",
        "            train_set['annotations'].append(ann)\n",
        "\n",
        "    # Split scene tags\n",
        "    for ann in scene_anns:\n",
        "        if ann['image_id'] in test_img_ids:\n",
        "            test_set['scene_annotations'].append(ann)\n",
        "        elif ann['image_id'] in val_img_ids:\n",
        "            val_set['scene_annotations'].append(ann)\n",
        "        elif ann['image_id'] in train_img_ids:\n",
        "            train_set['scene_annotations'].append(ann)\n",
        "\n",
        "    # Write dataset splits\n",
        "    ann_train_out_path = args[\"dataset_dir\"] + '/' + 'annotations_' + str(i) +'_train.json'\n",
        "    ann_val_out_path   = args[\"dataset_dir\"] + '/' + 'annotations_' + str(i) + '_val.json'\n",
        "    ann_test_out_path  = args[\"dataset_dir\"] + '/' + 'annotations_' + str(i) + '_test.json'\n",
        "\n",
        "    with open(ann_train_out_path, 'w+') as f:\n",
        "        f.write(json.dumps(train_set))\n",
        "\n",
        "    with open(ann_val_out_path, 'w+') as f:\n",
        "        f.write(json.dumps(val_set))\n",
        "\n",
        "    with open(ann_test_out_path, 'w+') as f:\n",
        "        f.write(json.dumps(test_set))\n"
      ],
      "metadata": {
        "id": "EDh8FFzFhqut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#detr"
      ],
      "metadata": {
        "id": "8s9VTnF5FG-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TACODataset(TvCocoDetection):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_folder,\n",
        "        ann_file,\n",
        "        transforms,\n",
        "        cache_mode=False,\n",
        "        local_rank=0,\n",
        "        local_size=1,\n",
        "        use_crowd=False,\n",
        "    ):\n",
        "        super(TACODataset, self).__init__(\n",
        "            img_folder,\n",
        "            ann_file,\n",
        "            cache_mode=cache_mode,\n",
        "            local_rank=local_rank,\n",
        "            local_size=local_size,\n",
        "        )\n",
        "        self._transforms = transforms\n",
        "        self.prepare = ConvertCocoPolysToMask(use_crowd)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super(TACODataset, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = {\"image_id\": image_id, \"annotations\": target}\n",
        "        img, target = self.prepare(img, target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class ConvertCocoPolysToMask(object):\n",
        "    def __init__(self, use_crowd):\n",
        "       self.use_crowd = use_crowd\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "        image_id = torch.tensor([image_id])\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        if not self.use_crowd:\n",
        "          anno = [obj for obj in anno if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0]\n",
        "\n",
        "        boxes = [obj[\"bbox\"] for obj in anno]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64)\n",
        "\n",
        "        segmentations = [obj[\"segmentation\"] for obj in anno]\n",
        "        masks = self.convert_coco_poly_to_mask(segmentations=segmentations, height=h, width=w)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        masks = masks[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "\n",
        "        # for conversion to coco api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
        "        iscrowd = torch.tensor(\n",
        "            [obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno]\n",
        "        )\n",
        "        target[\"area\"] = area[keep]\n",
        "        target[\"iscrowd\"] = iscrowd[keep]\n",
        "\n",
        "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
        "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def convert_coco_poly_to_mask(self, segmentations, height, width):\n",
        "      masks = []\n",
        "      for polygons in segmentations:\n",
        "          rles = coco_mask.frPyObjects(polygons, height, width)\n",
        "          mask = coco_mask.decode(rles)\n",
        "          if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "          mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "          mask = mask.any(dim=2)\n",
        "          masks.append(mask)\n",
        "      if masks:\n",
        "          masks = torch.stack(masks, dim=0)\n",
        "      else:\n",
        "          masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "      return masks\n",
        "\n",
        "\n",
        "def make_coco_transforms(image_set):\n",
        "\n",
        "    normalize = T.Compose(\n",
        "        [T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
        "    )\n",
        "\n",
        "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
        "\n",
        "    if image_set == \"train\":\n",
        "        return T.Compose(\n",
        "            [\n",
        "                T.RandomHorizontalFlip(),\n",
        "                T.RandomSelect(\n",
        "                    T.RandomResize(scales, max_size=1333),\n",
        "                    T.Compose(\n",
        "                        [\n",
        "                            T.RandomResize([400, 500, 600]),\n",
        "                            T.RandomSizeCrop(384, 600),\n",
        "                            T.RandomResize(scales, max_size=1333),\n",
        "                        ]\n",
        "                    ),\n",
        "                ),\n",
        "                normalize,\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    if image_set == \"val\":\n",
        "        return T.Compose([T.RandomResize([800], max_size=1333), normalize,])\n",
        "    if image_set == \"test\":\n",
        "        return None\n",
        "\n",
        "    raise ValueError(f\"unknown {image_set}\")\n",
        "\n",
        "def create_dataset(split):\n",
        "  ann_file = Path(\"/content/TACO-expl/data/annotations_0_\" + split +\".json\")\n",
        "  img_folder = Path(\"/content/MyDrive/MyDrive/\")\n",
        "\n",
        "  dataset = TACODataset(\n",
        "      img_folder,\n",
        "      ann_file,\n",
        "      transforms=make_coco_transforms(split),\n",
        "      local_rank=mutils.get_local_rank(),\n",
        "      local_size=mutils.get_local_size(),\n",
        "  )\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "SMqe-f0uqtjo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/HDETR/H-Deformable-DETR/releases/download/v0.1/r50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth\n",
        "!mv r50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth r50.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ERweJ8HTfm9",
        "outputId": "a8243832-7d15-4835-8661-984aa45c6bad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-30 17:39:43--  https://github.com/HDETR/H-Deformable-DETR/releases/download/v0.1/r50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/517883062/ce62ee7a-43ec-4230-8bf1-348a1530d246?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240130T173943Z&X-Amz-Expires=300&X-Amz-Signature=bc43ec6c03e8e4043f4761e0ef3220f6fb31052a46373affdfb4caa95abe7310&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=517883062&response-content-disposition=attachment%3B%20filename%3Dr50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-01-30 17:39:43--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/517883062/ce62ee7a-43ec-4230-8bf1-348a1530d246?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240130T173943Z&X-Amz-Expires=300&X-Amz-Signature=bc43ec6c03e8e4043f4761e0ef3220f6fb31052a46373affdfb4caa95abe7310&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=517883062&response-content-disposition=attachment%3B%20filename%3Dr50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 192422339 (184M) [application/octet-stream]\n",
            "Saving to: ‘r50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth’\n",
            "\n",
            "r50_hybrid_branch_l 100%[===================>] 183.51M   131MB/s    in 1.4s    \n",
            "\n",
            "2024-01-30 17:39:45 (131 MB/s) - ‘r50_hybrid_branch_lambda1_group6_t1500_dp0_mqs_lft_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage_36eps.pth’ saved [192422339/192422339]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "args = AttrDict()\n",
        "args.output_dir = './out/'\n",
        "args.with_box_refine = True\n",
        "args.two_stage = True\n",
        "args.dim_feedforward = 2048\n",
        "args.num_queries_one2one = 300\n",
        "args.num_queries_one2many = 1500\n",
        "args.k_one2many = 6\n",
        "args.lambda_one2many = 1.0\n",
        "args.mixed_selection = True\n",
        "args.look_forward_twice = True\n",
        "args.dataset_file = \"coco\"\n",
        "args.device = 'cuda'\n",
        "args.hidden_dim = 256\n",
        "args.position_embedding = 'sine'\n",
        "args.position_embedding_scale = np.pi *2\n",
        "#args.lr_backbone =2e-5\n",
        "args.lr_backbone = 0\n",
        "args.masks = False\n",
        "args.num_feature_levels = 4\n",
        "args.backbone = \"resnet50\"\n",
        "args.dilation = False\n",
        "args.nheads = 8\n",
        "args.enc_layers = 6\n",
        "args.dec_layers = 6\n",
        "args.dim_feedforwards = 2048\n",
        "args.dropout = 0\n",
        "args.dec_n_points = 4\n",
        "args.enc_n_points = 4\n",
        "args.use_checkpoint = True\n",
        "args.aux_loss = False\n",
        "args.cls_loss_coef=2\n",
        "args.giou_loss_coef=2\n",
        "args.focal_alpha=0.25\n",
        "args.topk=100\n",
        "args.bbox_loss_coef=5\n",
        "args.set_cost_class=2\n",
        "args.set_cost_bbox=5\n",
        "args.set_cost_giou=2"
      ],
      "metadata": {
        "id": "hcD6Qhp9AE7K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_detr(args):\n",
        "  model,crit, postproc = HDDETR.models.build(args)\n",
        "  postproc.update({\"segm\": HDDETR.models.segmentation.PostProcessSegm()})\n",
        "  crit.losses.append(\"masks\")\n",
        "  crit.num_classes = 10\n",
        "  crit.weight_dict[\"loss_mask\"] = 8\n",
        "  crit.weight_dict[\"loss_dice\"] = 8\n",
        "  model.num_queries = 300\n",
        "  model.transformer.two_stage_num_proposals = 300\n",
        "  model.load_state_dict(torch.load(\"r50.pth\")[\"model\"])\n",
        "  return model, crit, postproc"
      ],
      "metadata": {
        "id": "jAq0eFHI2YUO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from HDDETR.util.misc import NestedTensor\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "dev = torch.device(\"cuda\")\n",
        "model.to(dev)\n",
        "to_t = transforms.ToTensor()\n",
        "img = Image.open(\"treno.jpg\")\n",
        "a = to_t(img).reshape((1,3,640,480))\n",
        "a = a.to(dev)\n",
        "mask = torch.zeros((1,640,480), dtype=torch.bool, device=dev)\n",
        "mask = mask.to(dev)\n",
        "nt = NestedTensor(a, mask)\n",
        "nt = nt.to(dev)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Aa-WuoDL55-M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "60e85507-514b-4acc-a28f-f8415f3ef1fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom HDDETR.util.misc import NestedTensor\\nfrom PIL import Image\\nimport torch\\nfrom torchvision import transforms\\n\\ndev = torch.device(\"cuda\")\\nmodel.to(dev)\\nto_t = transforms.ToTensor()\\nimg = Image.open(\"treno.jpg\")\\na = to_t(img).reshape((1,3,640,480))\\na = a.to(dev)\\nmask = torch.zeros((1,640,480), dtype=torch.bool, device=dev)\\nmask = mask.to(dev)\\nnt = NestedTensor(a, mask)\\nnt = nt.to(dev)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskFrozenDETR(nn.Module):\n",
        "  def __init__(self, detr, device, num_classes):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.detr = detr\n",
        "    self.detr.num_queries = self.detr.num_queries_one2one\n",
        "    self.detr.transformer.two_stage_num_proposals = self.detr.num_queries_one2one\n",
        "    self.num_classes = num_classes\n",
        "    for param in detr.parameters():\n",
        "      param.requires_grad = False\n",
        "    # vedi deformable encoder block\n",
        "    self.feature_enc_1 = DeformableTransformerEncoderLayer(d_model=256,dropout=0, activation='gelu')\n",
        "    self.feature_enc_2 = DeformableTransformerEncoderLayer(d_model=256,dropout=0, activation='gelu')\n",
        "    self.box_enc_1 = DeformableTransformerEncoderLayer(d_model=128,dropout=0, activation='gelu')\n",
        "    self.box_enc_2 = DeformableTransformerEncoderLayer(d_model=128,dropout=0, activation='gelu')\n",
        "    self.channel_mapper = nn.Linear(256, 128)\n",
        "    self.query_channel_mapper = nn.Linear(256, 128)\n",
        "    self.roialign = RoIAlign(output_size=(32,32),spatial_scale=0.25, sampling_ratio=-1)\n",
        "    self.class_adapter = nn.Linear(256, num_classes)\n",
        "    self.topk = 96\n",
        "  def _paste(self, roi, empty_mask, flatboxes, index):\n",
        "      ox = int(round(flatboxes[index][0].item()))\n",
        "      oy = int(round(flatboxes[index][1].item()))\n",
        "      x1 = min(roi.shape[1], empty_mask[index].shape[1]-ox)\n",
        "      y1 = min(roi.shape[0], empty_mask[index].shape[0]-oy)\n",
        "\n",
        "      empty_mask[index][oy:oy+roi.shape[0],\n",
        "                        ox:ox+roi.shape[1]] = roi[:y1,:x1]\n",
        "      return empty_mask\n",
        "\n",
        "  def forward(self, input, sizes=None):\n",
        "    if isinstance(input, torch.Tensor):\n",
        "      input = mutils.nested_tensor_from_tensor_list(input)\n",
        "\n",
        "    bs, _, h, w = input.tensors.shape\n",
        "\n",
        "    # get output from the H-DETR\n",
        "    detr_out = self.detr(input)\n",
        "\n",
        "    # compute reference points for the following encoder layers\n",
        "    ref_points = self.detr.transformer.encoder.get_reference_points(detr_out[\"intermediate_enc_out\"][\"spatial_shapes\"],\n",
        "                                                                    detr_out[\"intermediate_enc_out\"][\"valid_ratios\"],\n",
        "                                                                    self.device)\n",
        "    # remove key not required by DeformableTransformerEncoderLayer.forward()\n",
        "    detr_out[\"intermediate_enc_out\"].pop(\"valid_ratios\")\n",
        "\n",
        "    # pass the multi-scale encoder maps to the two deformable layers\n",
        "    enc_maps = self.feature_enc_1(**detr_out[\"intermediate_enc_out\"], reference_points=ref_points)\n",
        "    detr_out[\"intermediate_enc_out\"][\"src\"] = enc_maps\n",
        "    enc_maps = self.feature_enc_2(**detr_out[\"intermediate_enc_out\"], reference_points=ref_points).permute(0, 2, 1)\n",
        "\n",
        "    # interpolate he maps to the backbone dimension\n",
        "    # detr_out[\"backbone_out\"].shape\n",
        "    backbone_h, backbone_w = detr_out[\"backbone_out\"].tensors.shape[-1], detr_out[\"backbone_out\"].tensors.shape[-2]\n",
        "    fe = mutils.interpolate(enc_maps, backbone_h*backbone_w, mode=\"linear\")\n",
        "\n",
        "    # sum the maps and reduce dimensionality\n",
        "    f = detr_out[\"backbone_out\"].tensors.view((bs, 256, -1 )) + fe\n",
        "\n",
        "    # map channel reduction\n",
        "    mapped_f = self.channel_mapper(f.permute(0, 2, 1))\n",
        "\n",
        "    # computing class for each proposal\n",
        "    logits = self.class_adapter(detr_out['decoder_out'][-1])\n",
        "\n",
        "    # computing top 100 proposals, boxes and queries\n",
        "    ci = logits.sigmoid()\n",
        "    topk_values, topk_indexes = torch.topk(\n",
        "            ci.view(logits.shape[0], -1), self.topk, dim=1\n",
        "    )\n",
        "    topk_boxes = topk_indexes // logits.shape[2]\n",
        "    boxes = box_ops.box_cxcywh_to_xyxy(detr_out[\"pred_boxes\"])\n",
        "    boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n",
        "    img_h, img_w = sizes.unbind(1)\n",
        "    scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
        "    boxes = boxes * scale_fct[:, None, :]\n",
        "    boxes = torch.clamp(boxes, min=0)\n",
        "    object_queries = detr_out[\"decoder_out\"][-1, :, :self.detr.num_queries_one2one, :]\n",
        "    object_queries = torch.gather(object_queries, 1, topk_boxes.unsqueeze(-1).repeat(1,1,object_queries.shape[-1]))\n",
        "    logits = torch.gather(logits, 1, topk_boxes.unsqueeze(-1).repeat(1,1, num_classes))\n",
        "\n",
        "\n",
        "    batchindexes = torch.arange(bs).reshape(bs,1,1).repeat(1,self.topk,1).to(self.device)\n",
        "    roiboxes = torch.cat([batchindexes, boxes], -1).reshape(bs*self.topk, 5)\n",
        "\n",
        "    Ri = self.roialign(mapped_f.reshape((bs, 128, backbone_h, backbone_w )), roiboxes)\n",
        "    maskRi = self.roialign(detr_out[\"backbone_out\"].mask.to(torch.float32).unsqueeze(1), roiboxes)\\\n",
        "                 .to(torch.bool)\\\n",
        "                 .permute(0,2,3,1).squeeze()\n",
        "\n",
        "    Ri = Ri.permute(0, 2, 3, 1).reshape(bs*self.topk,32*32, 128)\n",
        "     # ?\n",
        "\n",
        "    valid_ratios_box = torch.stack([self.detr.transformer.get_valid_ratio(m.unsqueeze(0)) for m in maskRi], 0)\n",
        "\n",
        "    ref_point_box = self.detr.transformer.encoder.get_reference_points(torch.tensor([[32,32]]),\n",
        "                                                                    valid_ratios_box,\n",
        "                                                                    self.device)\n",
        "\n",
        "    maskRi = maskRi.reshape(bs*self.topk,32*32)\n",
        "    Ri = self.box_enc_1(Ri, padding_mask=maskRi,\n",
        "                        level_start_index=torch.tensor([0]).to(self.device),\n",
        "                        pos=None,\n",
        "                        reference_points=ref_point_box,\n",
        "                        spatial_shapes=torch.tensor([[32,32]]).to(self.device))\n",
        "    Ri = self.box_enc_2(Ri, padding_mask=maskRi,\n",
        "                        level_start_index=torch.tensor([0]).to(self.device),\n",
        "                        pos=None,\n",
        "                        reference_points=ref_point_box,\n",
        "                        spatial_shapes=torch.tensor([[32,32]]).to(self.device))\n",
        "\n",
        "    object_queries = self.query_channel_mapper(object_queries).reshape(bs*self.topk, 128, 1)\n",
        "\n",
        "\n",
        "    segmasks = torch.bmm(Ri, object_queries).sigmoid().reshape(bs * self.topk, 32, 32)\n",
        "    emptym = torch.zeros(bs*self.topk, h, w)\n",
        "    flatboxes = boxes.view(bs * self.topk, 4)\n",
        "    for m in range(segmasks.shape[0]):\n",
        "      box_w = max(int(round((flatboxes[m][2] - flatboxes[m][0]).item())), 1)\n",
        "      box_h = max(int(round((flatboxes[m][3] - flatboxes[m][1]).item())), 1)\n",
        "      inter = mutils.interpolate(segmasks[m].expand(1,1,-1,-1),\n",
        "                                 (box_h, box_w),\n",
        "                                 mode=\"bilinear\").squeeze((0,1))\n",
        "      emptym = self._paste(inter, emptym, flatboxes, m)\n",
        "\n",
        "\n",
        "\n",
        "    segmasks = emptym.reshape(bs, self.topk, h, w)\n",
        "\n",
        "    return {\"pred_masks\": segmasks,\n",
        "            \"pred_logits\":logits,\n",
        "            \"pred_boxes\": boxes}\n",
        "\n",
        "    # TODO:\n",
        "    # attaccare loss preimplementate\n",
        "    # attaccare pycocotools per l'evaluation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vohh7Js-FMQa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(42)\n",
        "batch_size = 2\n",
        "lr = 1.5e-4\n",
        "betas = (0.9, 0.999)\n",
        "weight_decay = 5e-5\n",
        "epochs = 6\n",
        "num_classes = 11 # 10 + background\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "dataset_train = create_dataset(\"train\")\n",
        "dataset_val = create_dataset(\"val\")\n",
        "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
        "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
        "\n",
        "batch_sampler_train = torch.utils.data.BatchSampler(\n",
        "        sampler_train, batch_size, drop_last=True\n",
        ")\n",
        "\n",
        "data_loader_train = DataLoader(\n",
        "        dataset_train,\n",
        "        batch_sampler=batch_sampler_train,\n",
        "        collate_fn=mutils.collate_fn,\n",
        "        pin_memory=True,)\n",
        "\n",
        "data_loader_val = DataLoader(\n",
        "        dataset_val,\n",
        "        batch_size,\n",
        "        sampler=sampler_val,\n",
        "        drop_last=False,\n",
        "        collate_fn=mutils.collate_fn,\n",
        "        pin_memory=True,)\n",
        "\n",
        "detr, criterion, postprocessor = load_detr(args)\n",
        "\n",
        "mfdetr = MaskFrozenDETR(detr, device, num_classes)\n",
        "mfdetr.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW([p for p in mfdetr.parameters() if p.requires_grad], lr=lr,\n",
        "                              betas=betas,\n",
        "                              weight_decay=weight_decay)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w0JOZXORcoX",
        "outputId": "6c06720b-ed5d-4c37-85b7-2984694a7062"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.25s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topk for eval: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def train_MFDETR(model, criterion ,postprocessors, dl_train, dl_val, optimizer, epochs, device):\n",
        "\n",
        "  for e in range(epochs):\n",
        "    model.train()\n",
        "    prefetcher = data_prefetcher(dl_train, device, prefetch=True)\n",
        "    samples, targets = prefetcher.next()\n",
        "    criterion.train()\n",
        "    metric_logger = mutils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter(\"lr\", mutils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
        "    metric_logger.add_meter(\n",
        "        \"class_error\", mutils.SmoothedValue(window_size=1, fmt=\"{value:.2f}\")\n",
        "    )\n",
        "    header = \"Epoch: [{}]\".format(e)\n",
        "    print_freq = 10\n",
        "\n",
        "    for b in metric_logger.log_every(range(len(dl_train)), print_freq, header):\n",
        "\n",
        "      sizes = torch.stack([t[\"size\"] for t in targets])\n",
        "      t = time.time()\n",
        "      outputs = model(samples, sizes)\n",
        "      print(f\"model exec: {time.time() - t}\")\n",
        "      t = time.time()\n",
        "      loss_dict = criterion(outputs, targets)\n",
        "      print(f\"loss comp: {time.time() - t}\")\n",
        "      t = time.time()\n",
        "      weight_dict = criterion.weight_dict\n",
        "      losses = sum(loss_dict[k]*weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "      loss_dict_reduced = mutils.reduce_dict(loss_dict)\n",
        "      loss_dict_reduced_unscaled = {\n",
        "            f\"{k}_unscaled\": v for k, v in loss_dict_reduced.items()}\n",
        "      loss_dict_reduced_scaled = {\n",
        "            k: v * weight_dict[k]\n",
        "            for k, v in loss_dict_reduced.items()\n",
        "            if k in weight_dict}\n",
        "      losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
        "\n",
        "      loss_value = losses_reduced_scaled.item()\n",
        "      if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            return model\n",
        "      print(f\"loss reduction: {time.time() - t}\")\n",
        "      t = time.time()\n",
        "      optimizer.zero_grad()\n",
        "      losses.backward()\n",
        "      optimizer.step()\n",
        "      print(f\"optimization: {time.time() - t}\")\n",
        "      metric_logger.update(\n",
        "            loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled\n",
        "      )\n",
        "      metric_logger.update(class_error=loss_dict_reduced[\"class_error\"])\n",
        "      metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "      samples, targets = prefetcher.next()\n",
        "\n",
        "    val_evaluation(model, criterion, postprocessors, dl_val)\n",
        "  return model\n",
        "\n",
        "def val_evaluation(model, crterion, postprocessors, dl_val):\n",
        "  model.eval()\n",
        "  criterion.eval()\n",
        "  iou_types = tuple(k for k in (\"segm\", \"bbox\") if k in postprocessors.keys())\n",
        "  coco_evaluator = CocoEvaluator(dl_val.dataset.base_ds.coco, iou_types)\n",
        "  metric_logger = mutils.MetricLogger(delimiter=\"  \")\n",
        "  metric_logger.add_meter(\n",
        "        \"class_error\", mutils.SmoothedValue(window_size=1, fmt=\"{value:.2f}\"))\n",
        "  header = \"Validation:\"\n",
        "  for samples, targets in metric_logger.log_every(dl_val, 10, header):\n",
        "      samples = samples.to(device)\n",
        "      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "      outputs = model(samples)\n",
        "      loss_dict = criterion(outputs, targets)\n",
        "      weight_dict = criterion.weight_dict\n",
        "\n",
        "      # reduce losses over all GPUs for logging purposes\n",
        "      loss_dict_reduced = mutils.reduce_dict(loss_dict)\n",
        "      loss_dict_reduced_scaled = {\n",
        "          k: v * weight_dict[k]\n",
        "          for k, v in loss_dict_reduced.items()\n",
        "          if k in weight_dict}\n",
        "      loss_dict_reduced_unscaled = {\n",
        "          f\"{k}_unscaled\": v for k, v in loss_dict_reduced.items()}\n",
        "      metric_logger.update(\n",
        "            loss=sum(loss_dict_reduced_scaled.values()),\n",
        "            **loss_dict_reduced_scaled,\n",
        "            **loss_dict_reduced_unscaled,)\n",
        "      metric_logger.update(class_error=loss_dict_reduced[\"class_error\"])\n",
        "\n",
        "      orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
        "      results = postprocessors[\"bbox\"](outputs, orig_target_sizes)\n",
        "      target_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n",
        "      results = postprocessors[\"segm\"](\n",
        "                results, outputs, orig_target_sizes, target_sizes)\n",
        "      res = {target[\"image_id\"].item(): output\n",
        "            for target, output in zip(targets, results)}\n",
        "      if coco_evaluator is not None:\n",
        "          coco_evaluator.update(res)\n",
        "\n",
        "  # gather the stats from all processes\n",
        "  metric_logger.synchronize_between_processes()\n",
        "  print(\"Averaged stats:\", metric_logger)\n",
        "  if coco_evaluator is not None:\n",
        "      coco_evaluator.synchronize_between_processes()\n",
        "      # accumulate predictions from all images\n",
        "  if coco_evaluator is not None:\n",
        "      coco_evaluator.accumulate()\n",
        "      coco_evaluator.summarize()\n",
        "  stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "  if coco_evaluator is not None:\n",
        "      if \"bbox\" in postprocessors.keys():\n",
        "          stats[\"coco_eval_bbox\"] = coco_evaluator.coco_eval[\"bbox\"].stats.tolist()\n",
        "      if \"segm\" in postprocessors.keys():\n",
        "          stats[\"coco_eval_masks\"] = coco_evaluator.coco_eval[\"segm\"].stats.tolist()\n",
        "\n",
        "  return stats, coco_evaluator\n"
      ],
      "metadata": {
        "id": "-R_MuOnmBxrP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_MFDETR(mfdetr, criterion, postprocessor, data_loader_train, data_loader_val, optimizer, 1, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRkLkswfMkYT",
        "outputId": "841ff260-f1c9-4ba2-f1b3-8f9b2570c18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model exec: 1.959366798400879\n",
            "loss comp: 0.4455716609954834\n",
            "loss reduction: 0.0014171600341796875\n",
            "optimization: 97.3411693572998\n",
            "Epoch: [0]  [   0/1532]  eta: 1 day, 19:00:34  lr: 0.000150  class_error: 100.00  loss: 3341.5967 (3341.5967)  loss_ce: 140.8284 (140.8284)  loss_bbox: 3189.2271 (3189.2271)  loss_giou: 2.6488 (2.6488)  loss_mask: 1.0350 (1.0350)  loss_dice: 7.8576 (7.8576)  loss_ce_unscaled: 70.4142 (70.4142)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 637.8454 (637.8454)  loss_giou_unscaled: 1.3244 (1.3244)  cardinality_error_unscaled: 94.0000 (94.0000)  loss_mask_unscaled: 0.1294 (0.1294)  loss_dice_unscaled: 0.9822 (0.9822)  time: 101.0672  data: 0.0000  max mem: 7580\n",
            "model exec: 0.6856245994567871\n",
            "loss comp: 0.03874468803405762\n",
            "loss reduction: 0.001726388931274414\n",
            "optimization: 51.51688623428345\n",
            "model exec: 0.781907320022583\n",
            "loss comp: 0.05082559585571289\n",
            "loss reduction: 0.0017323493957519531\n",
            "optimization: 52.204692125320435\n",
            "model exec: 0.9458801746368408\n",
            "loss comp: 0.1264805793762207\n",
            "loss reduction: 0.0015475749969482422\n",
            "optimization: 76.26873445510864\n",
            "model exec: 0.9647526741027832\n",
            "loss comp: 0.3333733081817627\n",
            "loss reduction: 0.00199127197265625\n",
            "optimization: 62.356977701187134\n",
            "model exec: 0.6423821449279785\n",
            "loss comp: 0.024550437927246094\n",
            "loss reduction: 0.013695716857910156\n",
            "optimization: 50.980804204940796\n",
            "model exec: 0.9211826324462891\n",
            "loss comp: 0.09282946586608887\n",
            "loss reduction: 0.008258819580078125\n",
            "optimization: 80.08717226982117\n",
            "model exec: 0.8334708213806152\n",
            "loss comp: 0.07097005844116211\n",
            "loss reduction: 0.0017657279968261719\n",
            "optimization: 65.7684679031372\n",
            "model exec: 0.9205589294433594\n",
            "loss comp: 0.055735111236572266\n",
            "loss reduction: 0.008503913879394531\n",
            "optimization: 80.80368399620056\n",
            "model exec: 0.8847208023071289\n",
            "loss comp: 0.051001548767089844\n",
            "loss reduction: 0.002062559127807617\n",
            "optimization: 69.62940979003906\n",
            "model exec: 1.0272221565246582\n",
            "loss comp: 0.06279969215393066\n",
            "loss reduction: 0.0017468929290771484\n",
            "optimization: 66.40091872215271\n",
            "Epoch: [0]  [  10/1532]  eta: 1 day, 6:05:34  lr: 0.000150  class_error: 100.00  loss: 1737.7399 (1903.3360)  loss_ce: 230.0020 (225.6565)  loss_bbox: 1569.6179 (1666.7253)  loss_giou: 2.1376 (2.2327)  loss_mask: 1.0322 (1.0286)  loss_dice: 7.7203 (7.6929)  loss_ce_unscaled: 115.0010 (112.8282)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 313.9236 (333.3451)  loss_giou_unscaled: 1.0688 (1.1163)  cardinality_error_unscaled: 92.5000 (85.9545)  loss_mask_unscaled: 0.1290 (0.1286)  loss_dice_unscaled: 0.9650 (0.9616)  time: 71.1793  data: 0.0000  max mem: 7580\n",
            "model exec: 1.7414216995239258\n",
            "loss comp: 0.2914466857910156\n",
            "loss reduction: 0.002056121826171875\n",
            "optimization: 164.153418302536\n",
            "model exec: 0.8159739971160889\n",
            "loss comp: 0.08598089218139648\n",
            "loss reduction: 0.0021004676818847656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detr, criterion, postprocessor = load_detr(args)\n",
        "prefetcher = data_prefetcher(data_loader_train, device, prefetch=True)\n",
        "samples, targets = prefetcher.next()\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9NEtvq-e0E4",
        "outputId": "456b6c88-ba15-4632-e3b8-6fd544af4425"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topk for eval: 100\n",
            "[{'boxes': tensor([[0.4073, 0.5974, 0.1229, 0.0812]], device='cuda:0'), 'labels': tensor([7], device='cuda:0'), 'masks': tensor([[[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]]], device='cuda:0'), 'image_id': tensor([2597], device='cuda:0'), 'area': tensor([2727.7170], device='cuda:0'), 'iscrowd': tensor([0], device='cuda:0'), 'orig_size': tensor([4000, 3000], device='cuda:0'), 'size': tensor([810, 608], device='cuda:0')}, {'boxes': tensor([[0.4258, 0.5101, 0.1553, 0.2129]], device='cuda:0'), 'labels': tensor([7], device='cuda:0'), 'masks': tensor([[[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]]], device='cuda:0'), 'image_id': tensor([1107], device='cuda:0'), 'area': tensor([1546.4883], device='cuda:0'), 'iscrowd': tensor([0], device='cuda:0'), 'orig_size': tensor([1536, 2048], device='cuda:0'), 'size': tensor([512, 682], device='cuda:0')}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwUPtYgZe-Gi",
        "outputId": "c43e688f-79e4-4297-a32f-16a24d9252dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': tensor([[0.4073, 0.5974, 0.1229, 0.0812]], device='cuda:0'),\n",
              " 'labels': tensor([7], device='cuda:0'),\n",
              " 'masks': tensor([[[False, False, False,  ..., False, False, False],\n",
              "          [False, False, False,  ..., False, False, False],\n",
              "          [False, False, False,  ..., False, False, False],\n",
              "          ...,\n",
              "          [False, False, False,  ..., False, False, False],\n",
              "          [False, False, False,  ..., False, False, False],\n",
              "          [False, False, False,  ..., False, False, False]]], device='cuda:0'),\n",
              " 'image_id': tensor([2597], device='cuda:0'),\n",
              " 'area': tensor([2727.7170], device='cuda:0'),\n",
              " 'iscrowd': tensor([0], device='cuda:0'),\n",
              " 'orig_size': tensor([4000, 3000], device='cuda:0'),\n",
              " 'size': tensor([810, 608], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detr, criterion, postprocessor = load_detr(args)\n",
        "prefetcher = data_prefetcher(data_loader_train, device, prefetch=True)\n",
        "samples, targets = prefetcher.next()\n",
        "print(samples.tensors.shape)\n",
        "detr = detr.to(torch.device(\"cuda\"))\n",
        "out = detr(samples)\n",
        "for k in out.keys():\n",
        "  print(k)\n",
        "  print(type(out[k]))\n",
        "\n",
        "  if type(out[k]) == torch.Tensor:\n",
        "    print(out[k].shape)\n",
        "  elif isinstance(out[k], dict):\n",
        "    for kk in out[k].keys():\n",
        "      print(kk)\n",
        "      if type(out[k][kk]) == torch.Tensor:\n",
        "        print(out[k][kk].shape)\n",
        "      else:\n",
        "        print(out[k][kk])\n",
        "  else:\n",
        "    print(out[k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmdiV03DXBER",
        "outputId": "aaf88f05-b037-46cc-b9c0-83b53047f789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topk for eval: 100\n",
            "torch.Size([3, 3, 810, 682])\n",
            "4\n",
            "torch.Size([3, 102, 86])\n",
            "vrr\n",
            "torch.Size([3, 4, 2])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder_out\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([6, 3, 300, 256])\n",
            "pred_logits\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 300, 91])\n",
            "pred_boxes\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 300, 4])\n",
            "pred_logits_one2many\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 0, 91])\n",
            "pred_boxes_one2many\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 0, 4])\n",
            "backbone_out\n",
            "<class 'HDDETR.util.misc.NestedTensor'>\n",
            "tensor([[[[0.0049, 0.0369, 0.0051,  ..., 0.0220, 0.0219, 0.0206],\n",
            "          [0.0048, 0.0052, 0.0342,  ..., 0.0053, 0.0047, 0.0188],\n",
            "          [0.0046, 0.0048, 0.0791,  ..., 0.0056, 0.0050, 0.0191],\n",
            "          ...,\n",
            "          [0.1141, 0.0891, 0.1674,  ..., 0.0054, 0.0049, 0.0188],\n",
            "          [0.1164, 0.1170, 0.1109,  ..., 0.0048, 0.0044, 0.0193],\n",
            "          [0.1293, 0.0303, 0.0901,  ..., 0.0183, 0.0197, 0.0287]],\n",
            "\n",
            "         [[0.0417, 0.1120, 0.1045,  ..., 0.1492, 0.1489, 0.1255],\n",
            "          [0.0504, 0.0879, 0.1362,  ..., 0.1457, 0.1457, 0.1119],\n",
            "          [0.1288, 0.1151, 0.1309,  ..., 0.1459, 0.1459, 0.1122],\n",
            "          ...,\n",
            "          [0.2525, 0.2837, 0.3458,  ..., 0.1457, 0.1457, 0.1127],\n",
            "          [0.2883, 0.2623, 0.2998,  ..., 0.1453, 0.1454, 0.1105],\n",
            "          [0.2336, 0.2009, 0.1915,  ..., 0.1028, 0.0985, 0.0777]],\n",
            "\n",
            "         [[0.1614, 0.0082, 0.1106,  ..., 0.0565, 0.1869, 0.2387],\n",
            "          [0.3053, 0.1502, 0.2526,  ..., 0.2896, 0.3948, 0.4863],\n",
            "          [0.2933, 0.0000, 0.1384,  ..., 0.2823, 0.2999, 0.3774],\n",
            "          ...,\n",
            "          [0.2061, 0.1298, 0.2783,  ..., 0.2858, 0.2755, 0.3995],\n",
            "          [0.2749, 0.0000, 0.7251,  ..., 0.3103, 0.4026, 0.3598],\n",
            "          [0.5070, 0.5971, 0.6663,  ..., 0.4549, 0.3153, 0.2133]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0351, 0.1113, 0.1135,  ..., 0.0257, 0.0258, 0.0216],\n",
            "          [0.0187, 0.1049, 0.1545,  ..., 0.0257, 0.0257, 0.0204],\n",
            "          [0.0131, 0.0916, 0.1382,  ..., 0.0257, 0.0257, 0.0204],\n",
            "          ...,\n",
            "          [0.1251, 0.1132, 0.1475,  ..., 0.0257, 0.0257, 0.0203],\n",
            "          [0.1251, 0.1245, 0.1556,  ..., 0.0258, 0.0258, 0.0205],\n",
            "          [0.1050, 0.0739, 0.1345,  ..., 0.0262, 0.0266, 0.0222]],\n",
            "\n",
            "         [[0.5574, 0.7346, 0.5055,  ..., 0.4857, 0.5100, 0.5326],\n",
            "          [0.1273, 0.5021, 0.2049,  ..., 0.5225, 0.5491, 0.5957],\n",
            "          [0.0143, 0.3889, 0.2171,  ..., 0.5163, 0.5447, 0.5970],\n",
            "          ...,\n",
            "          [0.6167, 0.5973, 0.0448,  ..., 0.5157, 0.5451, 0.5962],\n",
            "          [0.6289, 0.8185, 0.0180,  ..., 0.5702, 0.5922, 0.6257],\n",
            "          [0.4864, 0.2445, 0.0576,  ..., 0.5180, 0.5259, 0.4898]],\n",
            "\n",
            "         [[1.0912, 0.0000, 0.0000,  ..., 0.1567, 0.0000, 0.0000],\n",
            "          [0.5752, 0.1070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.7092, 0.5110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0804,  ..., 0.3671, 0.2651, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0068, 0.0041, 0.1117,  ..., 0.0042, 0.0052, 0.0052],\n",
            "          [0.0069, 0.0056, 0.0835,  ..., 0.0048, 0.0049, 0.0076],\n",
            "          [0.0056, 0.0056, 0.0626,  ..., 0.0050, 0.0058, 0.0091],\n",
            "          ...,\n",
            "          [0.0273, 0.0049, 0.0054,  ..., 0.0054, 0.0049, 0.0188],\n",
            "          [0.0277, 0.0046, 0.0048,  ..., 0.0048, 0.0044, 0.0193],\n",
            "          [0.0361, 0.0182, 0.0183,  ..., 0.0183, 0.0197, 0.0287]],\n",
            "\n",
            "         [[0.0808, 0.0439, 0.1327,  ..., 0.0122, 0.0232, 0.0722],\n",
            "          [0.0815, 0.0508, 0.0867,  ..., 0.0097, 0.0111, 0.0050],\n",
            "          [0.0710, 0.0881, 0.1040,  ..., 0.0104, 0.0095, 0.0319],\n",
            "          ...,\n",
            "          [0.1456, 0.1458, 0.1461,  ..., 0.1457, 0.1457, 0.1127],\n",
            "          [0.1432, 0.1455, 0.1459,  ..., 0.1453, 0.1454, 0.1105],\n",
            "          [0.1051, 0.1031, 0.1030,  ..., 0.1028, 0.0985, 0.0777]],\n",
            "\n",
            "         [[0.2107, 0.4565, 0.0110,  ..., 0.2444, 0.3677, 0.7809],\n",
            "          [0.8181, 0.5103, 0.3370,  ..., 0.0000, 0.4614, 0.7798],\n",
            "          [1.0747, 1.0217, 0.2350,  ..., 0.1447, 0.0000, 0.5733],\n",
            "          ...,\n",
            "          [0.1296, 0.3725, 0.2979,  ..., 0.2858, 0.2755, 0.3995],\n",
            "          [0.2718, 0.3149, 0.2991,  ..., 0.3103, 0.4026, 0.3598],\n",
            "          [0.0985, 0.4384, 0.3479,  ..., 0.4549, 0.3153, 0.2133]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.1472, 0.1851, 0.2606,  ..., 0.1519, 0.2352, 0.1774],\n",
            "          [0.1903, 0.2344, 0.2848,  ..., 0.2030, 0.2347, 0.1772],\n",
            "          [0.2471, 0.2309, 0.3005,  ..., 0.1811, 0.2067, 0.1314],\n",
            "          ...,\n",
            "          [0.0256, 0.0257, 0.0257,  ..., 0.0257, 0.0257, 0.0203],\n",
            "          [0.0258, 0.0258, 0.0258,  ..., 0.0258, 0.0258, 0.0205],\n",
            "          [0.0281, 0.0261, 0.0261,  ..., 0.0262, 0.0266, 0.0222]],\n",
            "\n",
            "         [[0.7375, 0.4006, 0.6861,  ..., 0.3637, 0.8192, 0.2677],\n",
            "          [0.5336, 0.4179, 1.0099,  ..., 0.6509, 0.6345, 0.5287],\n",
            "          [0.7894, 0.5741, 0.9564,  ..., 0.3602, 0.4665, 0.1855],\n",
            "          ...,\n",
            "          [0.5323, 0.5267, 0.5168,  ..., 0.5157, 0.5451, 0.5962],\n",
            "          [0.5656, 0.5707, 0.5698,  ..., 0.5702, 0.5922, 0.6257],\n",
            "          [0.4855, 0.5143, 0.5180,  ..., 0.5180, 0.5259, 0.4898]],\n",
            "\n",
            "         [[0.2127, 0.1598, 0.0000,  ..., 0.2160, 0.0458, 0.0000],\n",
            "          [0.0327, 0.0850, 0.0000,  ..., 0.3735, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0342, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.1890, 0.2809, 0.3667,  ..., 0.3671, 0.2651, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.1244, 0.1024, 0.1056,  ..., 0.0220, 0.0219, 0.0206],\n",
            "          [0.1001, 0.0816, 0.0766,  ..., 0.0053, 0.0047, 0.0188],\n",
            "          [0.1052, 0.0835, 0.0851,  ..., 0.0056, 0.0050, 0.0191],\n",
            "          ...,\n",
            "          [0.0273, 0.0049, 0.0054,  ..., 0.0054, 0.0049, 0.0188],\n",
            "          [0.0277, 0.0046, 0.0048,  ..., 0.0048, 0.0044, 0.0193],\n",
            "          [0.0361, 0.0182, 0.0183,  ..., 0.0183, 0.0197, 0.0287]],\n",
            "\n",
            "         [[0.2689, 0.2739, 0.2802,  ..., 0.1492, 0.1489, 0.1255],\n",
            "          [0.2584, 0.2601, 0.2606,  ..., 0.1457, 0.1457, 0.1119],\n",
            "          [0.2583, 0.2594, 0.2585,  ..., 0.1459, 0.1459, 0.1122],\n",
            "          ...,\n",
            "          [0.1456, 0.1458, 0.1461,  ..., 0.1457, 0.1457, 0.1127],\n",
            "          [0.1432, 0.1455, 0.1459,  ..., 0.1453, 0.1454, 0.1105],\n",
            "          [0.1051, 0.1031, 0.1030,  ..., 0.1028, 0.0985, 0.0777]],\n",
            "\n",
            "         [[0.1591, 0.1979, 0.2199,  ..., 0.0565, 0.1869, 0.2387],\n",
            "          [0.0326, 0.0890, 0.0485,  ..., 0.2896, 0.3948, 0.4863],\n",
            "          [0.0363, 0.1816, 0.0774,  ..., 0.2823, 0.2999, 0.3774],\n",
            "          ...,\n",
            "          [0.1296, 0.3725, 0.2979,  ..., 0.2858, 0.2755, 0.3995],\n",
            "          [0.2718, 0.3149, 0.2991,  ..., 0.3103, 0.4026, 0.3598],\n",
            "          [0.0985, 0.4384, 0.3479,  ..., 0.4549, 0.3153, 0.2133]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0409, 0.0178, 0.0142,  ..., 0.0257, 0.0258, 0.0216],\n",
            "          [0.0117, 0.0117, 0.0117,  ..., 0.0257, 0.0257, 0.0204],\n",
            "          [0.0116, 0.0118, 0.0118,  ..., 0.0257, 0.0257, 0.0204],\n",
            "          ...,\n",
            "          [0.0256, 0.0257, 0.0257,  ..., 0.0257, 0.0257, 0.0203],\n",
            "          [0.0258, 0.0258, 0.0258,  ..., 0.0258, 0.0258, 0.0205],\n",
            "          [0.0281, 0.0261, 0.0261,  ..., 0.0262, 0.0266, 0.0222]],\n",
            "\n",
            "         [[0.3213, 0.3684, 0.4147,  ..., 0.4857, 0.5100, 0.5326],\n",
            "          [0.2976, 0.3877, 0.4208,  ..., 0.5225, 0.5491, 0.5957],\n",
            "          [0.2984, 0.4409, 0.4665,  ..., 0.5163, 0.5447, 0.5970],\n",
            "          ...,\n",
            "          [0.5323, 0.5267, 0.5168,  ..., 0.5157, 0.5451, 0.5962],\n",
            "          [0.5656, 0.5707, 0.5698,  ..., 0.5702, 0.5922, 0.6257],\n",
            "          [0.4855, 0.5143, 0.5180,  ..., 0.5180, 0.5259, 0.4898]],\n",
            "\n",
            "         [[0.1492, 0.0000, 0.0000,  ..., 0.1567, 0.0000, 0.0000],\n",
            "          [0.0080, 0.0000, 0.0180,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.2595, 0.0579, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0342, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.1890, 0.2809, 0.3667,  ..., 0.3671, 0.2651, 0.0000]]]],\n",
            "       device='cuda:0')\n",
            "intermediate_enc_out\n",
            "<class 'dict'>\n",
            "src\n",
            "torch.Size([3, 11680, 256])\n",
            "valid_ratios\n",
            "torch.Size([3, 4, 2])\n",
            "spatial_shapes\n",
            "torch.Size([4, 2])\n",
            "level_start_index\n",
            "torch.Size([4])\n",
            "pos\n",
            "torch.Size([3, 11680, 256])\n",
            "padding_mask\n",
            "torch.Size([3, 11680])\n",
            "enc_outputs\n",
            "<class 'dict'>\n",
            "pred_logits\n",
            "torch.Size([3, 11680, 91])\n",
            "pred_boxes\n",
            "torch.Size([3, 11680, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor([True, False])\n",
        "print(a.to(torch.float32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4kYxKUVsTQq",
        "outputId": "fe9adaaf-c8b1-4082-9103-d9b5c9eae177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor([[1,2]])\n",
        "ab = torch.Tensor([[1,2]])\n",
        "abb = torch.Tensor([[1,2]])\n",
        "abbb = torch.Tensor([[1,2]])\n",
        "print(a.shape)\n",
        "print(torch.stack([a,ab,abb,abbb], 1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_87FGJj-QZ3s",
        "outputId": "c2e26383-af57-4472-863d-bfc3e1e52f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2])\n",
            "torch.Size([1, 4, 2])\n"
          ]
        }
      ]
    }
  ]
}
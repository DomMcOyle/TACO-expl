{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomMcOyle/TACO-expl/blob/add_detr/Training%20Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFLxtALRfcxt"
      },
      "source": [
        "# ML4CV project work\n",
        "\n",
        "Summary: Improving and explaining instance segmentation on a litter detection dataset\n",
        "\n",
        "Members:\n",
        "- Dell'Olio Domenico\n",
        "- Delvecchio Giovanni Pio\n",
        "- Disabato Raffaele\n",
        "\n",
        "The project was developed in order to improve instance segmentation results on the [TACO Dataset](http://tacodataset.org/).\n",
        "\n",
        "We decided to implement and test various architectures, among the highest scoring on COCO instance segmentation datasets, in order to compare their performances.\n",
        "We also tested some explainability methods on these models to try and explain model predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvaC-30OtBA_"
      },
      "source": [
        "## This notebook contains:\n",
        "- Dataset splitting and label replacing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW0_gODeKIeQ",
        "outputId": "62983f2a-5c28-47fb-fb05-f09b155637d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TACO-expl'...\n",
            "remote: Enumerating objects: 2490, done.\u001b[K\n",
            "remote: Counting objects: 100% (652/652), done.\u001b[K\n",
            "remote: Compressing objects: 100% (173/173), done.\u001b[K\n",
            "remote: Total 2490 (delta 524), reused 581 (delta 476), pack-reused 1838\u001b[K\n",
            "Receiving objects: 100% (2490/2490), 186.80 MiB | 23.09 MiB/s, done.\n",
            "Resolving deltas: 100% (1292/1292), done.\n",
            "/content/TACO-expl\n",
            "Branch 'add_detr' set up to track remote branch 'add_detr' from 'origin'.\n",
            "Switched to a new branch 'add_detr'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# repository cloning\n",
        "!git clone https://github.com/DomMcOyle/TACO-expl\n",
        "%cd /content/TACO-expl/\n",
        "!git checkout add_detr\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XATP-Y3ZglE6",
        "outputId": "a95e1252-84ca-4f7a-e7c1-f53f654dace6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'TACO-expl'\n",
            "/content/TACO-expl\n"
          ]
        }
      ],
      "source": [
        "# library imports\n",
        "%cd TACO-expl\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2dI-mnxFBNC"
      },
      "source": [
        "## Dataset splitting and label replacing\n",
        "As first pre-processing step, we obtain the so-called TACO-10 dataset from the official one provided. This dataset, differently from the ready-to-use one, has only 10 segmentation classes obtained by unifying all the available fine-grained classes.\n",
        "\n",
        "The ten new target classes are: \"Bottle\", \"Bottle cap\", \"Can\", \"Cigarette\", \"Cup\", \"Lid\", \"Other\", \"Plastic bag & wrapper\", \"Pop tab\", \"Straw\".\n",
        "\n",
        "The following code was adapted from the [TACO github repository](https://github.com/pedropro/TACO)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KutH3M6PmlFU"
      },
      "outputs": [],
      "source": [
        "keep_categories = [\"Bottle\", \"Bottle cap\", \"Can\", \"Cigarette\", \"Cup\",\n",
        "                   \"Lid\", \"Plastic bag & wrapper\", \"Pop tab\", \"Straw\"]\n",
        "\n",
        "def create_map(original, keep_supercategories):\n",
        "  \"\"\"\n",
        "  Function creating a map for the classes substitution\n",
        "  :param original: original categories of the dataset as described by the value \"categories\" in the dataset json\n",
        "  :param keep_supercategories: list of super-categories to keep in the final dataset\n",
        "  :return : a dictionary in the form \"category_to_replace\" : \"category_used_as_replacement\"\n",
        "  \"\"\"\n",
        "  class_map = {}\n",
        "  for cat in original:\n",
        "    if cat[\"supercategory\"] in keep_supercategories:\n",
        "      class_map[cat[\"name\"]] = cat[\"supercategory\"]\n",
        "    else:\n",
        "      class_map[cat[\"name\"]] = \"Other\"\n",
        "  return class_map\n",
        "\n",
        "def replace_dataset_classes(dataset, class_map):\n",
        "      \"\"\"\n",
        "      Replaces classes of dataset based on a dictionary\n",
        "\n",
        "      :param dataset: dataset description as obtained from the annotation json\n",
        "      :param class_map: mapping determining which class has to be substituted. Output of create_map.\n",
        "      \"\"\"\n",
        "      class_new_names = list(set(class_map.values()))\n",
        "      class_new_names.sort()\n",
        "      class_originals = copy.deepcopy(dataset['categories'])\n",
        "      dataset['categories'] = []\n",
        "      class_ids_map = {}  # map from old id to new id\n",
        "\n",
        "      # Assign background id 0\n",
        "      has_background = False\n",
        "      if 'Background' in class_new_names:\n",
        "          if class_new_names.index('Background') != 0:\n",
        "              class_new_names.remove('Background')\n",
        "              class_new_names.insert(0, 'Background')\n",
        "          has_background = True\n",
        "\n",
        "      # Replace categories\n",
        "      for id_new, class_new_name in enumerate(class_new_names):\n",
        "          # Make sure id:0 is reserved for background\n",
        "          id_rectified = id_new\n",
        "          if not has_background:\n",
        "              id_rectified += 1\n",
        "\n",
        "          category = {\n",
        "              'supercategory': '',\n",
        "              'id': id_rectified,  # Background has id=0\n",
        "              'name': class_new_name,\n",
        "          }\n",
        "          dataset['categories'].append(category)\n",
        "          # Map class names\n",
        "          for class_original in class_originals:\n",
        "              if class_map[class_original['name']] == class_new_name:\n",
        "                  class_ids_map[class_original['id']] = id_rectified\n",
        "\n",
        "      # Update annotations category id tag\n",
        "      for ann in dataset['annotations']:\n",
        "          ann['category_id'] = class_ids_map[ann['category_id']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RclP141dy5RU",
        "outputId": "4aa9b2a3-9f88-4655-c091-208a95a98bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'supercategory': '', 'id': 1, 'name': 'Bottle'}, {'supercategory': '', 'id': 2, 'name': 'Bottle cap'}, {'supercategory': '', 'id': 3, 'name': 'Can'}, {'supercategory': '', 'id': 4, 'name': 'Cigarette'}, {'supercategory': '', 'id': 5, 'name': 'Cup'}, {'supercategory': '', 'id': 6, 'name': 'Lid'}, {'supercategory': '', 'id': 7, 'name': 'Other'}, {'supercategory': '', 'id': 8, 'name': 'Plastic bag & wrapper'}, {'supercategory': '', 'id': 9, 'name': 'Pop tab'}, {'supercategory': '', 'id': 10, 'name': 'Straw'}]\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/TACO-expl/data/annotations.json\", \"r\") as f:\n",
        "    dataset = json.loads(f.read())\n",
        "\n",
        "class_map = create_map(dataset[\"categories\"], keep_categories)\n",
        "replace_dataset_classes(dataset, class_map)\n",
        "print(dataset[\"categories\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmL3HEVP0Eqh"
      },
      "source": [
        "After mapping the classes, a sanity check is employed on the dataset and it is then split in 80/10/10 proportion in Training, Validation and Test splits.\n",
        "\n",
        "The sanity check consists in removing all the images without boxes or with only ill-formed ones. If the image has some usable boxed and some unusable ones, the latter will be removed by the dataset loading class.\n",
        "\n",
        "It's important to notice that we employed only the \"official\" version of the dataset, which comprises 1500 images. We also tried using the \"unofficial\" one, which contains almoost 4000 samples, but since most of them were uploaded by unexpert users, the quality of segmentations and detections was extremely poor.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDh8FFzFhqut",
        "outputId": "34d88f39-2b50-473d-c17d-290b047f84f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "args = {\n",
        "    \"nr_trials\":1, # change if you want to generate more than one split\n",
        "    \"test_percentage\":0.1,\n",
        "    \"split_seed\": 42,\n",
        "    \"val_percentage\":0.1,\n",
        "    \"dataset_dir\":'/content/TACO-expl/data'\n",
        "}\n",
        "\n",
        "# annotation path\n",
        "ann_input_path = args[\"dataset_dir\"] + '/annotations.json'\n",
        "\n",
        "# Load annotations\n",
        "with open(ann_input_path, 'r') as f:\n",
        "    dataset = json.loads(f.read())\n",
        "\n",
        "# execute class mapping\n",
        "keep_categories = [\"Bottle\", \"Bottle cap\", \"Can\", \"Cigarette\", \"Cup\",\n",
        "                   \"Lid\", \"Plastic bag & wrapper\", \"Pop tab\", \"Straw\"]\n",
        "\n",
        "if keep_categories is not None:\n",
        "  class_map = create_map(dataset[\"categories\"], keep_categories)\n",
        "  replace_dataset_classes(dataset, class_map)\n",
        "\n",
        "anns = dataset['annotations']\n",
        "scene_anns = dataset['scene_annotations']\n",
        "imgs = dataset['images']\n",
        "nr_images = len(imgs)\n",
        "\n",
        "dimensions = {im['id']: [im['height'], im['width']] for im in dataset['images']}\n",
        "bad_ann = []\n",
        "image_with_at_least_one_ann = set()\n",
        "# check sanity of annotation\n",
        "for a in anns:\n",
        "    h, w = dimensions[a[\"image_id\"]]\n",
        "    boxes = a[\"bbox\"]\n",
        "    # guard against no boxes via resizing\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "    boxes[:, 2:] += boxes[:, :2]\n",
        "    boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "    boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "    segmentations = True if a[\"segmentation\"] else False\n",
        "\n",
        "    keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "    if (keep and segmentations):\n",
        "      image_with_at_least_one_ann.add(a[\"image_id\"])\n",
        "    else:\n",
        "      bad_ann.append(a)\n",
        "\n",
        "\n",
        "imgs = []\n",
        "# if image is without boxes is removed\n",
        "for i in dataset['images']:\n",
        "  if i[\"id\"] in image_with_at_least_one_ann:\n",
        "    imgs.append(i)\n",
        "  else:\n",
        "    print(\"removed image:\")\n",
        "    print(i)\n",
        "    print()\n",
        "print(bad_ann)\n",
        "\n",
        "for i in range(args[\"nr_trials\"]):\n",
        "\n",
        "    # create new dataset placeholder\n",
        "    train_set = {\n",
        "        'info': None,\n",
        "        'images': [],\n",
        "        'annotations': [],\n",
        "        'scene_annotations': [],\n",
        "        'licenses': [],\n",
        "        'categories': [],\n",
        "        'scene_categories': [],\n",
        "    }\n",
        "    train_set['info'] =  dataset['info']\n",
        "    train_set['categories'] = dataset['categories']\n",
        "    train_set['scene_categories'] = dataset['scene_categories']\n",
        "\n",
        "    val_set = copy.deepcopy(train_set)\n",
        "    test_set = copy.deepcopy(train_set)\n",
        "\n",
        "    train_set['images'], partial = train_test_split(imgs,\n",
        "                                                    random_state=args[\"split_seed\"],\n",
        "                                                               test_size=args[\"test_percentage\"]+args[\"val_percentage\"])\n",
        "    val_set['images'], test_set[\"images\"] = train_test_split(partial,\n",
        "                                                             random_state=args[\"split_seed\"],\n",
        "                                                             test_size=args[\"test_percentage\"]/(args[\"test_percentage\"]+args[\"val_percentage\"]))\n",
        "\n",
        "    # Aux Image Ids to split annotations\n",
        "    test_img_ids, val_img_ids, train_img_ids = [],[],[]\n",
        "    for img in test_set['images']:\n",
        "        test_img_ids.append(img['id'])\n",
        "\n",
        "    for img in val_set['images']:\n",
        "        val_img_ids.append(img['id'])\n",
        "\n",
        "    for img in train_set['images']:\n",
        "        train_img_ids.append(img['id'])\n",
        "\n",
        "    # Split instance annotations\n",
        "    for ann in anns:\n",
        "        if ann['image_id'] in test_img_ids:\n",
        "            test_set['annotations'].append(ann)\n",
        "        elif ann['image_id'] in val_img_ids:\n",
        "            val_set['annotations'].append(ann)\n",
        "        elif ann['image_id'] in train_img_ids:\n",
        "            train_set['annotations'].append(ann)\n",
        "\n",
        "    # Split scene tags\n",
        "    for ann in scene_anns:\n",
        "        if ann['image_id'] in test_img_ids:\n",
        "            test_set['scene_annotations'].append(ann)\n",
        "        elif ann['image_id'] in val_img_ids:\n",
        "            val_set['scene_annotations'].append(ann)\n",
        "        elif ann['image_id'] in train_img_ids:\n",
        "            train_set['scene_annotations'].append(ann)\n",
        "\n",
        "    # Write dataset splits\n",
        "    ann_train_out_path = args[\"dataset_dir\"] + '/' + 'annotations_off_' + str(i) +'_train.json'\n",
        "    ann_val_out_path   = args[\"dataset_dir\"] + '/' + 'annotations_off_' + str(i) + '_val.json'\n",
        "    ann_test_out_path  = args[\"dataset_dir\"] + '/' + 'annotations_off_' + str(i) + '_test.json'\n",
        "\n",
        "    with open(ann_train_out_path, 'w+') as f:\n",
        "        f.write(json.dumps(train_set))\n",
        "\n",
        "    with open(ann_val_out_path, 'w+') as f:\n",
        "        f.write(json.dumps(val_set))\n",
        "\n",
        "    with open(ann_test_out_path, 'w+') as f:\n",
        "        f.write(json.dumps(test_set))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FvaC-30OtBA_",
        "-2dI-mnxFBNC",
        "8s9VTnF5FG-q",
        "z2g_MxiHD4Gc"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}